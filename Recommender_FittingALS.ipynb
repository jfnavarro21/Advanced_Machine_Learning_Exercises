{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning (MScA, 32017)\n",
    "\n",
    "# Project Recommending Music with Audioscrobbler Data\n",
    "\n",
    "### Yuri Balasanov, Mihail Tselishchev, &copy; iLykei 2017\n",
    "\n",
    "## Fitting ALS model to Audioscrobbler (LastFM) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, \\\n",
    "StringType, Row\n",
    "from pyspark.ml.recommendation import ALS\n",
    "import pyspark.sql.functions as func\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.150.8.148:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x21acf8f49e8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Create paths to the data files. Add path to file with predictions for the test that will be calculated at the end of this notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to files\n",
    "artistdata_path = './data/artist_data.csv'\n",
    "userartist_path = './data/user_artist_data_train_small.csv'\n",
    "test_path = './data/LastFM_Test_Sample.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining schemas\n",
    "artistdata_struct = StructType([StructField('artistId', IntegerType()), \\\n",
    "                                StructField('name', StringType())])\n",
    "userartist_struct = StructType([StructField('userId', IntegerType()), \\\n",
    "                                StructField('artistId', IntegerType()), \\\n",
    "                                StructField('count', IntegerType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|artistId|              name|\n",
      "+--------+------------------+\n",
      "| 2000001|        Portishead|\n",
      "| 2000002|               Air|\n",
      "| 2000003|     Severed Heads|\n",
      "| 2000004|Marianne Faithfull|\n",
      "| 2000005|   Peace Orchestra|\n",
      "| 2000006|      Gallon Drunk|\n",
      "| 2000007|             Breed|\n",
      "| 2000008|         Omni Trio|\n",
      "| 2000009|    The Last Poets|\n",
      "| 2000010|    Rhythm & Sound|\n",
      "+--------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read artist names data\n",
    "artistdata_df = spark.read.csv(artistdata_path, sep = '\\t', schema = artistdata_struct)\n",
    "artistdata_df.cache()\n",
    "artistdata_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+\n",
      "| userId|artistId|count|\n",
      "+-------+--------+-----+\n",
      "|1000152| 2000001|   16|\n",
      "|1000152| 2000002|    6|\n",
      "|1000152| 2000011|    4|\n",
      "|1000152| 2000015|    3|\n",
      "|1000152| 2000023|   26|\n",
      "|1000152| 2000024|   24|\n",
      "|1000152| 2000026|   26|\n",
      "|1000152| 2000032|    3|\n",
      "|1000152| 2000039|   96|\n",
      "|1000152| 2000044|    3|\n",
      "+-------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read user-artist data\n",
    "userartist_df = spark.read.csv(userartist_path, sep = '\\t', schema = userartist_struct)\n",
    "userartist_df.cache()\n",
    "userartist_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "| userId|artistId|\n",
      "+-------+--------+\n",
      "|1000152| 2000024|\n",
      "|1000152| 2000137|\n",
      "|1000152| 2000170|\n",
      "|1000152| 2000173|\n",
      "|1000152| 2000254|\n",
      "|1000152| 2000275|\n",
      "|1000152| 2000277|\n",
      "|1000152| 2000414|\n",
      "|1000152| 2000606|\n",
      "|1000152| 2001006|\n",
      "+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split data:\n",
    "(training, test) = userartist_df.randomSplit([0.9, 0.1], seed=0)\n",
    "training.cache()\n",
    "# remove 'count' column from test:\n",
    "test = test.drop('count')\n",
    "test.cache()\n",
    "test.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting model\n",
    "\n",
    "Fit the ALS model. <br>\n",
    "Hyperparameters to specify: <br>\n",
    "\n",
    "-  `rank` between 5 and 40; default 10; the number of latent factors in the model\n",
    "-  `regParam` between 0.01 and 8; default 0.1; regularization parameter $\\lambda$\n",
    "-  `alpha` between 1 and 40; default 1; parameter $\\alpha$ appears in the expression for confidence $$c_{u,i}=1+\\alpha r_{u,i}$$ or $$c_{u,i}=1+\\alpha \\ln(1+\\frac{r_{u,i}}{\\epsilon}).$$ If $\\alpha=0$  confidence is always 1 regardless of rating$r_{u,i}$. As $\\alpha=0$ grows we pay more and more attention to how many times user $u$ consumed item $i$. Thus $\\alpha$ controls the relative weight of observed versus unobserved ratings. \n",
    "\n",
    "Search for hyperparameters on the grid of 4-5 values in each range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcast all artist ids\n",
    "allItemIDs = userartist_df.select('artistId').distinct().rdd.map(lambda x: x[0]).collect()\n",
    "bAllItemIDs = spark.sparkContext.broadcast(allItemIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcast 10000 most popular artist ids\n",
    "artists = userartist_df.groupBy('artistId') \\\n",
    ".agg(func.count(func.lit(1)).alias('num_of_users'))\n",
    "\n",
    "artists.cache()\n",
    "top_artists = artists. \\\n",
    "orderBy('num_of_users', ascending=False).limit(10000). \\\n",
    "rdd.map(lambda x: x['artistId']).collect()\n",
    "\n",
    "bTopItemIDs = spark.sparkContext.broadcast(top_artists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation of AUC is described in the book Advanced Analytics with Spark.\n",
    "\n",
    "In the calculation below parameter `positiveData` has the meaning of \"positive\" or \"good\" artist for the user. Parameter `predictFunction` is a function that takes user-item pairs and predicts estimated strength of interactions between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define meanAUC logic according to 'Advanced Analytics with Spark'\n",
    "\n",
    "def areaUnderCurve(positiveData, bAllItemIDs, predictFunction):\n",
    "    positivePredictions = predictFunction(positiveData.select(\"userId\", \"artistId\"))\\\n",
    "        .withColumnRenamed(\"prediction\", \"positivePrediction\")\n",
    "        \n",
    "    negativeData = positiveData.select(\"userId\", \"artistId\").rdd\\\n",
    "                    .groupByKey()\\\n",
    "                    .mapPartitions(lambda userIDAndPosItemIDs: \n",
    "                                   createNegativeItemSet(userIDAndPosItemIDs, \n",
    "                                                         bAllItemIDs))\\\n",
    "                    .flatMap(lambda x: x).map(lambda x: Row(userId=x[0], artistId=x[1])) \\\n",
    "                .toDF()\n",
    "    \n",
    "    negativePredictions = predictFunction(negativeData)\\\n",
    "        .withColumnRenamed(\"prediction\", \"negativePrediction\")\n",
    "\n",
    "    joinedPredictions = positivePredictions.join(negativePredictions, \"userId\")\\\n",
    "        .select(\"userId\", \"positivePrediction\", \"negativePrediction\").cache()\n",
    "        \n",
    "    allCounts = joinedPredictions\\\n",
    "        .groupBy(\"userId\").agg(func.count(func.lit(\"1\")).alias(\"total\"))\\\n",
    "        .select(\"userId\", \"total\")\n",
    "    correctCounts = joinedPredictions\\\n",
    "        .where(joinedPredictions.positivePrediction > joinedPredictions.negativePrediction)\\\n",
    "        .groupBy(\"userId\").agg(func.count(\"userId\").alias(\"correct\"))\\\n",
    "        .select(\"userId\", \"correct\")\n",
    "\n",
    "    joinedCounts = allCounts.join(correctCounts, \"userId\")\n",
    "    meanAUC = joinedCounts.select(\"userId\", (joinedCounts.correct / joinedCounts.total). \\\n",
    "                                  alias(\"auc\"))\\\n",
    "        .agg(func.mean(\"auc\")).first()\n",
    "\n",
    "    joinedPredictions.unpersist()\n",
    "\n",
    "    return meanAUC[0]\n",
    "\n",
    "\n",
    "def createNegativeItemSet(userIDAndPosItemIDs, bAllItemIDs):\n",
    "    allItemIDs = bAllItemIDs.value\n",
    "    return map(lambda x: getNegativeItemsForSingleUser(x[0], x[1], allItemIDs), \n",
    "               userIDAndPosItemIDs)\n",
    "\n",
    "\n",
    "def getNegativeItemsForSingleUser(userID, posItemIDs, allItemIDs):\n",
    "    posItemIDSet = set(posItemIDs)\n",
    "    negative = []\n",
    "    i = 0\n",
    "    # Keep about as many negative examples per user as positive.\n",
    "    # Duplicates are OK\n",
    "    while i < len(allItemIDs) and len(negative) < len(posItemIDSet):\n",
    "        itemID = random.choice(allItemIDs) \n",
    "        if itemID not in posItemIDSet:\n",
    "            negative.append(itemID)\n",
    "        i += 1\n",
    "    # Result is a collection of (user,negative-item) tuples\n",
    "    return map(lambda itemID: (userID, itemID), negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r= 5 a= 2 meanAUC = 0.742720019406539 for ALS-PREDICTION\n",
      "r= 5 a= 3 meanAUC = 0.7367667338981002 for ALS-PREDICTION\n",
      "r= 6 a= 2 meanAUC = 0.7386913106046608 for ALS-PREDICTION\n",
      "r= 6 a= 3 meanAUC = 0.7437800136573248 for ALS-PREDICTION\n"
     ]
    }
   ],
   "source": [
    "# building a model\n",
    "# Note that there are some hyperparameters, that should be fitted during cross-validation \n",
    "# (here we use default values for all hyperparameters but rank) \n",
    "for regParam in range(0.01,0.05,0.1,0.5,1):\n",
    "    for alpha in range(5,10,15,20):\n",
    "        for rank in range(5,10,15,20,25,30,35,40):\n",
    "            model = ALS(implicitPrefs=True, userCol=\"userId\", itemCol=\"artistId\", ratingCol=\"count\", \n",
    "                    rank=rank, alpha=alpha,regParam=regParam).fit(training)\n",
    "            # predict test data\n",
    "            predictions = model.transform(test)\n",
    "            predictions.cache()\n",
    "            predictions.take(3)\n",
    "            print('r=',rank,'a=',alpha,'rp=',regParam,'meanAUC =', areaUnderCurve(test, bTopItemIDs, model.transform), 'for ALS-PREDICTION')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
