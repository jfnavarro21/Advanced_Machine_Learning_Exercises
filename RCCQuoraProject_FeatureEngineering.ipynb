{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql.functions import isnan, when, count, length, lit, udf, col, struct\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import IDF, Tokenizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.53:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2e5c03cc7f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set files as train and test\n",
    "\n",
    "# sample sets for local computer\n",
    "trainFileName = \"./data/quora_train_1000.csv\"\n",
    "testFileName = \"./data/quora_test_1000.csv\"\n",
    "\n",
    "# large sets for RCC\n",
    "#trainFileName = \"data/quora_train.csv\"\n",
    "#testFileName = \"data/quora_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows = 1000\n",
      "+---+----+----+--------------------+--------------------+------------+\n",
      "| id|qid1|qid2|           question1|           question2|is_duplicate|\n",
      "+---+----+----+--------------------+--------------------+------------+\n",
      "|  0|   1|   2|What is the step ...|What is the step ...|           0|\n",
      "|  1|   3|   4|What is the story...|What would happen...|           0|\n",
      "|  2|   5|   6|How can I increas...|How can Internet ...|           0|\n",
      "|  3|   7|   8|Why am I mentally...|Find the remainde...|           0|\n",
      "|  4|   9|  10|Which one dissolv...|Which fish would ...|           0|\n",
      "|  5|  11|  12|Astrology: I am a...|I'm a triple Capr...|           1|\n",
      "+---+----+----+--------------------+--------------------+------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in training set, drop na's, return count and head\n",
    "sch = StructType([StructField('id',IntegerType()), \\\n",
    "                  StructField('qid1',IntegerType()),\\\n",
    "                  StructField('qid2',IntegerType()), \\\n",
    "                  StructField('question1',StringType()),\\\n",
    "                  StructField('question2',StringType()), \\\n",
    "                  StructField('is_duplicate',IntegerType())])\n",
    "train = spark.read.csv(trainFileName, header=True, escape='\"', \n",
    "                       quote='\"',schema=sch, multiLine = True)\n",
    "train = train.dropna()\n",
    "\n",
    "train.cache()\n",
    "print('Number of rows = %s' % train.count())\n",
    "train.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows = 1000\n",
      "+-------+--------------------+--------------------+\n",
      "|test_id|           question2|           question1|\n",
      "+-------+--------------------+--------------------+\n",
      "|      0|How do I show tha...|Emoticons: What g...|\n",
      "|      1|What is the scope...|Does ECE have a s...|\n",
      "|      2|What was the orig...|Why do prosecuted...|\n",
      "|      3|How  can someone ...|How do I grow tal...|\n",
      "|      4|Can weapons to pa...|What is the bigge...|\n",
      "|      5|What is the if I ...|What happens when...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in test set, drop nas, return count and head\n",
    "test = spark.read.csv(testFileName, header=True, escape='\"', \\\n",
    "                            encoding='utf8', multiLine = True)\n",
    "test = test.dropna()\n",
    "test.cache()\n",
    "print('Number of rows = %s' % test.count())\n",
    "test.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+\n",
      "|           question2|           question1|  id|\n",
      "+--------------------+--------------------+----+\n",
      "|How do I show tha...|Emoticons: What g...|1000|\n",
      "|What is the scope...|Does ECE have a s...|1001|\n",
      "|What was the orig...|Why do prosecuted...|1002|\n",
      "|How  can someone ...|How do I grow tal...|1003|\n",
      "|Can weapons to pa...|What is the bigge...|1004|\n",
      "+--------------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Drop unnecessary columns from train\n",
    "train = train.drop('qid1', 'qid2')\n",
    "#Create dataframe `test` with new column `id`\n",
    "maxTrainID = train.groupBy().max('id').collect()[0][0]\n",
    "test = test.withColumn(\"id\",(test.test_id+maxTrainID+1).cast(\"integer\")).drop('test_id')\n",
    "test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+------------+\n",
      "|           question2|           question1|  id|is_duplicate|\n",
      "+--------------------+--------------------+----+------------+\n",
      "|How do I show tha...|Emoticons: What g...|1000|          -1|\n",
      "|What is the scope...|Does ECE have a s...|1001|          -1|\n",
      "|What was the orig...|Why do prosecuted...|1002|          -1|\n",
      "|How  can someone ...|How do I grow tal...|1003|          -1|\n",
      "|Can weapons to pa...|What is the bigge...|1004|          -1|\n",
      "|What is the if I ...|What happens when...|1005|          -1|\n",
      "+--------------------+--------------------+----+------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add column is duplicate with -1 as a value for test\n",
    "test = test.withColumn('is_duplicate', lit(-1))\n",
    "test.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows = 2000\n",
      "+----+--------------------+--------------------+------------+\n",
      "|  id|           question1|           question2|is_duplicate|\n",
      "+----+--------------------+--------------------+------------+\n",
      "| 997|I and my girlfrie...|Why most of the c...|           0|\n",
      "| 998|Could we use cher...|Can we map the su...|           1|\n",
      "| 999|What is a good so...|Diving the Blue H...|           0|\n",
      "|1000|Emoticons: What g...|How do I show tha...|          -1|\n",
      "|1001|Does ECE have a s...|What is the scope...|          -1|\n",
      "|1002|Why do prosecuted...|What was the orig...|          -1|\n",
      "+----+--------------------+--------------------+------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join the train and test:  data\n",
    "data = train.union(test.select(train.columns))\n",
    "print('Number of rows = %s' % data.count())\n",
    "data.filter(data.id > 996).show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him']\n"
     ]
    }
   ],
   "source": [
    "# Define stop words\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "print(stop_words[:15])\n",
    "stop_words = set(stop_words)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmas_nltk and udf\n",
    "\n",
    "def lemmas_nltk(s):\n",
    "    return \" \".join([wordnet_lemmatizer.lemmatize(wordnet_lemmatizer.lemmatize(w,'n'),'v')\n",
    "                     for w in s.lower().split() if w.isalpha() & (not w in stop_words)])\n",
    "lemmas_nltk_udf = udf(lemmas_nltk, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcount function and udf\n",
    "def wordsCount(str): return str.count(' ')+1\n",
    "wordsCount_udf = udf(wordsCount, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio function and udf\n",
    "def ratio(x,y): return abs(x-y)/(x+y+1e-15) ############## divide by zero occures!!! ###########################\n",
    "ratio_udf = udf(ratio, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common ngrams function and udf\n",
    "import re\n",
    "regex = re.compile('([^\\s\\w]|_)+')\n",
    "def commonNgrams(s1, s2, n):\n",
    "    return len(set(nltk.ngrams(regex.sub('', s1).lower().split(), n))\n",
    "                & set(nltk.ngrams(regex.sub('', s2).lower().split(), n)) )\n",
    "commonNgrams_udf = udf(commonNgrams, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigram_ratio function and udf\n",
    "def unigram_ratio(ngrams, n1, n2):\n",
    "    return ngrams/(1+max(n1, n2))\n",
    "unigram_ratio_udf = udf(unigram_ratio, DoubleType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squared distance function\n",
    "def tfidfDist(a,b): return float(a.squared_distance(b))\n",
    "dist_udf = udf(tfidfDist, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project description\n",
    "#For the first part of this project create the following set of features:\n",
    "featureNames = ['lWCount1', 'lWCount2',\n",
    "                'qWCount1', 'qWCount2',\n",
    "                'lLen1', 'lLen2',\n",
    "                'qLen1', 'qLen2',\n",
    "                'lWCount_ratio', 'qWCount_ratio',\n",
    "                'lLen_ratio', 'qLen_ratio',\n",
    "                'qNgrams_1', 'qNgrams_2', 'qNgrams_3', \n",
    "                'lNgrams_1', 'lNgrams_2', 'lNgrams_3', \n",
    "                'qUnigram_ratio', 'lUnigram_ratio', \n",
    "                'tfidfDistance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------------+\n",
      "| id|           question1|           question2|is_duplicate|\n",
      "+---+--------------------+--------------------+------------+\n",
      "|  0|What is the step ...|What is the step ...|           0|\n",
      "|  1|What is the story...|What would happen...|           0|\n",
      "|  2|How can I increas...|How can Internet ...|           0|\n",
      "|  3|Why am I mentally...|Find the remainde...|           0|\n",
      "|  4|Which one dissolv...|Which fish would ...|           0|\n",
      "|  5|Astrology: I am a...|I'm a triple Capr...|           1|\n",
      "+---+--------------------+--------------------+------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show starting data frame\n",
    "data.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|              lemma1|              lemma2|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|step step guide i...|step step guide i...|\n",
      "|  1|      story kohinoor|would happen indi...|\n",
      "|  2|increase speed in...|internet speed in...|\n",
      "|  3|      mentally solve|find remainder di...|\n",
      "|  4|one dissolve wate...|fish would surviv...|\n",
      "|  5|capricorn sun cap...|triple capricorn ...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lemmatization\n",
    "for i in [\"1\",\"2\"]:\n",
    "    data = data.withColumn('lemma'+i, lemmas_nltk_udf(data[\"question\"+i]))\n",
    "\n",
    "data.select('id','lemma1','lemma2').show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors \\\n",
    ".load_word2vec_format('GoogleNews-vectors-negative300.bin',\n",
    "                      binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Reykjavik', 0.6050446629524231)\n",
      "('queen', 0.7118192911148071)\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['capital','Iceland'])[0])\n",
    "print(model.most_similar(positive=['woman', 'king'],\n",
    "                         negative=['man'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['dogs', 'having', 'lower', 'traditional']\n",
      "[-0.02835612 -0.01235805 -0.0893375   0.1272655  -0.03042328]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print('Words: %s' %words)\n",
    "M = []\n",
    "for w in words:\n",
    "    try: M.append(model[w])\n",
    "    except: continue\n",
    "M = np.array(M)\n",
    "if len(M)==0: question2vec = np.zeros(300)\n",
    "else: \n",
    "    question2vec = M.sum(axis=0)\n",
    "    norm = np.sqrt((question2vec ** 2).sum())\n",
    "    if norm>0: question2vec /= norm \n",
    "\n",
    "print(question2vec[:5])\n",
    "len(question2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quest2vec(words):\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try: M.append(model[w])\n",
    "        except: continue\n",
    "    M = np.array(M)\n",
    "    if len(M)==0: question2vec = np.zeros(300)\n",
    "    else: \n",
    "        question2vec = M.sum(axis=0)\n",
    "        norm = np.sqrt((question2vec ** 2).sum())\n",
    "        if norm>0: question2vec /= norm \n",
    "    return(question2vec[:5])\n",
    "q2v_udf = udf(quest2vec, DoubleType())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def questdiff(a,b): return float(a.squared_distance(b))\n",
    "dist_udf = udf(questdiff, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-1830c7e22785>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdata2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [\"1\",\"2\"]:\n",
    "    data2 = data.withColumn('q2v'+i, q2v_udf(data['lemma'+i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['dogs', 'pets', 'dog', 'puppy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01432754,  0.00739102, -0.0632403 ,  0.06878085, -0.04557976], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quest2vec(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+-----+-----+-----+-----+\n",
      "|lWCount1|lWCount2|qWCount1|qWCount2|lLen1|lLen2|qLen1|qLen2|\n",
      "+--------+--------+--------+--------+-----+-----+-----+-----+\n",
      "|       6|       5|      14|      12|   35|   28|   66|   57|\n",
      "|       2|       7|       8|      13|   14|   53|   51|   88|\n",
      "|       5|       4|      14|      10|   38|   28|   73|   59|\n",
      "|       2|       3|      11|       9|   14|   21|   50|   65|\n",
      "|       7|       4|      13|       7|   43|   23|   76|   39|\n",
      "|       6|       5|      16|      16|   30|   35|   86|   90|\n",
      "+--------+--------+--------+--------+-----+-----+-----+-----+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add word counts and lengths\n",
    "for i in [\"1\",\"2\"]:\n",
    "    data = data.withColumn('lWCount'+i, wordsCount_udf(data['lemma'+i]))\n",
    "    data = data.withColumn('qWCount'+i, wordsCount_udf(data['question'+i]))\n",
    "    data = data.withColumn('lLen'+i, length(data['lemma'+i]))\n",
    "    data = data.withColumn('qLen'+i, length(data['question'+i]))\n",
    "data.select('lWCount1','lWCount2','qWCount1','qWCount2',\n",
    "            'lLen1','lLen2','qLen1','qLen2').show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+--------------------+\n",
      "|      lWCount_ratio|      qWCount_ratio|         lLen_ratio|          qLen_ratio|\n",
      "+-------------------+-------------------+-------------------+--------------------+\n",
      "| 0.0909090909090909|0.07692307692307693| 0.1111111111111111| 0.07317073170731707|\n",
      "| 0.5555555555555555|0.23809523809523808|  0.582089552238806| 0.26618705035971224|\n",
      "|0.11111111111111109|0.16666666666666666|0.15151515151515152| 0.10606060606060606|\n",
      "|0.19999999999999996|                0.1|                0.2| 0.13043478260869565|\n",
      "| 0.2727272727272727|                0.3|0.30303030303030304|  0.3217391304347826|\n",
      "| 0.0909090909090909|                0.0|0.07692307692307693|0.022727272727272728|\n",
      "+-------------------+-------------------+-------------------+--------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add ratios for counts and ratios for lengths\n",
    "data = data.withColumn('lWCount_ratio', ratio_udf(data['lWCount1'],data['lWCount2']))\n",
    "data = data.withColumn('qWCount_ratio', ratio_udf(data['qWCount1'],data['qWCount2']))\n",
    "data = data.withColumn('lLen_ratio', ratio_udf(data['lLen1'],data['lLen2']))\n",
    "data = data.withColumn('qLen_ratio', ratio_udf(data['qLen1'],data['qLen2']))\n",
    "data.select('lWCount_ratio','qWCount_ratio','lLen_ratio','qLen_ratio').show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+---------+---------+-------------------+-------------------+\n",
      "|lNgrams_1|lNgrams_2|lNgrams_3|qNgrams_1|qNgrams_2|qNgrams_3|     qUnigram_ratio|     lUnigram_ratio|\n",
      "+---------+---------+---------+---------+---------+---------+-------------------+-------------------+\n",
      "|        4|        4|        3|       11|       11|       10| 0.7333333333333333| 0.5714285714285714|\n",
      "|        1|        0|        0|        4|        2|        1| 0.2857142857142857|              0.125|\n",
      "|        3|        0|        0|        4|        1|        0|0.26666666666666666|                0.5|\n",
      "|        0|        0|        0|        0|        0|        0|                0.0|                0.0|\n",
      "|        0|        0|        0|        4|        0|        0| 0.2857142857142857|                0.0|\n",
      "|        3|        0|        0|        9|        4|        1| 0.5294117647058824|0.42857142857142855|\n",
      "+---------+---------+---------+---------+---------+---------+-------------------+-------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#N-grams and n-gram ratios (1,2,3)\n",
    "for i in [1, 2, 3]:\n",
    "    data = data.withColumn(\"qNgrams_\"+str(i),commonNgrams_udf \\\n",
    "                           (data['question1'],data['question2'],lit(i)))\n",
    "    data = data.withColumn(\"lNgrams_\"+str(i),commonNgrams_udf \\\n",
    "                           (data['lemma1'],data['lemma2'],lit(i)))\n",
    "data = data.withColumn('qUnigram_ratio', unigram_ratio_udf \\\n",
    "                       (data.qNgrams_1,data.qWCount1,data.qWCount2))\n",
    "data = data.withColumn('lUnigram_ratio', unigram_ratio_udf \\\n",
    "                       (data.lNgrams_1,data.lWCount1,data.lWCount2))\n",
    "data.select('lNgrams_1','lNgrams_2','lNgrams_3', \\\n",
    "            'qNgrams_1','qNgrams_2','qNgrams_3', \\\n",
    "            'qUnigram_ratio','lUnigram_ratio').show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|              lemma1|              words1|              lemma2|              words2|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0|step step guide i...|[step, step, guid...|step step guide i...|[step, step, guid...|\n",
      "|  1|      story kohinoor|   [story, kohinoor]|would happen indi...|[would, happen, i...|\n",
      "|  2|increase speed in...|[increase, speed,...|internet speed in...|[internet, speed,...|\n",
      "|  3|      mentally solve|   [mentally, solve]|find remainder di...|[find, remainder,...|\n",
      "|  4|one dissolve wate...|[one, dissolve, w...|fish would surviv...|[fish, would, sur...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add tokenized lemmas :words1, words2\n",
    "tokenizer = Tokenizer(inputCol=\"lemma1\", outputCol=\"words1\")\n",
    "data = tokenizer.transform(data)\n",
    "tokenizer.setParams(inputCol=\"lemma2\", outputCol=\"words2\")\n",
    "data = tokenizer.transform(data)\n",
    "data.select('id','lemma1','words1','lemma2','words2').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               words|\n",
      "+--------------------+\n",
      "|[best, available,...|\n",
      "|     [contact, good]|\n",
      "|[convince, people...|\n",
      "|        [creativity]|\n",
      "|        [find, baby]|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|               words|                  tf|\n",
      "+--------------------+--------------------+\n",
      "|[best, available,...|(2088,[0,273,1338...|\n",
      "|     [contact, good]|(2088,[4,578],[1....|\n",
      "|[convince, people...|(2088,[10,71,1088...|\n",
      "|        [creativity]|        (2088,[],[])|\n",
      "|        [find, baby]|(2088,[24,965],[1...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CountVectorizerModel has a vocabulary of length  2088\n",
      "+---+--------------------+--------------------+\n",
      "| id|               words|                  tf|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|[step, step, guid...|(2088,[178,206,22...|\n",
      "|  1|   [story, kohinoor]|(2088,[522,1562],...|\n",
      "|  2|[increase, speed,...|(2088,[7,75,251,3...|\n",
      "|  3|   [mentally, solve]|(2088,[1060,1157]...|\n",
      "|  4|[one, dissolve, w...|(2088,[11,270,115...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+--------------------+--------------------+\n",
      "| id|               words|                  tf|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|[step, step, guid...|(2088,[178,226,35...|\n",
      "|  1|[would, happen, i...|(2088,[8,26,45,15...|\n",
      "|  2|[internet, speed,...|(2088,[75,145,251...|\n",
      "|  3|[find, remainder,...|   (2088,[24],[1.0])|\n",
      "|  4|[fish, would, sur...|(2088,[8,1294,172...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create corpus \n",
    "corpus = data.selectExpr('words1 as words').join(data.selectExpr('words2 as words'), on='words', how='full')\n",
    "corpus.show(5)\n",
    "# initialize CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"tf\", minDF=2.0)\n",
    "# Fit a countVectorizerModel to corpus\n",
    "cvModel = cv.fit(corpus)\n",
    "corpus = cvModel.transform(corpus)\n",
    "corpus.show(5)\n",
    "# Check # of wods in new vocabulary\n",
    "print('CountVectorizerModel has a vocabulary of length ',len(cvModel.vocabulary))\n",
    "# Apply CountVectorizerModel.transform () to question 1 and 2\n",
    "res1 = cvModel.transform(data.selectExpr('id', 'words1 as words'))\n",
    "res2 = cvModel.transform(data.selectExpr('id', 'words2 as words'))\n",
    "res1.show(5)\n",
    "res2.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "| id|               words|                  tf|                 idf|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  0|[step, step, guid...|(2088,[178,206,22...|(2088,[178,206,22...|\n",
      "|  1|   [story, kohinoor]|(2088,[522,1562],...|(2088,[522,1562],...|\n",
      "|  2|[increase, speed,...|(2088,[7,75,251,3...|(2088,[7,75,251,3...|\n",
      "|  3|   [mentally, solve]|(2088,[1060,1157]...|(2088,[1060,1157]...|\n",
      "|  4|[one, dissolve, w...|(2088,[11,270,115...|(2088,[11,270,115...|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "| id|               words|                  tf|                 idf|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  0|[step, step, guid...|(2088,[178,226,35...|(2088,[178,226,35...|\n",
      "|  1|[would, happen, i...|(2088,[8,26,45,15...|(2088,[8,26,45,15...|\n",
      "|  2|[internet, speed,...|(2088,[75,145,251...|(2088,[75,145,251...|\n",
      "|  3|[find, remainder,...|   (2088,[24],[1.0])|(2088,[24],[4.271...|\n",
      "|  4|[fish, would, sur...|(2088,[8,1294,172...|(2088,[8,1294,172...|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate idf\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"idf\")\n",
    "idfModel = idf.fit(corpus)\n",
    "res1 = idfModel.transform(res1)\n",
    "res2 = idfModel.transform(res2)\n",
    "res1.show(5)\n",
    "res2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------------------+\n",
      "| id|                idf1|                idf2|              dist|\n",
      "+---+--------------------+--------------------+------------------+\n",
      "|  0|(2088,[178,206,22...|(2088,[178,226,35...|31.436004150870346|\n",
      "|  1|(2088,[522,1562],...|(2088,[8,26,45,15...|121.45324418057099|\n",
      "|  2|(2088,[7,75,251,3...|(2088,[75,145,251...| 94.38709182458246|\n",
      "|  3|(2088,[1060,1157]...|(2088,[24],[4.271...| 114.2573847509046|\n",
      "|  4|(2088,[11,270,115...|(2088,[8,1294,172...| 262.3278971642576|\n",
      "|  5|(2088,[81,588,998...|(2088,[81,588,172...| 48.00463629773798|\n",
      "+---+--------------------+--------------------+------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# temporary dataframe containing idf1 and idf2 :res\n",
    "res = res1.selectExpr('id','idf as idf1').join(res2.selectExpr('id','idf as idf2'), on='id', how='inner')\n",
    "# Calculate the distance\n",
    "res = res.withColumn('dist', dist_udf(res['idf1'], res['idf2']))\n",
    "res.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|     tfidfDistance|\n",
      "+---+------------------+\n",
      "|  0|31.436004150870346|\n",
      "|  1|121.45324418057099|\n",
      "|  2| 94.38709182458246|\n",
      "|  3| 114.2573847509046|\n",
      "|  4| 262.3278971642576|\n",
      "|  5| 48.00463629773798|\n",
      "+---+------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop columns from data and join in tfidfDistance \n",
    "data = data.drop('words1', 'words2')\n",
    "data = data.join(res.selectExpr('id','dist as tfidfDistance'),on='id',how='inner')\n",
    "data.select('id','tfidfDistance').show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------------+--------------------+--------------------+--------+--------+-----+-----+--------+--------+-----+-----+------------------+-------------------+------------------+-------------------+---------+---------+---------+---------+---------+---------+------------------+------------------+------------------+\n",
      "| id|           question1|           question2|is_duplicate|              lemma1|              lemma2|lWCount1|qWCount1|lLen1|qLen1|lWCount2|qWCount2|lLen2|qLen2|     lWCount_ratio|      qWCount_ratio|        lLen_ratio|         qLen_ratio|qNgrams_1|lNgrams_1|qNgrams_2|lNgrams_2|qNgrams_3|lNgrams_3|    qUnigram_ratio|    lUnigram_ratio|     tfidfDistance|\n",
      "+---+--------------------+--------------------+------------+--------------------+--------------------+--------+--------+-----+-----+--------+--------+-----+-----+------------------+-------------------+------------------+-------------------+---------+---------+---------+---------+---------+---------+------------------+------------------+------------------+\n",
      "|  0|What is the step ...|What is the step ...|           0|step step guide i...|step step guide i...|       6|      14|   35|   66|       5|      12|   28|   57|0.0909090909090909|0.07692307692307693|0.1111111111111111|0.07317073170731707|       11|        4|       11|        4|       10|        3|0.7333333333333333|0.5714285714285714|31.436004150870346|\n",
      "|  1|What is the story...|What would happen...|           0|      story kohinoor|would happen indi...|       2|       8|   14|   51|       7|      13|   53|   88|0.5555555555555555|0.23809523809523808| 0.582089552238806|0.26618705035971224|        4|        1|        2|        0|        1|        0|0.2857142857142857|             0.125|121.45324418057099|\n",
      "+---+--------------------+--------------------+------------+--------------------+--------------------+--------+--------+-----+-----+--------+--------+-----+-----+------------------+-------------------+------------------+-------------------+---------+---------+---------+---------+---------+---------+------------------+------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+--------+--------+-----+-----+-----+-----+\n",
      "| id|lWCount1|lWCount2|qWCount1|qWCount2|lLen1|lLen2|qLen1|qLen2|\n",
      "+---+--------+--------+--------+--------+-----+-----+-----+-----+\n",
      "|  0|       6|       5|      14|      12|   35|   28|   66|   57|\n",
      "|  1|       2|       7|       8|      13|   14|   53|   51|   88|\n",
      "|  2|       5|       4|      14|      10|   38|   28|   73|   59|\n",
      "|  3|       2|       3|      11|       9|   14|   21|   50|   65|\n",
      "|  4|       7|       4|      13|       7|   43|   23|   76|   39|\n",
      "|  5|       6|       5|      16|      16|   30|   35|   86|   90|\n",
      "+---+--------+--------+--------+--------+-----+-----+-----+-----+\n",
      "only showing top 6 rows\n",
      "\n",
      "+---+-------------------+-------------------+-------------------+--------------------+\n",
      "| id|      lWCount_ratio|      qWCount_ratio|         lLen_ratio|          qLen_ratio|\n",
      "+---+-------------------+-------------------+-------------------+--------------------+\n",
      "|  0| 0.0909090909090909|0.07692307692307693| 0.1111111111111111| 0.07317073170731707|\n",
      "|  1| 0.5555555555555555|0.23809523809523808|  0.582089552238806| 0.26618705035971224|\n",
      "|  2|0.11111111111111109|0.16666666666666666|0.15151515151515152| 0.10606060606060606|\n",
      "|  3|0.19999999999999996|                0.1|                0.2| 0.13043478260869565|\n",
      "|  4| 0.2727272727272727|                0.3|0.30303030303030304|  0.3217391304347826|\n",
      "|  5| 0.0909090909090909|                0.0|0.07692307692307693|0.022727272727272728|\n",
      "+---+-------------------+-------------------+-------------------+--------------------+\n",
      "only showing top 6 rows\n",
      "\n",
      "+---+---------+---------+---------+---------+---------+---------+\n",
      "| id|qNgrams_1|qNgrams_2|qNgrams_3|lNgrams_1|lNgrams_2|lNgrams_3|\n",
      "+---+---------+---------+---------+---------+---------+---------+\n",
      "|  0|       11|       11|       10|        4|        4|        3|\n",
      "|  1|        4|        2|        1|        1|        0|        0|\n",
      "|  2|        4|        1|        0|        3|        0|        0|\n",
      "|  3|        0|        0|        0|        0|        0|        0|\n",
      "|  4|        4|        0|        0|        0|        0|        0|\n",
      "|  5|        9|        4|        1|        3|        0|        0|\n",
      "+---+---------+---------+---------+---------+---------+---------+\n",
      "only showing top 6 rows\n",
      "\n",
      "+---+-------------------+-------------------+------------------+\n",
      "| id|     qUnigram_ratio|     lUnigram_ratio|     tfidfDistance|\n",
      "+---+-------------------+-------------------+------------------+\n",
      "|  0| 0.7333333333333333| 0.5714285714285714|31.436004150870346|\n",
      "|  1| 0.2857142857142857|              0.125|121.45324418057099|\n",
      "|  2|0.26666666666666666|                0.5| 94.38709182458246|\n",
      "|  3|                0.0|                0.0| 114.2573847509046|\n",
      "|  4| 0.2857142857142857|                0.0| 262.3278971642576|\n",
      "|  5| 0.5294117647058824|0.42857142857142855| 48.00463629773798|\n",
      "+---+-------------------+-------------------+------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# return first row of dataframe\n",
    "data = data.select(['id']+featureNames+['is_duplicate'])\n",
    "data = data.cache()\n",
    "data.select(['id']+featureNames[:8]).show(6)\n",
    "data.select(['id']+featureNames[8:12]).show(6)\n",
    "data.select(['id']+featureNames[12:18]).show(6)\n",
    "data.select(['id']+featureNames[18:]).show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train and test features to separate .csv files\n",
    "outData = data.select(['id']+featureNames+['is_duplicate'])\n",
    "outData = outData.cache()\n",
    "outTrainFileName = \"train_features2.csv\"\n",
    "outTestFileName = \"test_features2.csv\"\n",
    "outData.filter(outData.id <= maxTrainID).\\\n",
    "coalesce(1).write.csv(outTrainFileName,header=True,mode='overwrite',quote=\"\")\n",
    "outData.filter(outData.id > maxTrainID).withColumn('id', outData.id-maxTrainID-1).\\\n",
    "coalesce(1).write.csv(outTestFileName,header=True,mode='overwrite',quote=\"\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
