{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning (MScA, 32017)\n",
    "\n",
    "# Project Paraphrase Detection\n",
    "\n",
    "### Yuri Balasanov, Leonid Nazarov, Andrey Kobyshev, &copy; iLykei 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "832eebdc-9bd3-7439-eac3-548f4730dc3e"
   },
   "source": [
    "This project is based on [Quora Question Pairs](https://www.kaggle.com/c/quora-question-pairs) competition held on the Kaggle platform in spring 2017. It appeared to be one of the most popular competitions in the Kaggle history. <br>\n",
    "The task was to detect duplicates among pairs of questions. <br>\n",
    "The problem is also known as **paraphrase detection**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql.functions import isnan, when, count, length, lit, udf, col, struct\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.150.1.178:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x17c6b3939b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structure\n",
    "\n",
    "Look at the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFileName = \"./data/quora_train_1000.csv\"\n",
    "testFileName = \"./data/quora_test_1000.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files contain 1000 rows with data. <br>\n",
    "Both these files are short versions of the project files. They can be analyzed in local spark environments. <br>\n",
    "\n",
    "Create chema, read train data file, remove records with missing observations and cache it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows = 1000\n",
      "+---+----+----+--------------------+--------------------+------------+\n",
      "| id|qid1|qid2|           question1|           question2|is_duplicate|\n",
      "+---+----+----+--------------------+--------------------+------------+\n",
      "|  0|   1|   2|What is the step ...|What is the step ...|           0|\n",
      "|  1|   3|   4|What is the story...|What would happen...|           0|\n",
      "|  2|   5|   6|How can I increas...|How can Internet ...|           0|\n",
      "|  3|   7|   8|Why am I mentally...|Find the remainde...|           0|\n",
      "|  4|   9|  10|Which one dissolv...|Which fish would ...|           0|\n",
      "|  5|  11|  12|Astrology: I am a...|I'm a triple Capr...|           1|\n",
      "+---+----+----+--------------------+--------------------+------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sch = StructType([StructField('id',IntegerType()), \\\n",
    "                  StructField('qid1',IntegerType()),\\\n",
    "                  StructField('qid2',IntegerType()), \\\n",
    "                  StructField('question1',StringType()),\\\n",
    "                  StructField('question2',StringType()), \\\n",
    "                  StructField('is_duplicate',IntegerType())])\n",
    "train = spark.read.csv(trainFileName, header=True, escape='\"', \n",
    "                       quote='\"',schema=sch, multiLine = True)\n",
    "train = train.dropna()\n",
    "\n",
    "train.cache()\n",
    "print('Number of rows = %s' % train.count())\n",
    "train.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data fields  \n",
    "\n",
    "*id* - the id of a training set question pair  \n",
    "*qid1, qid2* - unique ids of each question (only available in train.csv)  \n",
    "*question1, question2* - the full text of each question  \n",
    "*is_duplicate* - the target variable, set to 1 if question1 and question2 have essentially the same meaning, and 0 otherwise.\n",
    "\n",
    "See examples of non-duplicate and duplicate questions below.\n",
    "\n",
    "Pair 5: non-duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(question1='Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?')\n",
      "Row(question2='Which fish would survive in salt water?')\n"
     ]
    }
   ],
   "source": [
    "print(train.select('question1').collect()[4])\n",
    "print(train.select('question2').collect()[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pair 6: duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(question1='Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?')\n",
      "Row(question2=\"I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\")\n"
     ]
    }
   ],
   "source": [
    "print(train.select('question1').collect()[5])\n",
    "print(train.select('question2').collect()[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the test file, remove records with missing observations, cache it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows = 1000\n",
      "+-------+--------------------+--------------------+\n",
      "|test_id|           question2|           question1|\n",
      "+-------+--------------------+--------------------+\n",
      "|      0|How do I show tha...|Emoticons: What g...|\n",
      "|      1|What is the scope...|Does ECE have a s...|\n",
      "|      2|What was the orig...|Why do prosecuted...|\n",
      "|      3|How  can someone ...|How do I grow tal...|\n",
      "|      4|Can weapons to pa...|What is the bigge...|\n",
      "|      5|What is the if I ...|What happens when...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = spark.read.csv(testFileName, header=True, escape='\"', \\\n",
    "                            encoding='utf8', multiLine = True)\n",
    "test = test.dropna()\n",
    "test.cache()\n",
    "print('Number of rows = %s' % test.count())\n",
    "test.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that column *is_duplicate* is not present in test file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes we need to join both dataframes. <br>\n",
    "Drop columns that we don't need from `'train'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('qid1', 'qid2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Add to `test` a new column `id` which has integer values of `'test_id'` increased by maximum value of column `'id'` from `train`.\n",
    "Then remove column `'test_id'` from test.\n",
    "\n",
    "+--------------------+--------------------+----+ <br>\n",
    "|           question2|           question1|  id| <br>\n",
    "+--------------------+--------------------+----+ <br>\n",
    "|How do I show tha...|Emoticons: What g...|1000| <br>\n",
    "|What is the scope...|Does ECE have a s...|1001| <br>\n",
    "|What was the orig...|Why do prosecuted...|1002| <br>\n",
    "|How  can someone ...|How do I grow tal...|1003| <br>\n",
    "|Can weapons to pa...|What is the bigge...|1004| <br>\n",
    "+--------------------+--------------------+----+ <br>\n",
    "\n",
    "**Hint.** Use \n",
    "\n",
    "`.groupBy().max()`\n",
    "\n",
    "and \n",
    "\n",
    "`withColumn()`\n",
    "\n",
    "as the following example shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  X|  Y|\n",
      "+---+---+\n",
      "|0.0|1.0|\n",
      "|1.0|2.0|\n",
      "|2.0|3.0|\n",
      "|3.0|4.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a toy dataframe\n",
    "from pyspark.sql import Row\n",
    "df = sqlContext.createDataFrame([ \\\n",
    "     Row(_1=0.0, _2=1.0), \\\n",
    "     Row(_1=1.0, _2=2.0), \\\n",
    "     Row(_1=2.0,_2=3.0), \\\n",
    "     Row(_1=3.0,_2=4.0)],[\"X\",\"Y\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|max(Y)|\n",
      "+------+\n",
      "|   4.0|\n",
      "+------+\n",
      "\n",
      "maxY = 4.0\n"
     ]
    }
   ],
   "source": [
    "# Find maximum of column Y\n",
    "df.groupBy().max('Y').show()\n",
    "maxY=df.groupBy().max('Y').collect()[0][0]\n",
    "print('maxY = %s' % maxY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy().max('Y').collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  X|  Y|new|\n",
      "+---+---+---+\n",
      "|0.0|1.0|  4|\n",
      "|1.0|2.0|  5|\n",
      "|2.0|3.0|  6|\n",
      "|3.0|4.0|  7|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create new column\n",
    "df.withColumn(\"new\",(df.X+maxY).cast(\"integer\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert code in the following cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+\n",
      "|           question2|           question1|  id|\n",
      "+--------------------+--------------------+----+\n",
      "|How do I show tha...|Emoticons: What g...|1000|\n",
      "|What is the scope...|Does ECE have a s...|1001|\n",
      "|What was the orig...|Why do prosecuted...|1002|\n",
      "|How  can someone ...|How do I grow tal...|1003|\n",
      "|Can weapons to pa...|What is the bigge...|1004|\n",
      "+--------------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxTrainID=train.groupBy().max('id').collect()[0][0]\n",
    "test=test.withColumn(\"id\",(test.test_id+maxTrainID+1).cast(\"integer\")).drop('test_id')\n",
    "test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+\n",
      "|           question2|           question1|  id|\n",
      "+--------------------+--------------------+----+\n",
      "|How do I show tha...|Emoticons: What g...|1000|\n",
      "|What is the scope...|Does ECE have a s...|1001|\n",
      "|What was the orig...|Why do prosecuted...|1002|\n",
      "|How  can someone ...|How do I grow tal...|1003|\n",
      "|Can weapons to pa...|What is the bigge...|1004|\n",
      "+--------------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxTrainID = train.groupBy().max('id').collect()[0][0]\n",
    "test = test.withColumn(\"id\",(test.test_id+maxTrainID+1).cast(\"integer\")).drop('test_id')\n",
    "test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add column `'is_duplicate'` containing vaues `-1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+------------+\n",
      "|           question2|           question1|  id|is_duplicate|\n",
      "+--------------------+--------------------+----+------------+\n",
      "|How do I show tha...|Emoticons: What g...|1000|          -1|\n",
      "|What is the scope...|Does ECE have a s...|1001|          -1|\n",
      "|What was the orig...|Why do prosecuted...|1002|          -1|\n",
      "|How  can someone ...|How do I grow tal...|1003|          -1|\n",
      "|Can weapons to pa...|What is the bigge...|1004|          -1|\n",
      "|What is the if I ...|What happens when...|1005|          -1|\n",
      "+--------------------+--------------------+----+------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = test.withColumn('is_duplicate', lit(-1))\n",
    "test.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now join both dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows = 2000\n",
      "+----+--------------------+--------------------+------------+\n",
      "|  id|           question1|           question2|is_duplicate|\n",
      "+----+--------------------+--------------------+------------+\n",
      "| 997|I and my girlfrie...|Why most of the c...|           0|\n",
      "| 998|Could we use cher...|Can we map the su...|           1|\n",
      "| 999|What is a good so...|Diving the Blue H...|           0|\n",
      "|1000|Emoticons: What g...|How do I show tha...|          -1|\n",
      "|1001|Does ECE have a s...|What is the scope...|          -1|\n",
      "|1002|Why do prosecuted...|What was the orig...|          -1|\n",
      "+----+--------------------+--------------------+------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = train.union(test.select(train.columns))\n",
    "print('Number of rows = %s' % data.count())\n",
    "data.filter(data.id > 996).show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------------+\n",
      "| id|           question1|           question2|is_duplicate|\n",
      "+---+--------------------+--------------------+------------+\n",
      "|  0|What is the step ...|What is the step ...|           0|\n",
      "|  1|What is the story...|What would happen...|           0|\n",
      "|  2|How can I increas...|How can Internet ...|           0|\n",
      "|  3|Why am I mentally...|Find the remainde...|           0|\n",
      "|  4|Which one dissolv...|Which fish would ...|           0|\n",
      "|  5|Astrology: I am a...|I'm a triple Capr...|           1|\n",
      "+---+--------------------+--------------------+------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Python and Spark natural language processing tools\n",
    "\n",
    "From the very beginnig Spark platform was created largely for processing large number of text documents. <br>\n",
    "So, there is significant number of advanced tools for NLP.\n",
    "\n",
    "Here is the list of the major Python Natural Language Processing (NLP) packages:\n",
    "\n",
    "- [Natural Language Toolkit](http://www.nltk.org/) (NLTK)\n",
    "- [TextBlob](https://textblob.readthedocs.io/en/dev/) is built on top of NLTK, and it's more easily-accessible.\n",
    "- [Stanford's CoreNLP](https://stanfordnlp.github.io/CoreNLP/) is a Java library with Python wrappers. It presents in many existing production systems due to its speed.\n",
    "- [SpaCy](https://spacy.io/) is a new NLP library that's designed to be fast, streamlined, and production-ready. It's not as widely adopted, but if you're building a new application, you should give it a try.\n",
    "- [Gensim](https://radimrehurek.com/gensim/) is most commonly used for topic modeling and similarity detection. It's not a general-purpose NLP library, but for the tasks it does handle, it does them well.\n",
    "\n",
    "Some of these tools are also available as part of Spark ML. <br>\n",
    "\n",
    "Below there is a description of some important methods of creating features for natural text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the procedure of chopping document into pieces, called **tokens**. <br>\n",
    "In the process of tokenization some characters, such as punctuation, are usually removed. <br>\n",
    "Tokens may be words or sentences. <br>\n",
    "\n",
    "Below is an example from NLTK.\n",
    "\n",
    "Download a popular collection of packages from *nltk* which includes some useful methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]    |     .\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods *nltk.sent_tokenize* and *word_tokenize* belong to Penn Treebank Tokenizer module which is one module in the popular collection downloaded above.\n",
    "\n",
    "Syntax:\n",
    "\n",
    "`sent_tokenize(text, language='english')` <br>\n",
    "`word_tokenize(text, language='english', preserve_line=False)` <br>\n",
    "\n",
    "*sent_tokenize* returns sentense-tokenized copy of *text*. <br>\n",
    "*word_tokenize* returns words-tokenized copy of *text*. Text may be previously sentense_tokenized or not. <br> Parameter *preserve_line=True* keeps words-tokenized sentenses separate in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens sentences: ['Why am I mentally very lonely?', 'How can I solve it?']\n",
      "Tokens words: ['How', 'can', 'I', 'solve', 'it', '?']\n",
      "Tokens words1: ['Why', 'am', 'I', 'mentally', 'very', 'lonely', '?', 'How', 'can', 'I', 'solve', 'it', '?']\n"
     ]
    }
   ],
   "source": [
    "sents = nltk.sent_tokenize(data.filter(data.id == 3) \\\n",
    "                           .select('question1').collect()[0][0])\n",
    "print('Tokens sentences: %s' % sents)\n",
    "words = nltk.word_tokenize(sents[1])\n",
    "print('Tokens words: %s' % words)\n",
    "words1 = nltk.word_tokenize(data.filter(train.id == 3) \\\n",
    "                           .select('question1').collect()[0][0], \\\n",
    "                            preserve_line=False)\n",
    "print('Tokens words1: %s' % words1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stopwords and punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep only important words use the list of stopwords from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him']\n",
      "['solve', '?']\n"
     ]
    }
   ],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "print(stop_words[:15])\n",
    "stop_words = set(stop_words)\n",
    "words = [w for w in words if not w.lower() in stop_words]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that stopwords are converted to a set to make comparison faster. <br>\n",
    "\n",
    "It may also be useful to remove punctuation. <br>\n",
    "Function *s.isalpha()* tests if *s* is non-empty and all characters in *s* are alphabetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['solve']\n"
     ]
    }
   ],
   "source": [
    "words = [w for w in words if w.isalpha()]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS-Tagging\n",
    "\n",
    "Part-of-speech tagging (or POS-tagging) is the process of classifying words into their parts of speech and labeling them accordingly. \n",
    "\n",
    "Parts of speech are also known as **word classes** or **lexical categories**. \n",
    "\n",
    "Collection of tags used for particular task is called **tagset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('How', 'WRB'),\n",
       " ('can', 'MD'),\n",
       " ('I', 'PRP'),\n",
       " ('increase', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('speed', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('internet', 'JJ'),\n",
       " ('connection', 'NN'),\n",
       " ('while', 'IN'),\n",
       " ('using', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('VPN', 'NNP'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.word_tokenize(train.filter(train.id == 2). \\\n",
    "                           select('question1').collect()[0][0])\n",
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default tagger of nltk.pos_tag() uses the [Penn Treebank Tag Set](http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). NLTK provides documentation for each tag, which can be queried using the tag or the beginning of it, e.g. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\JohntheGreat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"tagsets\")\n",
    "nltk.help.upenn_tagset('N')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization and Stemming\n",
    "\n",
    "The purpose of **lemmatization** and **stemming** is to standardize words to their root (stem) or their common morphological meaning or \"minimal unit of meaning\" (lemma).\n",
    "\n",
    "[The Stanford NLP Group](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) desribes these procedures as follows.  \n",
    "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. \n",
    "\n",
    "For instance:\n",
    "\n",
    "am, are, is $\\Rightarrow$ be  \n",
    "car, cars, car's, cars' $\\Rightarrow$ car  \n",
    "\n",
    "Stemming usually refers to a crude heuristic process that cuts the ends and/or beginnings of words to extract their common roots.\n",
    "\n",
    "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.  \n",
    "\n",
    "The NLTK lemmatization method is based on [WordNet](http://wordnet.princeton.edu/)'s lemmatizer.  \n",
    "\n",
    "Syntax of lemmatizer is:\n",
    "\n",
    "`lemmatize(word,pos='n')`,\n",
    "\n",
    "where parameter *pos* means part-of-speech and is defauled to noun, but can also be provided as other pos value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'having', 'lower', 'traditional']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "words = ['dogs','having','lower','traditional']\n",
    "\n",
    "print(list(map(wordnet_lemmatizer.lemmatize,words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizer did not find correct lemmas for 'having' and 'lower'. We can fix this issue setting the second parameter of the fuction lemmatize - 'pos' . \n",
    "\n",
    "The appropriate 'pos' values for noun, verb, adjective and adverb are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n v a r\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "print(wn.NOUN,wn.VERB,wn.ADJ,wn.ADV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while the default value is 'n'. \n",
    "\n",
    "With correct 'pos' we get correct lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('have', 'low')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(wordnet_lemmatizer.lemmatize('having','v'),\n",
    "wordnet_lemmatizer.lemmatize('lower','a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "understand\n",
      "traditional\n",
      "traditional\n"
     ]
    }
   ],
   "source": [
    "print(wordnet_lemmatizer.lemmatize('understand','n'))\n",
    "print(wordnet_lemmatizer.lemmatize(wordnet_lemmatizer.lemmatize(('traditional'),'a'),'v'))\n",
    "print(wordnet_lemmatizer.lemmatize('traditional','r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to POS-tagging before lemmatizing.  \n",
    "\n",
    "NLTK provides several well known stemmers interfaces, such as [Porter stemmer](https://tartarus.org/martin/PorterStemmer/), Lancaster Stemmer, [Snowball Stemmer](http://snowballstem.org/) etc. \n",
    "\n",
    "The following example shows how to use a stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'have', 'lower', 'tradit']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print([stemmer.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the word 'traditional' transformation by stemmer and lemmatizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "Create function `lemmas_nltk(s):` which: <br>\n",
    "- Takes sentense `s` as argument \n",
    "- Splits it into words and transforms each word into lower case\n",
    "- Lemmatizes each word as verb and as noun\n",
    "- Removes stopwords, non-alphabetic words\n",
    "\n",
    "**Hint.** <br>\n",
    "\n",
    "Use `.join()`, `.lower()`, `.split()`, `.isalpha()`.\n",
    "\n",
    "To check the function run the code:\n",
    "\n",
    "`ss=train.select('question2').take(2)[1][0]` <br>\n",
    "`print(ss)` <br>\n",
    "`print(lemmas_nltk(ss))` <br>\n",
    "\n",
    "To receive the result:\n",
    "\n",
    "`What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?` <br>\n",
    "`would happen indian government steal kohinoor diamond` <br>\n",
    "\n",
    "Enter code in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'would happen indian government steal kohinoor diamond'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Skipped code\n",
    "#Function lemmas_nltk(s)\n",
    "\n",
    "# split, lower, remove stopwords and non alpha, then lemmatize\n",
    "\n",
    "def lemmas_nltk(s):\n",
    "    return \" \".join([wordnet_lemmatizer.lemmatize(wordnet_lemmatizer.lemmatize(w,'n'),'v') \\\n",
    "                     for w in s.lower().split() if w.isalpha() & (not w in stop_words)])\n",
    "\n",
    "lemmas_nltk(\"What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "Create user defined function from function `lemmas_nltk(s)` to apply it to new column <br>\n",
    "\n",
    "Function syntax\n",
    "\n",
    "`pyspark.sql.functions.udf(f=None, returnType=StringType)`\n",
    "\n",
    "Function creates a Column expression representing a user defined function (UDF).\n",
    "\n",
    "The user-defined functions must be deterministic. \n",
    "Parameters:\t\n",
    "\n",
    "`f – python function if used as a standalone function`\n",
    "`returnType – a pyspark.sql.types.DataType object`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skipped code\n",
    "lemmas_nltk_udf = udf(lemmas_nltk, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Apply function `lemmas_nltk_udf` to a column of second question in train sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           question2|\n",
      "+--------------------+\n",
      "|What is the step ...|\n",
      "|What would happen...|\n",
      "|How can Internet ...|\n",
      "|Find the remainde...|\n",
      "|Which fish would ...|\n",
      "|I'm a triple Capr...|\n",
      "|What keeps childe...|\n",
      "|What should I do ...|\n",
      "|When do you use \"...|\n",
      "|How do I hack Mot...|\n",
      "|What are some of ...|\n",
      "|How can I see all...|\n",
      "|How can you make ...|\n",
      "|What was your fir...|\n",
      "|What are the laws...|\n",
      "|How will a Trump ...|\n",
      "|What does manipul...|\n",
      "|How do guys feel ...|\n",
      "|Why do people ask...|\n",
      "|Which is the best...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|           question2|           question3|\n",
      "+--------------------+--------------------+\n",
      "|What is the step ...|step step guide i...|\n",
      "|What would happen...|would happen indi...|\n",
      "|How can Internet ...|internet speed in...|\n",
      "|Find the remainde...|find remainder di...|\n",
      "|Which fish would ...|fish would surviv...|\n",
      "|I'm a triple Capr...|triple capricorn ...|\n",
      "|What keeps childe...|keep childern act...|\n",
      "|What should I do ...|               great|\n",
      "|When do you use \"...|         use instead|\n",
      "|How do I hack Mot...|  hack motorola free|\n",
      "|What are some of ...|thing technician ...|\n",
      "|How can I see all...|         see youtube|\n",
      "|How can you make ...|    make physic easy|\n",
      "|What was your fir...|        first sexual|\n",
      "|What are the laws...|law change status...|\n",
      "|How will a Trump ...|trump presidency ...|\n",
      "|What does manipul...|        manipulation|\n",
      "|How do guys feel ...|     guy feel reject|\n",
      "|Why do people ask...|people ask quora ...|\n",
      "|Which is the best...|best digital mark...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ss=train.select('question2')\n",
    "#print(ss.take(2)[1][0])\n",
    "ss.show()\n",
    "ss2=ss.withColumn('question3', lemmas_nltk_udf(ss.question2))\n",
    "ss2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "Create function `wordsCount(str):` which returns word count of string, plus 1\n",
    "\n",
    "Check the function by running code:\n",
    "\n",
    "`wordsCount('I love data')`\n",
    "\n",
    "The answer should be 3\n",
    "\n",
    "Create wordsCount_udf using `.udf()` of type integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Skipped code\n",
    "# wordsCount() \n",
    "def wordsCount(str):\n",
    "    return str.count(' ')+1\n",
    "#wordsCount_udf\n",
    "wordsCount_udf = udf(wordsCount, IntegerType())\n",
    "wordsCount('I love data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color=blue>\n",
    "\n",
    "Create function `ratio(x,y):` which calculates\n",
    "$$\\frac{|x-y|}{x+y}.$$ \n",
    "Can you make with respect to division by zero?\n",
    "\n",
    "Create `ratio_udf` using `.udf()` of type double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio(x,y): return abs(x-y)/(x+y+1e-15) ############## divide by zero occurs!!! ###########################\n",
    "ratio_udf = udf(ratio, DoubleType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic similarity\n",
    "\n",
    "It is important to have a measure of distance between words based on the similarity of their meaning or semantic content as opposed to similarity of their syntactical representation (e.g. their string format or word count).  \n",
    "\n",
    "WordNet created in Cognitive Science Laboratory pf Princeton University gives tools for measuring semantic similarity. \n",
    "\n",
    "Nouns, verbs, adjectives and adverbs in this database are grouped into cognitive synonym sets (**synsets**). \n",
    "\n",
    "The most frequently encoded relation among synsets is the **super-subordinate relation** (also called **hyperonymy**, **hyponymy** or **ISA relation**). \n",
    "\n",
    "WordNet links more general synsets like *furniture, piece_of_furniture* to increasingly specific ones like *bed* and *bunkbed*. \n",
    "\n",
    "See more detailed description on the [WordNet](https://wordnet.princeton.edu/) page or [here](https://arxiv.org/ftp/arxiv/papers/1310/1310.8059.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01'), Synset('chase.v.01')]\n"
     ]
    }
   ],
   "source": [
    "print(wn.synsets('dog'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A synset is identified with a 3-part name of the form: word.pos.nn.\n",
    "\n",
    "Function *synset1.path_similarity(synset2)* returns a score denoting how similar two word senses are, based on the shortest path that connects the senses in the is-a (hypernym/hyponym) taxonomy. The score is in the range 0 to 1. Large score means similar words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of dog to pet = 0.25\n",
      "Similarity of dog to table = 0.07142857142857142\n",
      "Similarity of dog to horse = 0.125\n"
     ]
    }
   ],
   "source": [
    "dog = wn.synset('dog.n.01')\n",
    "pet = wn.synset('pet.n.01')\n",
    "table = wn.synset('table.n.01')\n",
    "print('Similarity of dog to pet = %s' % dog.path_similarity(pet))\n",
    "print('Similarity of dog to table = %s' % dog.path_similarity(table))\n",
    "print('Similarity of dog to horse = %s' % \\\n",
    "      wn.synsets('dog')[0].path_similarity(wn.synsets('horse')[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path Similarity is not the only mesure implemented in WordNet. \n",
    "\n",
    "There are, for example, **Leacock-Chodorow Similarity**, **Wu-Palmer Similarity**, **Resnik Similarity**, **Jiang-Conrath Similarity**, **Lin Similarity** (see descriptions [here](https://arxiv.org/ftp/arxiv/papers/1310/1310.8059.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec is a mapping of words from a vocabulary to vectors of real numbers. \n",
    "\n",
    "This mapping was [created](https://arxiv.org/pdf/1301.3781.pdf) using neural networks by a team of researchers led by Tomas Mikolov at Google.  \n",
    "\n",
    "The purpose of Word2vec is to group the vectors of similar words together in vectorspace using mathematical similarities. \n",
    "\n",
    "Word2vec creates vectors that are distributed numerical representations of word features, such as the context of individual words.\n",
    "\n",
    "Word2Vec model depends on the corpora using which the neural network was trained. \n",
    "\n",
    "Below we use Google’s pre-trained Word2Vec neural network model that can be downloaded [here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit). Its size is 1.5GB. \n",
    "\n",
    "This model includes word vectors for a vocabulary of 3 million words and phrases based on training set of roughly 100 billion words from  Google News dataset. Each vector contains 300 features. \n",
    "\n",
    "Load the dataset. This requires package *gensim* which can be [installed](https://anaconda.org/anaconda/gensim) by running "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*conda install -c anaconda gensim*\n",
    "\n",
    "in terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors \\\n",
    ".load_word2vec_format('/home/yuri/projects/Quora/GoogleNews-vectors-negative300.bin.gz',\n",
    "                      binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is able to capture multiple different degrees of similarity between words. Semantic and syntactic patterns can be reproduced using vector arithmetic. Patterns such as “Queen is to Woman as King is to Man” can be generated through algebraic operations on the vector representations of these words such that the vector representation of “King” - ”Man” + ”Woman” produces a result which is closest to the vector representation of “Queen” in the model. Such relationships can be generated for a range of semantic relations (such as Country—Capital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Reykjavik', 0.6050446629524231)\n",
      "('queen', 0.7118192911148071)\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['capital','Iceland'])[0])\n",
    "print(model.most_similar(positive=['woman', 'king'],\n",
    "                         negative=['man'])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find most similar word to 'Oz' and 'Toto'. <br>\n",
    "Find most similar word to 'Dorothy' and 'Toto'. <br>\n",
    "Can it recognize that positive association with 'Garland' (Judy) and 'Toto' (the dog) and negative association with 'Hamilton' (Margaret, who played Wicked Witch of the West) should be Dorothy? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('One_worshiper_Yehuda', 0.5837857127189636)\n",
      "('Bernice', 0.6208423376083374)\n",
      "('Pangako', 0.36821073293685913)\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['Oz', 'Toto'])[0])\n",
    "print(model.most_similar(positive=['Dorothy', 'Toto'])[0])\n",
    "print(model.most_similar(positive=['Garland', 'Toto'],\n",
    "                         negative=['Hamilton'])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, we expect too much from this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model enables vector presentation of the question. \n",
    "If *words*  is a list of question's words after removing stopwords then the vector representation *question2vec* can be obtained as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['dogs', 'having', 'lower', 'traditional']\n",
      "[-0.02835612 -0.01235805 -0.0893375   0.1272655  -0.03042328]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Words: %s' %words)\n",
    "M = []\n",
    "for w in words:\n",
    "    try: M.append(model[w])\n",
    "    except: continue\n",
    "M = np.array(M)\n",
    "if len(M)==0: question2vec = np.zeros(300)\n",
    "else: \n",
    "    question2vec = M.sum(axis=0)\n",
    "    norm = np.sqrt((question2vec ** 2).sum())\n",
    "    if norm>0: question2vec /= norm \n",
    "\n",
    "print(question2vec[:5])\n",
    "len(question2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to run the same code with only first word in *words*, i.e. *words[0]*. The returned vector is different, but also has length of 300.\n",
    "\n",
    "Any distance, like cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis etc. between question's vector representations may be used as measure of their similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "One more way of obtaining vector representations is using **Term frequency-inverse document frequency** ([TF-IDF](http://spark.apache.org/docs/2.2.0/ml-features.html#tf-idf)) methodolody implemented in `pyspark.ml.feature` package.\n",
    "The scheme consists of the following steps.  \n",
    "(1) Create the corpus of all questions from train and test datasets.  \n",
    "(2) Initialize class `CountVectorizer`.  \n",
    "(3) Fit a `CountVectorizerModel` to this corpus.  \n",
    "(4) Apply `CountVectorizerModel.transform()` to *train.question1, train.question2, test.question1, test.question2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF, Tokenizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+\n",
      "|text                                      |\n",
      "+------------------------------------------+\n",
      "|[Hi, I, heard, about, Spark]              |\n",
      "|[Logistic, regression, models, are, neat] |\n",
      "|[I, wish, Java, could, use, case, classes]|\n",
      "+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), )\n",
    "    ], [\"text\"])\n",
    "corpus.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Initialize a `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"text\", outputCol=\"vectors\", minDF=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntax of CountVectorizer:\n",
    "\n",
    "`CountVectorizer(minTF=1.0, minDF=1.0, vocabSize=262144, binary=False, inputCol=None, outputCol=None)`\n",
    "\n",
    "* `minTF` Filter to ignore rare words in a document. For each document, terms with frequency/count less than the given threshold are ignored. If this is an integer >= 1, then this specifies a count (of times the term must appear in the document); if this is a double in [0,1), then this specifies a fraction (out of the document's token count).\n",
    "* `minDF` Specifies the minimum number of different documents a term must appear in to be included in the vocabulary\n",
    "* `vocabSize` max size of the vocabulary. Default 1e18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a CountVectorizerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'wish',\n",
       " 'use',\n",
       " 'Spark',\n",
       " 'could',\n",
       " 'about',\n",
       " 'heard',\n",
       " 'regression',\n",
       " 'Hi',\n",
       " 'are',\n",
       " 'Logistic',\n",
       " 'case',\n",
       " 'classes',\n",
       " 'Java',\n",
       " 'neat',\n",
       " 'models']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel = cv.fit(corpus)\n",
    "cvModel.vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the model. <br>\n",
    "\n",
    "Column *vectors* contains number of words in the created vocabulary, list of word IDs in the vocabulary and real-valued vectors of frequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+-----------------------------------------------------+\n",
      "|text                                      |vectors                                              |\n",
      "+------------------------------------------+-----------------------------------------------------+\n",
      "|[Hi, I, heard, about, Spark]              |(16,[0,3,5,6,8],[1.0,1.0,1.0,1.0,1.0])               |\n",
      "|[Logistic, regression, models, are, neat] |(16,[7,9,10,14,15],[1.0,1.0,1.0,1.0,1.0])            |\n",
      "|[I, wish, Java, could, use, case, classes]|(16,[0,1,2,4,11,12,13],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+------------------------------------------+-----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = cvModel.transform(corpus)\n",
    "corpus.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"vectors\", outputCol=\"idf\")\n",
    "idfModel = idf.fit(corpus)\n",
    "corpus = idfModel.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(idf=SparseVector(16, {0: 0.2877, 3: 0.6931, 5: 0.6931, 6: 0.6931, 8: 0.6931})),\n",
       " Row(idf=SparseVector(16, {7: 0.6931, 9: 0.6931, 10: 0.6931, 14: 0.6931, 15: 0.6931}))]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.select('idf').take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that word 'I' appears twice in the corpus, so its weight is only half of weights of other words which appear once.\n",
    "\n",
    "Now create features calculating different characteristics of each representation and distances between questions' representations.  \n",
    "For examle `squared_distance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.4068381000739647"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf1 = corpus.select('idf').collect()[0][0]\n",
    "idf2 = corpus.select('idf').collect()[1][0]\n",
    "idf1.squared_distance(idf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams\n",
    "\n",
    "n-gram is a contiguous sequence of n items (letters, words etc.) from a given sequence of text. <br>\n",
    "An n-gram of size 1 is referred to as a \"unigram\"; size 2 is a \"bigram\". <br>\n",
    "The number of common n-grams in the pairs of texts is a natural measure of their syntactic similarity. <br>\n",
    "Counting common n-grams is easy in NLTK. <br>\n",
    "Count bigrams for the first pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "row_i = data.filter(data.id == i).collect()[0]\n",
    "quest1, quest2 = row_i.question1, row_i.question2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('step', 'by'), ('What', 'is'), ('step', 'guide'), ('the', 'step'), ('share', 'market'), ('india', '?'), ('guide', 'to'), ('to', 'invest'), ('market', 'in'), ('in', 'share'), ('is', 'the'), ('invest', 'in'), ('in', 'india'), ('by', 'step')}\n",
      "{('step', 'by'), ('What', 'is'), ('step', 'guide'), ('the', 'step'), ('share', 'market'), ('guide', 'to'), ('to', 'invest'), ('in', 'share'), ('is', 'the'), ('invest', 'in'), ('by', 'step')}\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(set(nltk.ngrams(nltk.word_tokenize(quest1), 2)))\n",
    "commonBigrams = (set(nltk.ngrams(nltk.word_tokenize(quest1), 2)) &\n",
    "     set(nltk.ngrams(nltk.word_tokenize(quest2), 2)))\n",
    "print(commonBigrams)\n",
    "print(len(commonBigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of letter bigrams for the same pair is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "print(len(set(nltk.ngrams(quest1, 2)) &\n",
    "     set(nltk.ngrams(quest2, 2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "Create function `commonNgrams(s1, s2, n):` which:\n",
    "\n",
    "1. Takes 2 documents or lemmatized documents `s1` and `s2` and length of n-gram `n`\n",
    "2. Splits them and transforms them into lower case\n",
    "3. Removes unnecessary special symbols by compiling '([^\\s\\w]|_)+'\n",
    "4. Calculates and returns number of common n-grams\n",
    "\n",
    "**Hint.** \n",
    "\n",
    "1. Use functions `.lower()`, `.split()`\n",
    "2. Import `re` and use `re.compile()` and `re.sub()` to remove special symbols\n",
    "\n",
    "Python function\n",
    "\n",
    "`re.compile(pattern, flags=0)`\n",
    "\n",
    "compiles a regular expression pattern into a regular expression object, which can be used for matching using its match() and search() methods, described below.\n",
    "\n",
    "The expression’s behaviour can be modified by specifying a flags value. Values can be any of the following variables, combined using bitwise OR (the | operator).\n",
    "\n",
    "The sequence\n",
    "\n",
    "`prog = re.compile(pattern)`\n",
    "`result = prog.match(string)`\n",
    "\n",
    "is equivalent to\n",
    "\n",
    "`result = re.match(pattern, string)`\n",
    "\n",
    "but using re.compile() and saving the resulting regular expression object for reuse is more efficient when the expression will be used several times in a single program.\n",
    "\n",
    "Python function `re.sub()` removes or substitutes one pattern with another.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " +  - ^ = \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "regex = re.compile('\\w')\n",
    "print(regex.sub('', 'X + Y - Z^2 = 5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "Create `commonNgrams_udf` by using `.udf()` with type integer\n",
    "\n",
    "Check `commonNgrams(s1, s2, n)` by running code\n",
    "\n",
    "`commonNgrams('I love data', 'I love MScA', 2)`\n",
    "\n",
    "Result should be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Skipped code\n",
    "# commonNgrams(s1,s2,n), commonNgrams_udf\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "Create function `unigram_ratio(ngrams, n1, n2):` which calculates unigram ratio by formula\n",
    "$$\\frac{ngrams}{1+\\max(n_1,n_2)},$$\n",
    "where `ngrams` is the number  of common grams and `$n_1$`,`$n_2$` are the word counts in 2 documents.\n",
    "\n",
    "To check the function call it using the following example from Zeno's paradox: \n",
    "\n",
    "ss1='Tortoise runs slow' <br>\n",
    "ss2='Achilles runs very fast' <br>\n",
    "ngr=commonNgrams(ss1,ss2,1) <br>\n",
    "print('ngr = %s' % ngr) <br>\n",
    "print(ss1.count(' ')) <br>\n",
    "print(ss2.count(' ')) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "Result should be 0.25 <br>\n",
    "\n",
    "Enter code in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Skipped code\n",
    "#unigram_ratio()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy string matching\n",
    "\n",
    "Sometimes texts are apparently duplicated but differ by a word or punctuation mark. <br>\n",
    "Here is an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What was your first sexual experience like? \n",
      " What was your first sexual experience? \n",
      " 1\n"
     ]
    }
   ],
   "source": [
    "i = 13\n",
    "row_i = train.filter(train.id == i).collect()[0]\n",
    "q1, q2, is_dupl = row_i.question1, row_i.question2, row_i.is_duplicate\n",
    "print(q1,\"\\n\",q2,\"\\n\",is_dupl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package [fuzzywuzzy](https://github.com/seatgeek/fuzzywuzzy) can handle well such cases. It uses [Levenshtein Distance](https://en.wikipedia.org/wiki/Levenshtein_distance) to calculate the differences between sequences. Functions **ratio, partial_ratio, token_sort_ratio** and **token_set_ratio** return similarity score from 0 to 100. Let us see how do they score the pair above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Install](https://anaconda.org/conda-forge/fuzzywuzzy) the package by running in terminal:\n",
    "\n",
    "*conda install -c conda-forge fuzzywuzzy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Ratio =  94\n",
      "Partial Ratio =  97\n",
      "Token Sort Ratio =  94\n",
      "Token Set Ratio =  100\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "print(\"Simple Ratio = \",fuzz.ratio(q1,q2))\n",
    "print(\"Partial Ratio = \",fuzz.partial_ratio(q1,q2))\n",
    "print(\"Token Sort Ratio = \",fuzz.token_sort_ratio(q1,q2))\n",
    "print(\"Token Set Ratio = \",fuzz.token_set_ratio(q1,q2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another Python module providing classes and functions for comparing sequences is [difflib](https://docs.python.org/3/library/difflib.html). Function **ratio** return a measure of the sequences’ similarity as a float in the range [0, 1]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9382716049382716\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "seq = difflib.SequenceMatcher()\n",
    "seq.set_seqs(q1.lower(), q2.lower())\n",
    "print(seq.ratio())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features creating example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `pyspark.sql.functions.length` function to create new columns with lengths of questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------------+----+----+\n",
      "| id|           question1|           question2|is_duplicate|len1|len2|\n",
      "+---+--------------------+--------------------+------------+----+----+\n",
      "|  0|What is the step ...|What is the step ...|           0|  66|  57|\n",
      "|  1|What is the story...|What would happen...|           0|  51|  88|\n",
      "|  2|How can I increas...|How can Internet ...|           0|  73|  59|\n",
      "|  3|Why am I mentally...|Find the remainde...|           0|  50|  65|\n",
      "|  4|Which one dissolv...|Which fish would ...|           0|  76|  39|\n",
      "+---+--------------------+--------------------+------------+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "for i in ['1', '2']:\n",
    "    data = data.withColumn('len'+i, length(data['question'+i]))\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "Create new column `lenRatio` with user-defined *function ratio_udf*\n",
    "\n",
    "Running code \n",
    "\n",
    "`data.select('question1','question2','len1','len2','lenRatio').show(5)`\n",
    "\n",
    "should show\n",
    "\n",
    "+--------------------+--------------------+----+----+-------------------+ <br>\n",
    "|           question1|           question2|len1|len2|           lenRatio| <br>\n",
    "+--------------------+--------------------+----+----+-------------------+ <br>\n",
    "|What is the step ...|What is the step ...|  66|  57|0.07317073170731707| <br>\n",
    "|What is the story...|What would happen...|  51|  88|0.26618705035971224| <br>\n",
    "|How can I increas...|How can Internet ...|  73|  59|0.10606060606060606| <br>\n",
    "|Why am I mentally...|Find the remainde...|  50|  65|0.13043478260869565| <br>\n",
    "|Which one dissolv...|Which fish would ...|  76|  39| 0.3217391304347826| <br>\n",
    "+--------------------+--------------------+----+----+-------------------+ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skipped code\n",
    "#LenRatio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment on test set\n",
    "\n",
    "As an anti-cheating measure, Kaggle has supplemented the test set with computer-generated question pairs. Those rows do not come from Quora, and are not counted in the scoring. All of the questions in the training set are genuine examples from Quora.  \n",
    "The test set you get for this project is different from Quora *test.csv* but it also contains a lot of pairs not counted in the scoring.  The *[train.csv](https://www.kaggle.com/c/quora-question-pairs/download/train.csv.zip)* is the original Quora train set, it can be downloaded from [competition page](https://www.kaggle.com/c/quora-question-pairs/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project description\n",
    "\n",
    "For the first part of this project create the following set of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featureNames = ['Lemmas1', 'Lemmas2',\n",
    "                'rawWords1', 'rawWords2',\n",
    "                'Len1', 'Len2',\n",
    "                'rawLen1', 'rawLen2',\n",
    "                'diff_lemmas_ratio', 'diff_rawWords_ratio',\n",
    "                'diff_len_ratio', 'diff_rawLen_ratio',\n",
    "                'raw_ngrams_1', 'raw_ngrams_2', 'raw_ngrams_3', \n",
    "                'ngrams_1', 'ngrams_2', 'ngrams_3', \n",
    "                'raw_unigram_ratio', 'unigram_ratio', \n",
    "                'tfidfDistance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "Create lemma string for each of the questions using function `lemmas_nltk_udf`\n",
    "\n",
    "+---+----+----+--------------------+--------------------+--------------------+ <br>\n",
    "| id|len1|len2|            lenRatio|              lemma1|              lemma2| <br>\n",
    "+---+----+----+--------------------+--------------------+--------------------+ <br>\n",
    "|  0|  66|  57| 0.07317073170731707|step step guide i...|step step guide i...| <br>\n",
    "|  1|  51|  88| 0.26618705035971224|      story kohinoor|would happen indi...| <br>\n",
    "|  2|  73|  59| 0.10606060606060606|increase speed in...|internet speed in...| <br>\n",
    "|  3|  50|  65| 0.13043478260869565|      mentally solve|find remainder di...| <br>\n",
    "|  4|  76|  39|  0.3217391304347826|one dissolve wate...|fish would surviv...| <br>\n",
    "|  5|  86|  90|0.022727272727272728|capricorn sun cap...|triple capricorn ...| <br>\n",
    "+---+----+----+--------------------+--------------------+--------------------+ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skipped code\n",
    "#Lemmas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions length, lemmas length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "Create columns of lemmas counts: \n",
    "\n",
    "- `Lemmas1`, `Lemmas2`: word counts in lemmas\n",
    "- `rawWords1`, `rawWords2`:  word counts in original questions\n",
    "- `Len1`, `Len2`: lengths of lemmas\n",
    "- `rawLen1`, `rawLen2`: lengths of questions.\n",
    "\n",
    "Use functions `wordsCount_udf`.\n",
    "\n",
    "Selection by code\n",
    "\n",
    "`data.select('Lemmas1','Lemmas2','rawWords1','rawWords2','Len1','Len2','rawLen1','rawLen2').show(6)`\n",
    "\n",
    "Should look like:\n",
    "\n",
    "+-------+-------+---------+---------+----+----+-------+-------+ <br>\n",
    "|Lemmas1|Lemmas2|rawWords1|rawWords2|Len1|Len2|rawLen1|rawLen2| <br>\n",
    "+-------+-------+---------+---------+----+----+-------+-------+ <br>\n",
    "|      6|      5|       14|       12|  35|  28|     66|     57| <br>\n",
    "|      2|      7|        8|       13|  14|  53|     51|     88| <br>\n",
    "|      5|      4|       14|       10|  38|  28|     73|     59| <br>\n",
    "|      2|      3|       11|        9|  14|  21|     50|     65| <br>\n",
    "|      7|      4|       13|        7|  43|  23|     76|     39| <br>\n",
    "|      6|      5|       16|       16|  30|  35|     86|     90| <br>\n",
    "+-------+-------+---------+---------+----+----+-------+-------+ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordsCount_udf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-3aeec12bcc2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Lemmas'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordsCount_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemma'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rawWords'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordsCount_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Len'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemma'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rawLen'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wordsCount_udf' is not defined"
     ]
    }
   ],
   "source": [
    "for i in [\"1\",\"2\"]:\n",
    "    data = data.withColumn('Lemmas'+i, wordsCount_udf(data['lemma'+i]))\n",
    "    data = data.withColumn('rawWords'+i, wordsCount_udf(data['question'+i]))\n",
    "    data = data.withColumn('Len'+i, length(data['lemma'+i]))\n",
    "    data = data.withColumn('rawLen'+i, length(data['question'+i]))\n",
    "data.select('Lemmas1','Lemmas2','rawWords1','rawWords2','Len1','Len2','rawLen1','rawLen2').show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lengths ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "Create columns of lengths ratios using `ratio_udf`:\n",
    "\n",
    "- `diff_lemmas_ratio`: ratio of `Lemmas1` and `Lemmas2`\n",
    "- `diff_rawWords_ratio`: ratio of `rawWords1`and `rawWords2`\n",
    "- `diff_len_ratio`: ratio of `Len1` and `Len2`\n",
    "- `diff_rawLen_ratio`: ratio of `rawLen1` and `rawLen2`\n",
    "\n",
    "Running\n",
    "\n",
    "`data.select('diff_lemmas_ratio','diff_rawWords_ratio','diff_len_ratio','diff_rawLen_ratio').show(6)`\n",
    "\n",
    "should show\n",
    "\n",
    "+-------------------+-------------------+-------------------+--------------------+ <br>\n",
    "|  diff_lemmas_ratio|diff_rawWords_ratio|     diff_len_ratio|   diff_rawLen_ratio| <br>\n",
    "+-------------------+-------------------+-------------------+--------------------+ <br>\n",
    "| 0.0909090909090909|0.07692307692307693| 0.1111111111111111| 0.07317073170731707| <br>\n",
    "| 0.5555555555555555|0.23809523809523808|  0.582089552238806| 0.26618705035971224| <br>\n",
    "|0.11111111111111109|0.16666666666666666|0.15151515151515152| 0.10606060606060606| <br>\n",
    "|0.19999999999999996|                0.1|                0.2| 0.13043478260869565| <br>\n",
    "| 0.2727272727272727|                0.3|0.30303030303030304|  0.3217391304347826| <br>\n",
    "| 0.0909090909090909|                0.0|0.07692307692307693|0.022727272727272728| <br>\n",
    "+-------------------+-------------------+-------------------+--------------------+ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skipped code\n",
    "#Lengths ratios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "Create features:\n",
    "\n",
    "- `raw_ngrams_1`, `raw_ngrams_2`, `raw_ngrams_3`: n-grams of questions\n",
    "- `ngrams_1`, `ngrams_2`, `ngrams_3`: n-grams of lemmas\n",
    "- `raw_unigram_ratio`: unigram ratio for questions\n",
    "- `unigram_ratio`: unigram ratio for lemmas\n",
    "\n",
    "Use function `commonNgrams_udf`\n",
    "\n",
    "Code\n",
    "\n",
    "`data.select('ngrams_1','ngrams_2','ngrams_3', \\` <br>\n",
    "            `'raw_ngrams_1','raw_ngrams_2','raw_ngrams_3', \\` <br>\n",
    "            `'raw_unigram_ratio','unigram_ratio').show(6)` <br>\n",
    "            \n",
    "should show\n",
    "\n",
    "+--------+--------+--------+------------+------------+------------+-------------------+-------------------+ <br>\n",
    "|ngrams_1|ngrams_2|ngrams_3|raw_ngrams_1|raw_ngrams_2|raw_ngrams_3|  raw_unigram_ratio|      unigram_ratio| <br>\n",
    "+--------+--------+--------+------------+------------+------------+-------------------+-------------------+ <br>\n",
    "|       4|       4|       3|          11|          11|          10| 0.7333333333333333| 0.5714285714285714| <br>\n",
    "|       1|       0|       0|           4|           2|           1| 0.2857142857142857|              0.125| <br>\n",
    "|       3|       0|       0|           4|           1|           0|0.26666666666666666|                0.5| <br>\n",
    "|       0|       0|       0|           0|           0|           0|                0.0|                0.0| <br>\n",
    "|       0|       0|       0|           4|           0|           0| 0.2857142857142857|                0.0| <br>\n",
    "|       3|       0|       0|           9|           4|           1| 0.5294117647058824|0.42857142857142855| <br>\n",
    "+--------+--------+--------+------------+------------+------------+-------------------+-------------------+ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skipped code\n",
    "#N-grams and n-gram ratios\n"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 122,
  "_is_fork": false,
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
