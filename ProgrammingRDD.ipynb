{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MScA, Advanced Machine Learning (32009)\n",
    "\n",
    "# Week 2 Workshop\n",
    "\n",
    "# Programming with RDD\n",
    "\n",
    "### Yuri Balasanov, &copy; iLykei, 2017\n",
    "\n",
    "Main text: Jeffrey Aven, Sams Teach Yourself Apache SPARK in 24 Hours,Pearson Education, Inc., 2017 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fundamental object in Spark programming is called Resilient Distributed Dataset (**RDD**). <br>\n",
    "\n",
    "-  RDDs are resilient because if a node performing a Spark operation failes the dataset can be reconstructed. This is achieved by Spark knows the lineage of precursors of each RDD\n",
    "-  RDDs are distributed because data are partitionedand distributed as in-memory collections of objects across workers in the cluster\n",
    "-  RDDs are datasets consisting of records, each of which uniquely identifiable. Records can be collections of fields, like rows in a table of a relational database, or lines of text in a file, or some other format.\n",
    "\n",
    "RDDs are **immutable**: after they are created they cannot be updated. Instead of updating an RDD a new RDD will be created with lineage tracked. Child RDDs can be created by by performing a transformation, like Map or Filter functions or performing an action like, for example count. \n",
    "\n",
    "## Loading data in RDD\n",
    "\n",
    "### Loading data from file or files\n",
    "\n",
    "There are 2 main finctions for creating RDDs from text files:\n",
    "\n",
    "-  `sc.textFile(name,minPartitions=None,use_unicode=True)` - importing a single file; each line of the file represents a record\n",
    "-  `sc.wholeTextFiles()` importing a group or directory of files each of which becomes a record the file name as key and file content as value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>**Example**</font> <br>\n",
    "\n",
    "<font color=green>Read one of the license files for Spark</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Copyright (c) 2009-2011, Barthelemy Dagenais All rights reserved.', '', 'Redistribution and use in source and binary forms, with or without', 'modification, are permitted provided that the following conditions are met:', '', '- Redistributions of source code must retain the above copyright notice, this', 'list of conditions and the following disclaimer.', '', '- Redistributions in binary form must reproduce the above copyright notice,', 'this list of conditions and the following disclaimer in the documentation', 'and/or other materials provided with the distribution.', '', '- The name of the author may not be used to endorse or promote products', 'derived from this software without specific prior written permission.', '', 'THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"', 'AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE', 'IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE', 'ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE', 'LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR', 'CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF', 'SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS', 'INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN', 'CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)', 'ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE', 'POSSIBILITY OF SUCH DAMAGE.', '']\n"
     ]
    }
   ],
   "source": [
    "lic=sc.textFile(\"C:/dev/spark-2.2.0-bin-hadoop2.7/spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-py4j.txt\")\n",
    "print(lic.take(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>Read one of the license files for SparTotal number of lines in the file:</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lic.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lic.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>Read one of the license files for Spark</font>\n",
    "Read all files from the licence folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The MIT License (MIT)', '']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "licAll=sc.textFile(\"C:/dev/spark-2.2.0-bin-hadoop2.7/spark-2.2.0-bin-hadoop2.7/licenses/*.txt\")\n",
    "licAll.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "licAll.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>This means that each file is a partition.</font>\n",
    "\n",
    "<font color=green>Count total number of lines in all files:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1075"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "licAll.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>Use `wholeTextFiles()` instead.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/C:/dev/spark-2.2.0-bin-hadoop2.7/spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-AnchorJS.txt',\n",
       "  'The MIT License (MIT)\\n\\nCopyright (c) <year> <copyright holders>\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in\\nall copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\\nTHE SOFTWARE.'),\n",
       " ('file:/C:/dev/spark-2.2.0-bin-hadoop2.7/spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-antlr.txt',\n",
       "  '[The BSD License]\\nCopyright (c) 2012 Terence Parr and Sam Harwell\\nAll rights reserved.\\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\\nNeither the name of the author nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "licWhole=sc.wholeTextFiles(\"C:/dev/spark-2.2.0-bin-hadoop2.7/spark-2.2.0-bin-hadoop2.7/licenses/*.txt\")\n",
    "licWhole.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "licWhole.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>Now the entire folder is one file with 2 partitions, each file is a record:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "licWhole.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file:/C:/dev/spark-2.2.0-bin-hadoop2.7/spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-antlr.txt',\n",
       " 'file:/C:/dev/spark-2.2.0-bin-hadoop2.7/spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-boto.txt',\n",
       " 'file:/C:/dev/spark-2.2.0-bin-hadoop2.7/spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-cloudpickle.txt',\n",
       " 'file:/C:/dev/spark-2.2.0-bin-hadoop2.7/spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-d3.min.js.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "licWholeNames=licWhole.keys()\n",
    "licWholeNames.collect()[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The MIT License (MIT)\\n\\nCopyright (c) <year> <copyright holders>\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in\\nall copies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\\nTHE SOFTWARE.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "licWhole.values().take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating RDD programmatically\n",
    "\n",
    "RDDs can be created by methods like, for example, `sc.parallelize()` or `sc.range()`.\n",
    "\n",
    "#### Parallelize \n",
    "\n",
    "Method `parallelize()` has syntax <br>\n",
    "\n",
    "`sc.parallelize(c, numSlices=None)`, <br>\n",
    "\n",
    "where `c` is a collection - a list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example**\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[16] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallelrdd=sc.parallelize(range(10))\n",
    "parallelrdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallelrdd.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallelrdd.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallelrdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Range \n",
    "\n",
    "Method `range` has syntax <br>\n",
    "\n",
    "`sc.range(start,end=1,step=1,numSlices=None)`.\n",
    "\n",
    "Create RDD with 1000 integers starting from 0 incrementing by 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>**Example**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[20] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rangerdd=sc.range(0,1000,1,2)\n",
    "rangerdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rangerdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rangerdd.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rangerdd.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rangerdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Operations on RDD: Transformations\n",
    "\n",
    "There are 2 main classes of operations on RDD: Transformations and Actions.\n",
    "\n",
    "Transformation is an operation applied to each element of RDD which results in new RDD.\n",
    "Most common transformations are mapping and filtering functions, among them are `map()` and `filter()`.\n",
    "\n",
    "### Functional Transformations\n",
    "\n",
    "#### Map()\n",
    "\n",
    "Syntax:\n",
    "\n",
    "`.map(<function>, preservePartitioning=False)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>**Example**</font> <br>\n",
    "<font color=green>$x$ below is a predictor for linear regression model.</font>\n",
    "<font color=green>Prepare predictors for polynomial regression: $x,~x^2$</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 5, 4, 8, 6, 1, 7, 2, 3, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(9, 81),\n",
       " (5, 25),\n",
       " (4, 16),\n",
       " (8, 64),\n",
       " (6, 36),\n",
       " (1, 1),\n",
       " (7, 49),\n",
       " (2, 4),\n",
       " (3, 9),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.random import permutation\n",
    "x=sc.parallelize(permutation(range(10)))\n",
    "print(x.collect())\n",
    "xPolynomial=x.map(lambda element: (element,element**2))\n",
    "xPolynomial.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FlatMap()\n",
    "\n",
    "Syntax:\n",
    "\n",
    "`.flatMap(<function>, preservePartitioning=False)`\n",
    "\n",
    "Instead of creating a list from each record of the original RDD this function creates a single list, \"flattening\" the nested structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>**Example**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 81, 5, 25, 4, 16, 8, 64, 6, 36, 1, 1, 7, 49, 2, 4, 3, 9, 0, 0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xPolynomialFlat=x.flatMap(lambda element: (element,element**2))\n",
    "xPolynomialFlat.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter()\n",
    "\n",
    "Syntax:\n",
    "\n",
    "`.filter(<function>)`\n",
    "\n",
    "The `filter` transformation evaluates a Boolean expression of each element (record) of the original RDD. The returned Boolean value determines whether the corresponding element is returned in the output RDD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>**Example**</font> <br>\n",
    "<br>\n",
    "<font color=green>Filter OUT even numbers from sequence of integers 0 to 8.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originalrdd=sc.parallelize([0,1,2,3,4,5,6,7,8])\n",
    "newrdd=originalrdd.filter(lambda x: x % 2)\n",
    "newrdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principle of big data programming: <br>\n",
    "**FILTER EARLY, FILTER OFTEN**. <br>\n",
    "It helps avoiding carrying unnecessary records through a process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**Exercise. Transform the text**</font> <br>\n",
    "\n",
    "<font color=blue>Create an RDD `txtDoc`</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THESE',\n",
       " 'VIOLENT',\n",
       " 'DELIGHTS',\n",
       " 'VIOLENT',\n",
       " 'THEIR',\n",
       " 'TRIUMP',\n",
       " 'POWDER',\n",
       " 'WHICH',\n",
       " 'CONSUME']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtDoc=sc.parallelize(['These violent delights have violent ends',\n",
    "                       'And in their triump die like fire and powder',\n",
    "                       'Which as they kiss consume'])\n",
    "txtDocUpper=txtDoc.map(lambda x: x.upper()).flatMap(lambda x: x.split(\" \")).filter(lambda x: len(x)>4)\n",
    "\n",
    "txtDocUpper.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "-  Convert `txtDoc` to upper case to make it look like this <br>\n",
    "<br>\n",
    "['THESE VIOLENT DELIGHTS HAVE VIOLENT ENDS', <br>\n",
    " 'AND IN THEIR TRIUMP DIE LIKE FIRE AND POWDER', <br>\n",
    " 'WHICH AS THEY KISS CONSUME'] <br>\n",
    "<br> \n",
    "-  Split the text into a combined list of separate words <br>\n",
    "<br>\n",
    "['THESE', 'VIOLENT', 'DELIGHTS', 'HAVE', 'VIOLENT', 'ENDS', 'AND', 'IN', 'THEIR', 'TRIUMP', 'DIE', 'LIKE', 'FIRE', 'AND', 'POWDER', 'WHICH', 'AS', 'THEY', 'KISS', 'CONSUME'] <br>\n",
    "<br> \n",
    "-  Select only words that are longer than 4 letters <br>\n",
    "<br>\n",
    "['THESE',\n",
    " 'VIOLENT',\n",
    " 'DELIGHTS',\n",
    " 'VIOLENT',\n",
    " 'THEIR',\n",
    " 'TRIUMP',\n",
    " 'POWDER',\n",
    " 'WHICH',\n",
    " 'CONSUME']\n",
    "<br> \n",
    " Enter code in the following cell.\n",
    " </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Skipped code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping, Sorting, Elimination of Duplicates Transformations\n",
    "\n",
    "The following 3 transformations are for grouping, sorting and eliminating duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GroupBy()\n",
    "\n",
    "Syntax:\n",
    "\n",
    "`.groupBy(<function>, numPartitions=None)`\n",
    "\n",
    "This transformation returns an RDD of items  grouped by a given function.\n",
    "The function may either specify a particular key by which to group or specify an expression to be evaluated with elements to specify the group (for example, group odd and even numbers separately).\n",
    "\n",
    "**This functions is not recommended for aggregation of values (like sum or count). The  `groupBy` transformation does not do any aggregation before shuffling data, it also requires that all values for a given key fit into memory**.\n",
    "\n",
    "#### SortBy()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.sortBy(<keyfunc>, ascending=True, numPartitions=None)`\n",
    "\n",
    "This transformation sorts an RDD by the function that defines the key for a given dataset.\n",
    "\n",
    "#### Distinct()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.distinct(numPartitions=None)`\n",
    "\n",
    "This transformation returns a new RDD containing distinct elements. It is used to remove duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**Exercise. Select long words and sort them**</font> <br>\n",
    "<font color=blue>\n",
    "<br>\n",
    "First 3 steps below repeat the steps of the previous exercise.   <br>\n",
    "<br>\n",
    "1 Turn lines from Shakespeare in `txtDoc` created earlier into upper case <br>\n",
    "<br>\n",
    "['THESE VIOLENT DELIGHTS HAVE VIOLENT ENDS', <br>\n",
    " 'AND IN THEIR TRIUMP DIE LIKE FIRE AND POWDER', <br>\n",
    " 'WHICH AS THEY KISS CONSUME'] <br>\n",
    " <br>\n",
    "2 Split the RDD by space <br>\n",
    "<br>\n",
    "['THESE', 'VIOLENT', 'DELIGHTS', 'HAVE', 'VIOLENT', 'ENDS', 'AND', 'IN', 'THEIR', 'TRIUMP', 'DIE', 'LIKE', 'FIRE', 'AND', 'POWDER', 'WHICH', 'AS', 'THEY', 'KISS', 'CONSUME'] <br>\n",
    "<br>\n",
    "3 Select words longer than 4 letters <br>\n",
    "<br>\n",
    "['THESE', 'VIOLENT', 'DELIGHTS', 'VIOLENT', 'THEIR', 'TRIUMP', 'POWDER', 'WHICH', 'CONSUME'] <br>\n",
    "<br>\n",
    "4 Select distinct words <br>\n",
    "<br>\n",
    "['VIOLENT', 'THEIR', 'THESE', 'TRIUMP', 'POWDER', 'DELIGHTS', 'WHICH', 'CONSUME'] <br>\n",
    "<br>\n",
    "5 Sort distinct words in alphabetic order <br>\n",
    "<br>\n",
    "['CONSUME', 'DELIGHTS', 'POWDER', 'THEIR', 'THESE', 'TRIUMP', 'VIOLENT', 'WHICH'] <br>\n",
    " </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CONSUME',\n",
       " 'DELIGHTS',\n",
       " 'POWDER',\n",
       " 'THEIR',\n",
       " 'THESE',\n",
       " 'TRIUMP',\n",
       " 'VIOLENT',\n",
       " 'WHICH']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Skipped code\n",
    "txtDocUpperAlpha= txtDocUpper.distinct().sortBy(lambda x: x)\n",
    "txtDocUpperAlpha.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Transformations\n",
    "\n",
    "The following 3 transformations are set operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Union()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`RDD1.union(RDD2)`\n",
    "\n",
    "This function outputs an RDD which is `RDD2` appended to `RDD1`. \n",
    "The two RDDs are not required to have the same structure.\n",
    "Duplicates from the two RDDs are not filtered out.\n",
    "The output RDD is not sorted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>**Example** <br>\n",
    "<br>\n",
    "Create another RDD with Sheakespear's lines and union it with `txtDoc`\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['These violent delights have violent ends',\n",
       " 'And in their triump die like fire and powder',\n",
       " 'Which as they kiss consume',\n",
       " 'My bounty is as boundless as the sea',\n",
       " 'My love as deep the more I give to thee',\n",
       " 'The more I have for both are infinite']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtDoc2=sc.parallelize(['My bounty is as boundless as the sea',\n",
    "                        'My love as deep the more I give to thee',\n",
    "                        'The more I have for both are infinite'])\n",
    "txtDocUnion = txtDoc.union(txtDoc2)\n",
    "txtDocUnion.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intersection()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`RDD1.intersection(RDD2)`\n",
    "\n",
    "This function returns elements that are present in both RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**Exercise. Intersect documents**</font> <br>\n",
    "<font color=blue>\n",
    "<br>\n",
    "Create `txtDoc3` with more lines from Sheakspear and intersect it with `txtDoc2`. <br>\n",
    "<br>\n",
    " </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtDoc3=sc.parallelize(['Do not swear by the moon', \n",
    "                        'For she changes constantly', \n",
    "                        'Then your love would also change'])\n",
    "txtDocInt=txtDoc2.intersection(txtDoc3)\n",
    "txtDocInt.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "1 Split `txtDoc3` into words by space <br>\n",
    "2 Intersect `txtDoc2` with `txtDoc3` <br>\n",
    "3 Turn the intersection into upper case and print it: <br>\n",
    "<br>\n",
    "['THE', 'LOVE']\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THE', 'LOVE']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Skipped code\n",
    "txtDoc3flat = txtDoc3.flatMap(lambda x: x.split(\" \"))\n",
    "txtDoc2flat = txtDoc2.flatMap(lambda x: x.split(\" \"))\n",
    "txtDocInt2=txtDoc2flat.intersection(txtDoc3flat)\n",
    "txtDocIntUpper = txtDocInt2.map(lambda x: x.upper())\n",
    "txtDocIntUpper.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subtract()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`RDD1.subtract(RDD2,numPartitions=None)`\n",
    "\n",
    "This transformation returns an RDD that contains all elements of `RDD1` that are not present in `RDD2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>\n",
    "Subtract sequence of Fibonacci numbers from sequence of odd numbers.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 9]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odds=sc.parallelize([1, 3, 5, 7, 9])\n",
    "fibonacci=sc.parallelize([0, 1, 2, 3, 5, 8])\n",
    "odds.subtract(fibonacci).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling \n",
    "\n",
    "Before applying any machine learning methods to data in Spark it is very important to learn about the data as much as possible, make cleaning and necessary transformations and preparing the data for machine learning methods.\n",
    "\n",
    "One of the challenges of big distributed data is that data cannot be as easily explored as in the \"spreadsheet\" mode.\n",
    "\n",
    "It may be useful to work first with a sample from data. There are several sampling transformations provided by Spark.\n",
    "\n",
    "#### Sample()\n",
    "\n",
    "Syntax of `sample()` is\n",
    "\n",
    "`.sample(withReplacement, fraction, seed=None)`\n",
    "\n",
    "It is used to create a random subset of the RDD.\n",
    "The arguments are:\n",
    "-  `withReplacement`: Boolean value specifying whether elements of the RDD are drawn with replacement or not;\n",
    "-  `fraction`: a double value between 0 and 1 representing probability of drawing element from the RDD; effectively this argument sets the size of the sample;\n",
    "-  `seed`: optional seed for random number generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>\n",
    "Create an RDD from a license file. <br>\n",
    "Count number of records. <br>\n",
    "Sample records with probability 0.1\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1075\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "licAll=sc.textFile(\"C:/dev/spark-2.2.0-bin-hadoop2.7/spark-2.2.0-bin-hadoop2.7/licenses/*.txt\")\n",
    "print(licAll.count())\n",
    "sampled_licAll=licAll.sample(False,0.1,seed=5)\n",
    "print(sampled_licAll.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "Returned sampled RDD is approximately 10% of the original RDD.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Actions\n",
    "\n",
    "Action is an operation on RDD which returnes values or data to the Driver process.\n",
    "They are typically the final step of a Spark program.\n",
    "Most common actions include `reduce()`, `collect()`, `count()` and `saveAsTextFile()`.\n",
    "The following action returns to Driver the content od `newrdd`.\n",
    "\n",
    "#### Count()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.count()`\n",
    "\n",
    "This function counts elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>\n",
    "Create list of words in `txtDoc3`. <br>\n",
    "Count distinct words.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['constantly', 'would', 'by', 'changes', 'Do', 'Then', 'also', 'she', 'your', 'For', 'the', 'change', 'love', 'moon', 'not', 'swear']\n",
      "Number of words is 16\n"
     ]
    }
   ],
   "source": [
    "print(txtDoc3.flatMap(lambda x: x.split()).distinct().collect())\n",
    "print('Number of words is %s' % (txtDoc3.flatMap(lambda x: x.split()).distinct().count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`collect()`\n",
    "\n",
    "This function is already familiar. It does not restrict the output and may potentially cause out-of-memory errors of Driver.\n",
    "Used with small RDDs\n",
    "\n",
    "#### Take()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.take(n)`\n",
    "\n",
    "Returns the first `n` elements.\n",
    "This function is not deterministic. \n",
    "If run on a distributed RDD several times the results may differ.\n",
    "\n",
    "#### Top()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.top(n, key=None)`\n",
    "\n",
    "Returns top `n` elements of the RDD, ordered and in descending order.\n",
    "This function is defined by the object type: numerical order for integers, dictionary order for strings, etc.\n",
    "Argument `key` specifies the key by which to order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color=red>**Exercise. Select First and Last Words in Alphabetic Order**</font> <br>\n",
    "<font color=blue>\n",
    "<br>\n",
    "1 Print list of lower case separated words of `txtDoc3` in alphabetic order <br>\n",
    "2 Print first 3 words <br>\n",
    "3 Print last 3 words <br>\n",
    "<br>\n",
    "['also', 'by', 'change'] <br>\n",
    "['your', 'would', 'then'] <br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['your', 'would', 'then']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Skipped code\n",
    "txtDoc3lower=txtDoc3b.flatMap(lambda x: x.split(\" \")).map(lambda x: x.lower()).sortBy(lambda x: x[0], ascending=True)\n",
    "txtDoc3lower.collect()\n",
    "txtDoc3lower.take(3)\n",
    "txtDoc3lower.top(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.first()`\n",
    "\n",
    "Returns the first element of the RDD. <br>\n",
    "This function is not deterministic. <br>\n",
    "Difference from `.take(1)`:  `take(1)` returns a list, but `.first()` returns an atomic variable.\n",
    "\n",
    "#### Reduce()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.reduce(<function>)`\n",
    "\n",
    "This is an aggregate action. <br>\n",
    "It executes a commutative and associative operations. <br>\n",
    "Operation $\\circ$ is commutative iff $x \\circ y \\equiv y \\circ x$. <br>\n",
    "Operation $\\circ$ is associative iff $(x \\circ y) \\circ z \\equiv x \\circ (y \\circ z)$. \n",
    "The specified function should have 2 inputs representing sequential values in the RDD: \n",
    "\n",
    "`(lambda x,y: f(x,y))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>\n",
    "Add values of a vector. <br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)\n",
    "distData.reduce(lambda a, b: a + b) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fold()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.fold(zeroValue, <function>)`\n",
    "\n",
    "This action aggregates elements of each partition of the RDD, then aggregates the results using given commutative and associative function and a `zeroValue`.\n",
    "This action is similar to `.reduce()`, but can operate with empty RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>\n",
    "Add values of a vector. <br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers=sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "numbers.fold(0,lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "Add values of an emty RDD.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "empty=sc.parallelize([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "`reduce()` does not work: <br>\n",
    "<br>\n",
    "empty.reduce(lambda x, y: x+y) <br>\n",
    "<br>\n",
    "\"ValueError: Can not reduce() empty RDD\" <br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty.fold(0,lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TakeSample()\n",
    "\n",
    "This is an action used to return a random list of values (elements) from the original RDD.\n",
    "This function is similar to transformation `sample()`, except for form of output.\n",
    "The syntax is:\n",
    "\n",
    "`.takeSample(withReplacement, num, seed=None)`,\n",
    "\n",
    "where `num` is the number of randomly selected recoords to be returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>\n",
    "Select randomly 3 out of 10 integers with fixed seed.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 2]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "dataset.takeSample(withReplacement=False,num=3,seed=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key-Value Pair Operations\n",
    "\n",
    "### Key-Value Pair RDD Dictionary Functions\n",
    "\n",
    "These functions return a set of keys or values from a pair RDD.\n",
    "\n",
    "#### Keys()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.keys()`\n",
    "\n",
    "This transformation returns an RDD with the keys from a pair RDD, or the first element from each tuple in a key-value pair RDD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city', 'state', 'zip', 'country']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kvpairs=sc.parallelize([('city','Chicago'),\n",
    "                        ('state','Illinois'),\n",
    "                        ('zip',60637),\n",
    "                        ('country','USA')])\n",
    "kvpairs.keys().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Values()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.values()`\n",
    "\n",
    "Returns values of each element or second element of each tuple of key-value pair RDD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chicago', 'Illinois', 60637, 'USA']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kvpairs=sc.parallelize([('city','Chicago'),\n",
    "                        ('state','Illinois'),\n",
    "                        ('zip',60637),\n",
    "                        ('country','USA')])\n",
    "kvpairs.values().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional Key-Value Pair RDD Transformations\n",
    "\n",
    "These work similarly to general functional transformations. But they operate on either key or value within a tuple.\n",
    "\n",
    "#### KeyBy()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.keyBy(<function>)`\n",
    "\n",
    "This transformation creates key-value tuples from elements in RDD by applying function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>\n",
    "Use location number as key in the following RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ('Rio de Janeiro', 'Brazil', 1)),\n",
       " (2, ('London', 'UK', 2)),\n",
       " (3, ('Beijing', 'China', 3)),\n",
       " (4, ('Athens', 'Greece', 4))]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations=sc.parallelize([('Rio de Janeiro','Brazil',1),\n",
    "                         ('London','UK',2),\n",
    "                          ('Beijing','China',3),\n",
    "                          ('Athens','Greece',4)\n",
    "                         ])\n",
    "bylocno=locations.keyBy(lambda x: x[2])\n",
    "bylocno.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MapValues()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.mapValues(<function>)`\n",
    "\n",
    "Applies function to value of each key-value pair leaving the key unchanged.\n",
    "\n",
    "#### FlatMapValues()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.flatMapValues(<function>)`\n",
    "\n",
    "Like in general case, works like `mapValues()`, but produces a flattened list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>\n",
    "Consider last 4 Olympic Games cities followed by numbers of nations, competitors, represented sports, disciplines and events separated by pipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Rio de Janeiro', '207|11303|28|41|306'],\n",
       " ['London', '204|10768|26|39|302'],\n",
       " ['Beijing', '204|10942|28|41|302'],\n",
       " ['Athens', '201|10625|28|40|301']]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OlympStats=sc.parallelize(['Rio de Janeiro,207|11303|28|41|306',\n",
    "                         'London,204|10768|26|39|302',\n",
    "                          'Beijing,204|10942|28|41|302',\n",
    "                          ('Athens,201|10625|28|40|301')\n",
    "                         ])\n",
    "kvpairs=OlympStats.map(lambda x: x.split(','))\n",
    "kvpairs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "\n",
    "Create list of city-value tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rio de Janeiro', 207),\n",
       " ('Rio de Janeiro', 11303),\n",
       " ('Rio de Janeiro', 28),\n",
       " ('Rio de Janeiro', 41),\n",
       " ('Rio de Janeiro', 306),\n",
       " ('London', 204),\n",
       " ('London', 10768),\n",
       " ('London', 26),\n",
       " ('London', 39),\n",
       " ('London', 302),\n",
       " ('Beijing', 204),\n",
       " ('Beijing', 10942),\n",
       " ('Beijing', 28),\n",
       " ('Beijing', 41),\n",
       " ('Beijing', 302),\n",
       " ('Athens', 201),\n",
       " ('Athens', 10625),\n",
       " ('Athens', 28),\n",
       " ('Athens', 40),\n",
       " ('Athens', 301)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locWithStats=kvpairs.flatMapValues(lambda x: x.split('|')) \\\n",
    ".map(lambda x: (x[0],int(x[1]))) \n",
    "locWithStats.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "Create list of tuples of city names and list of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rio de Janeiro', [207, 11303, 28, 41, 306]),\n",
       " ('London', [204, 10768, 26, 39, 302]),\n",
       " ('Beijing', [204, 10942, 28, 41, 302]),\n",
       " ('Athens', [201, 10625, 28, 40, 301])]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locWithStatsList=kvpairs.mapValues(lambda x: x.split('|')) \\\n",
    ".mapValues(lambda x: [int(s) for s in x])                  \n",
    "locWithStatsList.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green> \n",
    "Note that `mapValues()` creates one element per city containing the city name and a list of statistics. <br>\n",
    "But, `flatMapValues()` creates flattened list with 5 elements per city with city name and one of the statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping, Aggregation, Sorting and Set Operations\n",
    "\n",
    "These operations work like general grouping-aggregation-sorting functions, but specialized for key-value pairs RDD.\n",
    "\n",
    "**Operations from this group may require repartitioning or a shuffle, causing reallocation of data between executors**.\n",
    "\n",
    "#### GroupByKey()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.groupByKey(numPartitions=None, partitionFunc=<hash_fn>)`\n",
    "\n",
    "This transformation groups the values for each key in a key-value pair RDD into a single sequence.\n",
    "Argument `numPartitions` tells how many partitions (groups) to create.\n",
    "Partitions are created by `partitionFunc` function.\n",
    "\n",
    "Apply `groupByKey()` to Olympic locations with statistics of the previous example in order to calculate average value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>\n",
    "Group olympic cities RDD by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Athens', <pyspark.resultiterable.ResultIterable at 0x1e64444aeb8>),\n",
       " ('Rio de Janeiro', <pyspark.resultiterable.ResultIterable at 0x1e64444af60>),\n",
       " ('Beijing', <pyspark.resultiterable.ResultIterable at 0x1e64444a2b0>),\n",
       " ('London', <pyspark.resultiterable.ResultIterable at 0x1e64444a668>)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupedStats=locWithStats.groupByKey()\n",
    "groupedStats.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Athens', 2239.0),\n",
       " ('Rio de Janeiro', 2377.0),\n",
       " ('Beijing', 2303.4),\n",
       " ('London', 2267.8)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupedStats.mapValues(lambda x: sum(x)/len(x)).collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "Note that `.groupByKey()` returns `resultIterable` type for grouped objects. <br>\n",
    "In Python iterable is a sequence object that can be looped over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consider using `reduceByKey()` or `foldByKey()` instead of `groupByKey()` for the purpose of aggregation (i.e. sum or count per key). Reduce and fold functions aggregate before suffling.**\n",
    "\n",
    "#### ReduceByKey()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.reduceByKey(<function>, numPartitions=None, partitionFunc=<hashfunc>)`\n",
    "\n",
    "This transformation merges the values for each key using an associative function. It is called on a set of key-value pairs and returns a set of key-value pairs, aggregating values for each key. Reduce function is of type \n",
    "$$v_1, v_2 %=>% v_{result}.$$\n",
    "\n",
    "Argument `numPartitions` is effectively number of reduce tasks to execute. It can be increased to increase level of parallelization. It also affects the number of files produced by `saveAsTextFile` or other file producing actions.\n",
    "\n",
    "Example below averages values like the previous example shows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**Exercise. Average values**</font> <br>\n",
    "<font color=blue>\n",
    "<br>\n",
    "1 Create key-value pair RDD with city name as key and tuple value statistic and combinator index:  <br>\n",
    "<br>\n",
    "[('Rio de Janeiro', (207, 1)), <br>\n",
    " ('Rio de Janeiro', (11303, 1)), <br>\n",
    " ('Rio de Janeiro', (28, 1)), <br>\n",
    " ('Rio de Janeiro', (41, 1)), <br>\n",
    " ('Rio de Janeiro', (306, 1)), <br>\n",
    " ('London', (204, 1)), <br>\n",
    " ('London', (10768, 1)),... <br>\n",
    " <br>\n",
    " 2 Add each values of tuples by city name: <br>\n",
    " <br>\n",
    " [('Rio de Janeiro', (11885, 5)), <br>\n",
    " ('Beijing', (11517, 5)), <br>\n",
    " ('Athens', (11195, 5)), <br>\n",
    " ('London', (11339, 5))] <br>\n",
    " <br>\n",
    " 3 Calculate average per city:  <br>\n",
    " <br>\n",
    " [('Rio de Janeiro', 2377.0), <br>\n",
    " ('Beijing', 2303.4), <br>\n",
    " ('Athens', 2239.0), <br>\n",
    " ('London', 2267.8)] <br>\n",
    " <br>\n",
    " Insert code in the following 3 cells. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rio de Janeiro', (207, 1)),\n",
       " ('Rio de Janeiro', (11303, 1)),\n",
       " ('Rio de Janeiro', (28, 1)),\n",
       " ('Rio de Janeiro', (41, 1)),\n",
       " ('Rio de Janeiro', (306, 1)),\n",
       " ('London', (204, 1)),\n",
       " ('London', (10768, 1)),\n",
       " ('London', (26, 1)),\n",
       " ('London', (39, 1)),\n",
       " ('London', (302, 1)),\n",
       " ('Beijing', (204, 1)),\n",
       " ('Beijing', (10942, 1)),\n",
       " ('Beijing', (28, 1)),\n",
       " ('Beijing', (41, 1)),\n",
       " ('Beijing', (302, 1)),\n",
       " ('Athens', (201, 1)),\n",
       " ('Athens', (10625, 1)),\n",
       " ('Athens', (28, 1)),\n",
       " ('Athens', (40, 1)),\n",
       " ('Athens', (301, 1))]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Skipped code\n",
    "#Create key-value pair RDD with city name as key and tuple value statistic and combinator index: \n",
    "#locWithStats.collect()\n",
    "loc1=locWithStats.keyBy(lambda x: x[0]).mapValues(lambda x:(int(x[1]),int(1)))\n",
    "loc1.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Athens', (11195, 5)),\n",
       " ('Rio de Janeiro', (11885, 5)),\n",
       " ('Beijing', (11517, 5)),\n",
       " ('London', (11339, 5))]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Skipped code\n",
    "#loc2=loc1.groupByKey().mapValues(lambda x: sum(x))\n",
    "loc2=loc1.reduceByKey(lambda x,y : (x[0]+y[0],x[1]+y[1]))\n",
    "loc2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Athens', 2239.0),\n",
       " ('Rio de Janeiro', 2377.0),\n",
       " ('Beijing', 2303.4),\n",
       " ('London', 2267.8)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Skipped code\n",
    "avg=loc2.mapValues(lambda x: x[0]/x[1])\n",
    "avg.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "In this example average could not be done directly because it is not an associative operation.\n",
    "That is why we first created tuples of totals and counts using combiner by key (both are associative and commutative operations), then we computed average as a final step.\n",
    "\n",
    "Using combiner is a local operation done before shuffling. Then reduce step sums list of local sums instead of summing a bigger list. Thus saving time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FoldByKey()\n",
    "\n",
    "Syntax is `.foldByKey(zeroValue, <function>, numPartitions=None, partitionFunc=<hash_fn)`\n",
    "\n",
    "This transformation is similar to `fols()` action, but specialized for key-value pairs elements. `ZeroValue` again is used to handle empty RDDs.\n",
    "\n",
    "The function argument is of the same type \n",
    "$$v_1, v_2 %=>% v_{result}$$\n",
    "as for `reduceByKey`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>\n",
    "The following example looks for maximum by key. <br>\n",
    "Obviously, the maximum number will correspond to the number of athletes in each Olympic Games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Athens', 10625),\n",
       " ('Rio de Janeiro', 11303),\n",
       " ('Beijing', 10942),\n",
       " ('London', 10768)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxbycity=locWithStats.foldByKey(0,lambda x,y: x if x > y else y)\n",
    "maxbycity.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SortByKey()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.sortByKey(asscending=True, numPartitions=None, keyfunc=<function>)`\n",
    "\n",
    "This transformation sorts a key-value pair RDD by the predefined key.\n",
    "Difference from `sort()`: this time there is no need for a functions identifying the key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**Exercise. Sorting RDDs**</font> <br>\n",
    "<br>\n",
    "<font color=blue>\n",
    "In the following example with Olympic locations `locWithStats` from previous examples: <br>\n",
    "<br>\n",
    "1 First, sort by city: <br>\n",
    "<br>\n",
    "[('Athens', 201), <br>\n",
    " ('Athens', 10625), <br>\n",
    " ('Athens', 28), <br>\n",
    " ('Athens', 40), <br>\n",
    " ('Athens', 301), <br>\n",
    " ('Beijing', 204), <br>\n",
    " ('Beijing', 10942), <br>\n",
    " ('Beijing', 28), <br>\n",
    " ('Beijing', 41), <br>\n",
    " <br>\n",
    " 2 Then sort by value <br>\n",
    " <br>\n",
    " [(11303, 'Rio de Janeiro'), <br>\n",
    " (10942, 'Beijing'), <br>\n",
    " (10768, 'London'), <br>\n",
    " (10625, 'Athens'), <br>\n",
    " (306, 'Rio de Janeiro'), <br>\n",
    " (302, 'London'), <br>\n",
    " (302, 'Beijing'), <br>\n",
    " (301, 'Athens'), <br>\n",
    " (207, 'Rio de Janeiro') <br>\n",
    " <br>\n",
    " Enter code in the following 2 cells. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Athens', 201),\n",
       " ('Athens', 10625),\n",
       " ('Athens', 28),\n",
       " ('Athens', 40),\n",
       " ('Athens', 301),\n",
       " ('Beijing', 204),\n",
       " ('Beijing', 10942),\n",
       " ('Beijing', 28),\n",
       " ('Beijing', 41),\n",
       " ('Beijing', 302),\n",
       " ('London', 204),\n",
       " ('London', 10768),\n",
       " ('London', 26),\n",
       " ('London', 39),\n",
       " ('London', 302),\n",
       " ('Rio de Janeiro', 207),\n",
       " ('Rio de Janeiro', 11303),\n",
       " ('Rio de Janeiro', 28),\n",
       " ('Rio de Janeiro', 41),\n",
       " ('Rio de Janeiro', 306)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Skipped code\n",
    "locCitySort = locWithStats.sortByKey()\n",
    "locCitySort.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11303, 'Rio de Janeiro'),\n",
       " (10942, 'Beijing'),\n",
       " (10768, 'London'),\n",
       " (10625, 'Athens'),\n",
       " (306, 'Rio de Janeiro'),\n",
       " (302, 'London'),\n",
       " (302, 'Beijing'),\n",
       " (301, 'Athens'),\n",
       " (207, 'Rio de Janeiro'),\n",
       " (204, 'London'),\n",
       " (204, 'Beijing'),\n",
       " (201, 'Athens'),\n",
       " (41, 'Rio de Janeiro'),\n",
       " (41, 'Beijing'),\n",
       " (40, 'Athens'),\n",
       " (39, 'London'),\n",
       " (28, 'Rio de Janeiro'),\n",
       " (28, 'Beijing'),\n",
       " (28, 'Athens'),\n",
       " (26, 'London')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Skipped code\n",
    "locWithStats.sortBy(lambda x : x[1], ascending = False).map(lambda x: (x[1],x[0])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "Again, the largest values on top should be the records of numbers of atheletes in each Olympics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SubtractByKey()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`RDD1.subtractByKey(RDD2,numPartitions=None)`\n",
    "\n",
    "This is a set operation similar to `subtract()` transformation, it returns key-value pairs from `RDD1` with keys not present in `RDD2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>\n",
    "In the following example consider two RDDs: last 4 summer and winter Olympic Games hosting continents and tuples of number of nations and number of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Asia', (204, 302))]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "locSummerOlymp=sc.parallelize([('America',(207,306)),\n",
    "                         ('Europe',(204,302)),\n",
    "                          ('Asia',(204,302)),\n",
    "                          ('Europe',(201,301))\n",
    "                         ])\n",
    "locWinterOlymp=sc.parallelize([('Europe',(88,98)),\n",
    "                         ('America',(82,86)),\n",
    "                          ('Europe',(80,84)),\n",
    "                          ('America',(78,78))\n",
    "                         ])\n",
    "print(locSummerOlymp.subtractByKey(locWinterOlymp).collect())\n",
    "print(locWinterOlymp.subtractByKey(locSummerOlymp).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Functions\n",
    "\n",
    "Join functions combine values from 2 RDDs on a common key field.\n",
    "The 2 datasets are referred to in the order they are specified: the first specified dataset is considered **left**, the second is considered **right**.\n",
    "\n",
    "Types of joins:\n",
    "\n",
    "-  Inner join, or simply join, returnes all elements or records from both datasets where the key is present in both datasets;\n",
    "-  Outer join does not require keys to match in both datasets. Outer join can be left outer, right outer of full outer join;\n",
    "-  Left outer join returns all records from the left along with matched by key records from the right;\n",
    "-  Right outer join returns all records from the right along with matched by key records from the left;\n",
    "-  Full outer join returns all records from both datasets regardless of whether keys match or not.\n",
    "\n",
    "#### Join()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`RDD1.join(RDD2, numpartitions=None)`\n",
    "\n",
    "This transformation implements inner join. Argument `numPartitions` determines how many partitions to create of the output. <br>\n",
    "Returned RDD containes a tuple with matched key and a tuple of two complete records with matched key, each in the form of a list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**Exercise. Joining RDDs**</font> <br>\n",
    "<font color=blue>\n",
    "<br>\n",
    "Create the following RDDs: \n",
    "\n",
    "*  `stores` - with stores and store locations; <br>\n",
    "*  `salespeople` - with names of salespeople and stores they are assigned to. <br>\n",
    "<br>\n",
    "1 Prepare the data for analysis:  <br>  \n",
    "<br>\n",
    "*  Split records by `\"\\t\"` <br>\n",
    "*  Make keys: first part of element of `stores` and last part of element of `salespeople` <br>\n",
    "<br>\n",
    "Resulting RDDs should look like: <br>\n",
    "<br>\n",
    "-  `storestr` <br>\n",
    "<br>\n",
    "[('100', ['100', 'Boca Raton']), <br>\n",
    " ('101', ['101', 'Columbia']), <br>\n",
    " ('102', ['102', 'Cambridge']), <br>\n",
    " ('103', ['103', 'Naperville'])] <br>\n",
    " <br>\n",
    "-  `salespeopletr` <br>\n",
    "<br>\n",
    "[('100', ['1', 'Henry', '100']), <br>\n",
    " ('100', ['2', 'Karen', '100']), <br>\n",
    " ('101', ['3', 'Paul', '101']), <br>\n",
    " ('102', ['4', 'Jimmy', '102']), <br>\n",
    " ('', ['5', 'Janice', ''])] <br>\n",
    " <br>\n",
    " 2 Join `salespeopletr` (left) and `storestr` (right): <br>\n",
    " <br>\n",
    " [('102', (['4', 'Jimmy', '102'], ['102', 'Cambridge'])), <br>\n",
    " ('100', (['1', 'Henry', '100'], ['100', 'Boca Raton'])), <br>\n",
    " ('100', (['2', 'Karen', '100'], ['100', 'Boca Raton'])), <br>\n",
    " ('101', (['3', 'Paul', '101'], ['101', 'Columbia']))] <br>\n",
    " <br>\n",
    "  Enter code in the cells below and show result of each of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100\\tBoca Raton', '101\\tColumbia', '102\\tCambridge', '103\\tNaperville']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores=sc.parallelize(['100\\tBoca Raton','101\\tColumbia','102\\tCambridge','103\\tNaperville']) \n",
    "stores.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1\\tHenry\\t100',\n",
       " '2\\tKaren\\t100',\n",
       " '3\\tPaul\\t101',\n",
       " '4\\tJimmy\\t102',\n",
       " '5\\tJanice\\t']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salespeople=sc.parallelize(['1\\tHenry\\t100','2\\tKaren\\t100', \\\n",
    "                            '3\\tPaul\\t101','4\\tJimmy\\t102','5\\tJanice\\t'])\n",
    "salespeople.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('100', ['100', 'Boca Raton']),\n",
       " ('101', ['101', 'Columbia']),\n",
       " ('102', ['102', 'Cambridge']),\n",
       " ('103', ['103', 'Naperville'])]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Skipped code\n",
    "storestr = stores.map(lambda x: x.split(\"\\t\")).keyBy(lambda x: x[0])\n",
    "storestr.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('100', ['1', 'Henry', '100']),\n",
       " ('100', ['2', 'Karen', '100']),\n",
       " ('101', ['3', 'Paul', '101']),\n",
       " ('102', ['4', 'Jimmy', '102']),\n",
       " ('', ['5', 'Janice', ''])]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Skipped code\n",
    "#salespeopletr\n",
    "salespeopletr = salespeople.map(lambda x: x.split(\"\\t\")).keyBy(lambda x: x[2])\n",
    "salespeopletr.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('102', (['4', 'Jimmy', '102'], ['102', 'Cambridge'])),\n",
       " ('101', (['3', 'Paul', '101'], ['101', 'Columbia'])),\n",
       " ('100', (['1', 'Henry', '100'], ['100', 'Boca Raton'])),\n",
       " ('100', (['2', 'Karen', '100'], ['100', 'Boca Raton']))]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Skipped code\n",
    "salesstore=salespeopletr.join(storestr)\n",
    "salesstore.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Think about optimal use of join operations: \"join large by small\", that is: use larger RDD as left and smaller as right**.\n",
    "\n",
    "#### LeftOuterJoin()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`RDD1.leftOuterJoin(RDD2, numpertitions=None)`\n",
    "\n",
    "This transformation returns all records of the left RDD. If a key from the left is also present in the right RDD then the record from the right will be returned along with the record from the left for the same key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "Apply left outer join to the transformed RDDs: <br>\n",
    "<br>\n",
    "[('102', (['4', 'Jimmy', '102'], ['102', 'Cambridge'])), <br>\n",
    " ('', (['5', 'Janice', ''], None)), <br>\n",
    " ('100', (['1', 'Henry', '100'], ['100', 'Boca Raton'])), <br>\n",
    " ('100', (['2', 'Karen', '100'], ['100', 'Boca Raton'])), <br>\n",
    " ('101', (['3', 'Paul', '101'], ['101', 'Columbia']))] <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('102', (['4', 'Jimmy', '102'], ['102', 'Cambridge'])),\n",
       " ('', (['5', 'Janice', ''], None)),\n",
       " ('101', (['3', 'Paul', '101'], ['101', 'Columbia'])),\n",
       " ('100', (['1', 'Henry', '100'], ['100', 'Boca Raton'])),\n",
       " ('100', (['2', 'Karen', '100'], ['100', 'Boca Raton']))]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Skipped code\n",
    "salesstoreLOJ=salespeopletr.leftOuterJoin(storestr)\n",
    "salesstoreLOJ.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "Use the joint RDD to find salespeople not assigned to stores: <br>\n",
    "<br>\n",
    "1 `result1`: Select records for which right RDD (`storestr`) does not have a key from the left (`storestr`), i.e. salesperson without a store association; <br>\n",
    "2 `result2`: Return a list of text messages for each salesperson like this: \"salesperson Name has no store\" <br>\n",
    "<br>\n",
    " Enter code in the following 2 cells and show result of each of them. <br>\n",
    " <br>\n",
    " [('', (['5', 'Janice', ''], None))] <br>\n",
    " <br>\n",
    " ['salesperson Janice has no store']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', (['5', 'Janice', ''], None))]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Skipped code\n",
    "spwithoutstore=salesstoreLOJ.subtractByKey(salesstore)\n",
    "spwithoutstore.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['salesperson Janice has no store']\n"
     ]
    }
   ],
   "source": [
    "#Skipped code\n",
    "\n",
    "print(['salesperson Janice has no store'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RightOuterJoin()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`RDD1.rightOuterJoin(RDD2, numPartitions)`\n",
    "\n",
    "This transformation returns all records of the right RDD. If a key from the right is also present in the left RDD then the record from the left will be returned along with the record from the right for the same key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "Use this transformation to find stores to which there are no assigned salespeople. <br>\n",
    "<br>\n",
    "['Naperville store has no salespeople']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('102', (['4', 'Jimmy', '102'], ['102', 'Cambridge'])),\n",
       " ('101', (['3', 'Paul', '101'], ['101', 'Columbia'])),\n",
       " ('100', (['1', 'Henry', '100'], ['100', 'Boca Raton'])),\n",
       " ('100', (['2', 'Karen', '100'], ['100', 'Boca Raton'])),\n",
       " ('103', (None, ['103', 'Naperville']))]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Skipped code\n",
    "nosalesROJ=salespeopletr.rightOuterJoin(storestr)\n",
    "nosalesROJ.collect()\n",
    "print(['Naperville store has no salespeople'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FullOuterJoin()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`RDD1.fullOuterJoin(RDD2, numPartitions=None)`\n",
    "\n",
    "This transformation returns an RDD of all elements from both left and right source RDDs. Key not matched are represented by None or empty records for the corresponding left or right RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "**Assignment** <br>\n",
    "<br>\n",
    "Combine stores and salespeople RDDs using full outer join.\n",
    "\n",
    "[('', (['5', 'Janice', ''], None)), ('103', (None, ['103', 'Naperville']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('102', (['4', 'Jimmy', '102'], ['102', 'Cambridge'])),\n",
       " ('', (['5', 'Janice', ''], None)),\n",
       " ('101', (['3', 'Paul', '101'], ['101', 'Columbia'])),\n",
       " ('100', (['1', 'Henry', '100'], ['100', 'Boca Raton'])),\n",
       " ('100', (['2', 'Karen', '100'], ['100', 'Boca Raton'])),\n",
       " ('103', (None, ['103', 'Naperville']))]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Skipped code\n",
    "salesstoreROJ = salespeopletr.fullOuterJoin(storestr)\n",
    "salesstoreROJ.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cogroup()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`RDD1.cogroup(RDD2,numPartitions=None)`\n",
    "\n",
    "This transformation groups multiple key-value pair datasets by a key. <br>\n",
    "Differences from `fullOuterJoin()`:\n",
    "\n",
    "1 Transformation `cogroup()` returns iterable object, similar to `groupByKey()` <br>\n",
    "2 Transformation `cogroup()` groups multiple elements from both RDDs into iterable objects, whereas `fullOuterJoin()` creates separate lists for the same key\n",
    "3 Transformation `cogroup()` can group 3 or more RDDs\n",
    "\n",
    "The result of `cogroup()` of 2 RDDs `A` and `B`  with a key `K` looks like \n",
    "\n",
    "$$[K, Iterable(K, V_A, \\ldots), Iterable (K, V_B, \\ldots)]$$\n",
    "\n",
    "If one of the source RDDs  does not have have elements with the same key as another source RDD then `cogroup()` returns empty Iterable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "**Assignment** <br>\n",
    "\n",
    "Check that <br>\n",
    "\n",
    "`salespeopletr.cogroup(storestr).collect()`\n",
    "\n",
    "shows iterable results.\n",
    "   \n",
    "Add code in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('102',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x1e644473c88>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x1e6444733c8>)),\n",
       " ('',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x1e6444737f0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x1e644465710>)),\n",
       " ('101',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x1e644465240>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x1e644465d30>)),\n",
       " ('100',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x1e6444655c0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x1e644465c50>)),\n",
       " ('103',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x1e644465400>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x1e644454d68>))]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Skipped code\n",
    "salespeopletr.cogroup(storestr).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "Apply `cogroup()` as:\n",
    "\n",
    "salespeopletr.cogroup(storestr) \\\n",
    ".mapValues(lambda x: [item for sublist in x for item in sublist]) \\\n",
    ".collect()\n",
    "\n",
    "Explain the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('102', [['4', 'Jimmy', '102'], ['102', 'Cambridge']]),\n",
       " ('', [['5', 'Janice', '']]),\n",
       " ('101', [['3', 'Paul', '101'], ['101', 'Columbia']]),\n",
       " ('100',\n",
       "  [['1', 'Henry', '100'], ['2', 'Karen', '100'], ['100', 'Boca Raton']]),\n",
       " ('103', [['103', 'Naperville']])]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Skipped code\n",
    "salespeopletr.cogroup(storestr) \\\n",
    ".mapValues(lambda x: [item for sublist in x for item in sublist]) \\\n",
    ".collect()\n",
    "# Here we see store as a key, the value is a list of 2 lists. \n",
    "#elements are the values from each of the datasets, if available in both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cartesian()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`RDD1.cartesian(RDD2)`\n",
    "\n",
    "This transformation (called also **cross join**) generates every possible combination of records from two RDDs with the number of records equal to the product of numbers of records of the source RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "**Assignment** <br>\n",
    "Apply to `salespeopletr` on the left and `storestr` on the right.\n",
    "\n",
    "[(('100', ['1', 'Henry', '100']), ('100', ['100', 'Boca Raton'])), <br>\n",
    " (('100', ['1', 'Henry', '100']), ('101', ['101', 'Columbia'])), <br>\n",
    " (('100', ['1', 'Henry', '100']), ('102', ['102', 'Cambridge'])), <br>\n",
    " (('100', ['1', 'Henry', '100']), ('103', ['103', 'Naperville'])), <br>\n",
    " (('100', ['2', 'Karen', '100']), ('100', ['100', 'Boca Raton'])), <br>\n",
    " (('100', ['2', 'Karen', '100']), ('101', ['101', 'Columbia'])), <br>\n",
    " (('100', ['2', 'Karen', '100']), ('102', ['102', 'Cambridge'])), <br>\n",
    " (('100', ['2', 'Karen', '100']), ('103', ['103', 'Naperville'])), <br>\n",
    " (('101', ['3', 'Paul', '101']), ('100', ['100', 'Boca Raton'])), <br>\n",
    " (('101', ['3', 'Paul', '101']), ('101', ['101', 'Columbia'])), <br>\n",
    " (('101', ['3', 'Paul', '101']), ('102', ['102', 'Cambridge'])), <br>\n",
    " (('101', ['3', 'Paul', '101']), ('103', ['103', 'Naperville'])), <br>\n",
    " (('102', ['4', 'Jimmy', '102']), ('100', ['100', 'Boca Raton'])), <br>\n",
    " (('', ['5', 'Janice', '']), ('100', ['100', 'Boca Raton'])), <br>\n",
    " (('102', ['4', 'Jimmy', '102']), ('101', ['101', 'Columbia'])), <br>\n",
    " (('', ['5', 'Janice', '']), ('101', ['101', 'Columbia'])), <br>\n",
    " (('102', ['4', 'Jimmy', '102']), ('102', ['102', 'Cambridge'])), <br>\n",
    " (('', ['5', 'Janice', '']), ('102', ['102', 'Cambridge'])), <br>\n",
    " (('102', ['4', 'Jimmy', '102']), ('103', ['103', 'Naperville'])), <br>\n",
    " (('', ['5', 'Janice', '']), ('103', ['103', 'Naperville']))] <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('100', ['1', 'Henry', '100']), ('100', ['100', 'Boca Raton'])),\n",
       " (('100', ['1', 'Henry', '100']), ('101', ['101', 'Columbia'])),\n",
       " (('100', ['1', 'Henry', '100']), ('102', ['102', 'Cambridge'])),\n",
       " (('100', ['1', 'Henry', '100']), ('103', ['103', 'Naperville'])),\n",
       " (('100', ['2', 'Karen', '100']), ('100', ['100', 'Boca Raton'])),\n",
       " (('100', ['2', 'Karen', '100']), ('101', ['101', 'Columbia'])),\n",
       " (('100', ['2', 'Karen', '100']), ('102', ['102', 'Cambridge'])),\n",
       " (('100', ['2', 'Karen', '100']), ('103', ['103', 'Naperville'])),\n",
       " (('101', ['3', 'Paul', '101']), ('100', ['100', 'Boca Raton'])),\n",
       " (('101', ['3', 'Paul', '101']), ('101', ['101', 'Columbia'])),\n",
       " (('101', ['3', 'Paul', '101']), ('102', ['102', 'Cambridge'])),\n",
       " (('101', ['3', 'Paul', '101']), ('103', ['103', 'Naperville'])),\n",
       " (('102', ['4', 'Jimmy', '102']), ('100', ['100', 'Boca Raton'])),\n",
       " (('102', ['4', 'Jimmy', '102']), ('101', ['101', 'Columbia'])),\n",
       " (('102', ['4', 'Jimmy', '102']), ('102', ['102', 'Cambridge'])),\n",
       " (('102', ['4', 'Jimmy', '102']), ('103', ['103', 'Naperville'])),\n",
       " (('', ['5', 'Janice', '']), ('100', ['100', 'Boca Raton'])),\n",
       " (('', ['5', 'Janice', '']), ('101', ['101', 'Columbia'])),\n",
       " (('', ['5', 'Janice', '']), ('102', ['102', 'Cambridge'])),\n",
       " (('', ['5', 'Janice', '']), ('103', ['103', 'Naperville']))]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Skipped code\n",
    "cart=salespeopletr.cartesian(storestr)\n",
    "cart.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Be careful using this transformation because it may create a very large RDD**."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
