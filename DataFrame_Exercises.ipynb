{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MScA, Advanced Machine Learning (32009)\n",
    "\n",
    "# Week 2 Workshop: More Programming Tools\n",
    "\n",
    "## Yuri Balasanov, &copy; iLykei, 2017\n",
    "\n",
    "Main text: Jeffrey Aven, Sams Teach Yourself Apache SPARK in 24 Hours,Pearson Education, Inc., 2017 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking Jobs, Stages and Storage\n",
    "\n",
    "Main principle of working with RDD is keeping them in memory as much as possible and discarding them as soon as they are not needed anymore.\n",
    "In case when an RDD needsto be reused it can be cached or persisted to disk.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>\n",
    "In the following example `newrdd` is created, processed and recomputed again to reprocess it.\n",
    "\n",
    "Type in your browser http://localhost:4040/storage/ to see the log of scheduler jobs and stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "originalrdd=sc.parallelize([0,1,2,3,4,5,6,7,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "This command creates a new RDD, but it is not executed yet (lazy execution). <br>\n",
    "There are no new records in the log at port `:4040`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "originalrdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "Function `.collect()` forces evaluation: scheduler log shows one collect job and stage 0 completed. Nomemory used according to Storage tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newrdd=originalrdd.filter(lambda x: x % 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "RDD `newrdd` created, but not evaluated yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noelements=newrdd.count()       # processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "First time processing `newrdd` is `count`. It is reflected as new job and stage 1.\n",
    "Click on each stage to see the corresponding DAG. <br>\n",
    "Stage 0 is `parallelize`, stage 1 is `count`. <br>\n",
    "Check timeline of each job. <br>\n",
    "There is still no storage used. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 elements in the collection [1, 3, 5, 7]\n"
     ]
    }
   ],
   "source": [
    "listofelements=newrdd.collect() # re-processing\n",
    "print(\"There are %s elements in the collection %s\" % (noelements,listofelements))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "RDD `newrdd` is reprocessed: collect action caused reevaluating the RDD because object `newrdd` was not stored. <br>\n",
    "Find the corresponding DAG in the log. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "<br>\n",
    "Now persist newrdd to avoid re-computing it. <br>\n",
    "Shutdown and restart the notebook Kernel to reset the scheduler log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "originalrdd=sc.parallelize([0,1,2,3,4,5,6,7,8])\n",
    "newrdd=originalrdd.filter(lambda x: x % 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "Again, for the first time `originalrdd` is transformed to make `newrdd`. <br>\n",
    "There is no information in the log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newrdd.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "The RDD object persisted to disk, but no records appeared in the log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noelements=newrdd.count()       # processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "First processing forced evaluation. <br>\n",
    "There is new `Job 0` in the log: DAG of `Stage 0` shows the process from creation by `parallelize` to `persist` to `count`. <br>\n",
    "All partitions of RDD are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 elements in the collection [1, 3, 5, 7]\n"
     ]
    }
   ],
   "source": [
    "listofelements=newrdd.collect() # re-processing\n",
    "print(\"There are %s elements in the collection %s\" % (noelements,listofelements))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "Log shows that the same DAG was executed for reevaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Lineage and Storage Level\n",
    "\n",
    "Spark plans execution of a program as Directed Acyclic Graph (**DAG**). \n",
    "DAG is a sequence of operations separated into stages.\n",
    "Some stages, like map, are completely parallelized, others, like reduce require a shuffle.\n",
    "This results in stage dependency.\n",
    "\n",
    "The Spark driver keeps track of RDD lineages and is able to reevaluate any RDDs when necessary.\n",
    "\n",
    "Summary of lineage execution can be observed by `toDebugString()`. <br>  \n",
    "\n",
    "RDDs are stored in their partitions on worker nodes.\n",
    "There are 6 main storage levels available to RDDs: <br>\n",
    "\n",
    "1 MEMORY_ONLY: default level; <br>\n",
    "2 MEMORY_AND_DISK: RDDs are stored in memory and if they do not fit in memory are stored on disk; <br>\n",
    "3 MEMORY_ONLY_SER: RDD partitions are stored as serialized (encoded) objects in memory; serialization requires less memory; <br>\n",
    "4 MEMORY_AND_DISK_SER: Serialized RDD partitions are stored in memory and if they don't fit are spilled to disk; <br>\n",
    "5 DISK_ONLY: RDD partitions are stored on disk only; <br>\n",
    "6 OFF_HEAP: RDDs are stored using external storage system. <br>\n",
    "\n",
    "Storage level is regulated by flags in StorageClass constructor: <br>\n",
    "\n",
    "`StorageLevel(useDisk, useMemory, useOfHeap, deserialized, replication=1)`\n",
    "\n",
    "All flage, but `replication` are Boolean, `replication` is an integer defaulting to 1.\n",
    "\n",
    "Storage levels codes in flags: <br>\n",
    "\n",
    "MEMORY_ONLY=(False, True, False, True, 1) <br>\n",
    "MEMORY_AND_DISK=(True, True, False, True,1) <br>\n",
    "MEMORY_ONLY_SER=(False, True, False, False,1) <br>\n",
    "MEMORY_AND_DISK_SER=(True, True, False, False,1) <br>\n",
    "DISK_ONLY-(True, False, False, False,1) <br>\n",
    "\n",
    "Check storage level by `getStorageLevel()` <br>\n",
    "\n",
    "Choosing storage level helps tuning Spark jobs and accomodating large scale operations that may not fit in default memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>\n",
    "Consider example of RDD lineage. <br>\n",
    "Shutdown and Restart Kernel to reset scheduler log before running the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['These violent delights have violent ends',\n",
       " 'And in their triump die like fire and powder',\n",
       " 'Which as they kiss consume']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtDoc=sc.parallelize(['These violent delights have violent ends',\n",
    "                       'And in their triump die like fire and powder',\n",
    "                       'Which as they kiss consume'])\n",
    "txtDoc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(8) ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:480 []'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, False, False, False, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(txtDoc.toDebugString())\n",
    "txtDoc.getStorageLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "RDD has been created and evaluated by `.collect()`; `toDebugString()` shows the same information that can be found in the DAG. <br>\n",
    "Level of storage is not specified: all flags are `False`. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(8) PythonRDD[1] at RDD at PythonRDD.scala:48 []\\n |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:480 []'\n",
      "['These', 'violent', 'delights', 'have', 'violent', 'ends', 'And', 'in', 'their', 'triump', 'die', 'like', 'fire', 'and', 'powder', 'Which', 'as', 'they', 'kiss', 'consume']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, False, False, False, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=txtDoc.flatMap(lambda x: x.split())\n",
    "print(words.toDebugString())\n",
    "print(words.collect())\n",
    "words.getStorageLevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "There has been new transformation: RDD, `words`, is a transformation of `txtDoc`. <br>\n",
    "This transformation is reflected in `.toDebugString()` and in the `Stage 1` DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(8) PythonRDD[2] at RDD at PythonRDD.scala:48 []\\n |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:480 []'\n",
      "['violent', 'delights', 'violent', 'triump', 'powder', 'consume']\n"
     ]
    }
   ],
   "source": [
    "longwords=words.filter(lambda x:len(x)>5)\n",
    "print(longwords.toDebugString())\n",
    "print(longwords.take(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numwords=longwords.count()\n",
    "numwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(8) PythonRDD[2] at RDD at PythonRDD.scala:48 []\\n |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:480 []'\n"
     ]
    }
   ],
   "source": [
    "print(longwords.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(8) PythonRDD[11] at collect at <ipython-input-7-e70847af13b8>:2 []\\n |  MapPartitionsRDD[10] at mapPartitions at PythonRDD.scala:427 []\\n |  ShuffledRDD[9] at partitionBy at NativeMethodAccessorImpl.java:0 []\\n +-(8) PairwiseRDD[8] at distinct at <ipython-input-7-e70847af13b8>:1 []\\n    |  PythonRDD[7] at distinct at <ipython-input-7-e70847af13b8>:1 []\\n    |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:480 []'\n"
     ]
    }
   ],
   "source": [
    "longwords_distinct=longwords.distinct()\n",
    "longwords_distinct.collect()\n",
    "print(longwords_distinct.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "Observe new jobs and stages in the log. <br>\n",
    "Note that using `distinct()` required `Shuffle` stage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching, Persistence and Checkpointing\n",
    "\n",
    "### Caching\n",
    "\n",
    "Caching an RDD persists data in memory. <br>\n",
    "Cached data can be reused multiple times without reevaluation when actions are called. <br>\n",
    "Caching does not trigger execution or computation. It is a suggestion: if there is not enough memory for an RDD it will be reevaluated every time it is processed. <br>\n",
    "Caching does not spill to disk because it only uses memory. <br>\n",
    "However, a cached RDD will be persisted to memory if storage level is MEMORY_ONLY_SER. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>\n",
    "Count frequencies of words from Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[17] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtDoc=sc.parallelize(['These violent delights have violent ends',\n",
    "                       'And in their triump die like fire and powder',\n",
    "                       'Which as they kiss consume'])\n",
    "words=txtDoc.flatMap(lambda x: x.split()) \\\n",
    ".map(lambda x: (x,1)) \\\n",
    ".reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "words.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "Counting words will trigger computation for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "Now Storage tab shows cached partitions. <br>\n",
    "Log contains 1 job (`count`) and 2 stages (`reduceByKey` and `count`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('These', 1), ('ends', 1), ('Which', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "New job appeared in the log showing skipped stages. <br>\n",
    "This is a result of caching: new stage did not require reevaluating the lineage of RDDs. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "Second `count` job has skipped stage due to caching. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persisting\n",
    "\n",
    "Cached partitions are stored in memory on executor JVMs on Spark Workers. <br>\n",
    "If a worker node fails the cached data will need to be recreated. <br>\n",
    "<br>\n",
    "The `persist` method offers additional storage options including MEMORY_AND_DISK, MEMORY_ONLY_SER, MEMORY_AND_DISK_SER. <br>\n",
    "When using persistence with one of the disk storage options, the persisted partitions are stored as local files on the worker nodes running Spark executors for the application. <br>\n",
    "Persisted data can be used in case of memory failure. <br>\n",
    "<br>\n",
    "Persist can also use replication on alternative nodes which helps avoiding recreation of data when some nodes fail. <br>\n",
    "<br>\n",
    "**Spark RDDs are fault-tolerant regardless of persistence. Persistence increases efficiency of the property** <br>\n",
    "<br>\n",
    "Persistence is also a suggestion: it only takes place after an action is called to trigger evaluation of an RDD. <br>\n",
    "If there is not enough memory persistence may not be implemented. <br>\n",
    "Persistence storage status can be checked by `getStorageLevel()`.\n",
    "\n",
    "#### Persist()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.persist(storageLevel=storageLevel.MEMORY_ONLY_SER)`\n",
    "\n",
    "The method specifies the storage level and storage attributes. <br>\n",
    "The argument `storageLevel` can be either a static constant or a set of storage flags.\n",
    "<br>\n",
    "The following instructions are equivalent: <br>\n",
    "\n",
    "`.persist(StorageLevel.MEMORY_AND_DISK_SER_2)` <br>\n",
    "`.persist(StorageLevel(True,True,False,False,2)`\n",
    "\n",
    "#### Unpersist()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`unpersist()`\n",
    "\n",
    "The method `unpersist` unpersists the RDD. <br>\n",
    "This method can also be used to remove an RDD that cached using `cache`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Checkpointing\n",
    "\n",
    "Checkpointing saves data to a file and can be used to transport results beyond the application to other driver processes or applications. <br>\n",
    "\n",
    "Checkpointing is an expensive operation. It is executed when an action is required.\n",
    "\n",
    "#### SetCheckPointDir()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`SparkContext.setCheckPointDir()`\n",
    "\n",
    "This method sets the directory in which RDDs are checkpointed.\n",
    "\n",
    "#### Checkpoint()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.checkpoint()`\n",
    "\n",
    "This method marks RDD for checkpointing; it must be called before any action. <br>\n",
    "After checkpointing is over the RDD and its lineage, including all references will be removed. <br>\n",
    "\n",
    "Checkpoint directory must be specified using `setCheckPointDir()` before checkpointing RDD.\n",
    "\n",
    "#### IsCheckPointed()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.isCheckPointed()`\n",
    "\n",
    "Returns Boolean response telling if the RDD has been checkpointed.\n",
    "\n",
    "#### GetCheckPointFile()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.getCheckPointFile()`\n",
    "\n",
    "Returns the name of the file to which the RDD was checkpointed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving RDD Output\n",
    "\n",
    "Typically it useful to save the final step of an RDD transformations to an external storage, which may be an HDFS, S3 or a local file system.\n",
    "\n",
    "#### SaveAsTextFile()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.saveAsTextFile(path, compressionCodecClass=None)`\n",
    "\n",
    "This method is an action that saves RDD to a directory of text file with one file per partition. Each file contains string representation of elements. <br>\n",
    "\n",
    "The directory specified by the path cannot exist before the Spark job is run. Spark cannot override files and folders due to immutability rule.\n",
    "\n",
    "#### SaveAsSequenceFile()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.saveAsSequenceFile(path, compressionCodecClass=None)`\n",
    "\n",
    "This is Spark action saving RDD to a directory containing Sequence Filesconsisting of uniform key-value pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>\n",
    "Create name of directory where the data will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'savedData-20171009012726'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now=datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_dir=\"savedData\"\n",
    "dataDir=\"{}-{}\".format(root_dir,now)\n",
    "dataDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txtDoc=sc.parallelize(['These violent delights have violent ends',\n",
    "                       'And in their triump die like fire and powder',\n",
    "                       'Which as they kiss consume'])\n",
    "words=txtDoc.flatMap(lambda x: x.split()).keyBy(lambda x: x)\n",
    "words.saveAsSequenceFile(dataDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "Check that the directory appeared in the same folder where the notebook is located."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast Variables\n",
    "\n",
    "Broadcast variables are read-only variables set by Spark Driver that are made available to the worker nodes, and through them available to any tasks running on executors on those workers. <br>\n",
    "Broadcast variables are shared by a special protocol which makes such sharing more efficient than directly pushing variables from Driver to executor processes. <br>\n",
    "\n",
    "#### Broadcast()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`SparkContext.broadcast(value)`\n",
    "\n",
    "The method creates an instance of a Broadcast object within the specific SparkContext. <br>\n",
    "\n",
    "Argument `value` is an object that will be serialized and used in the Broadcast object. It can be any valid Python object.\n",
    "\n",
    "Once initialized, the broadcast variable can be called within the SparkContext.\n",
    "\n",
    "#### Value()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.value()`\n",
    "\n",
    "This method returns value from the broadcast variable. It can be used within a lambda function in a `map` or `filter` operation.\n",
    "\n",
    "#### Unpersist()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.unpersist(blocking=False)`\n",
    "\n",
    "This method is used to remove broadcast variable from memory from all the workers where it was present. <br>\n",
    "\n",
    "Boolean argument `blocking` the operation shoud be blocking (block until the variable is unpersisted on all nodes). If memory needs to be released immediately set `blocking=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>\n",
    "Create RDD `words` from a Sheakspeare qiote and broadcast it. <br>\n",
    "Then unpersist it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('These', 1), ('ends', 1), ('Which', 1), ('And', 1), ('like', 1), ('their', 1), ('die', 1), ('as', 1), ('powder', 1), ('in', 1), ('violent', 2), ('delights', 1), ('fire', 1), ('have', 1), ('and', 1), ('triump', 1), ('they', 1), ('consume', 1), ('kiss', 1)]\n"
     ]
    }
   ],
   "source": [
    "txtDoc=sc.parallelize(['These violent delights have violent ends',\n",
    "                       'And in their triump die like fire and powder',\n",
    "                       'Which as they kiss consume'])\n",
    "words=txtDoc.flatMap(lambda x: x.split()) \\\n",
    ".map(lambda x: (x,1)) \\\n",
    ".reduceByKey(lambda x,y: x+y)\n",
    "print(words.collect())\n",
    "br_words=sc.broadcast(words.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ends', 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.broadcast.Broadcast at 0x1935379cd68>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(br_words.value[1])\n",
    "br_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "br_words.unpersist(blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ends', 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.broadcast.Broadcast at 0x1935379cd68>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(br_words.value[1])\n",
    "br_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "Note that after unpersisting the broadcast variable it still shows.\n",
    "The variable was removed from all workers, but stayed on the driver.\n",
    "When it is used the driver has to send it again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulators\n",
    "\n",
    "This is another type of shared variables in Spark: they can be updated in a special way: they are numeric values that can be incremented. <br>\n",
    "Accumulators allow aggregating values as the program is running. <br>\n",
    "Accumulators are updated once per successfully completed task in a Spark application. <br>\n",
    "Worker nodes send updates for accumulator to the driver because driver is the only process that can read accumulator value. <br>\n",
    "Accumulators can use integer or float values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>\n",
    "In this example accumulator variable is created, updated and read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc=sc.accumulator(0)\n",
    "def addone(x):\n",
    "    global acc\n",
    "    acc+=1\n",
    "    return x+1\n",
    "myrdd=sc.parallelize([1,2,3,4,5])\n",
    "myrdd.map(lambda x: addone(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "records processed: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"records processed: \" + str(acc.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL DataFrames\n",
    "\n",
    "Spark SQL DataFrame is similar to dataframe object in R. <br>\n",
    "DataFrames, like RDDs are evaluated as DAGs with lazy evaluation. <br>\n",
    "They also provide lineage and fault tolerance. <br>\n",
    "Caching and persistence methods can also be applied to DataFrames. <br>\n",
    "\n",
    "**In latest versions of Spark it is recommended to use DataFrames rather than RDDs whenever possible.**\n",
    "\n",
    "### Creating DataFrame from Existing RDD\n",
    "\n",
    "#### CreateDataFrame()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`SQLContext.createDataFrame(data, schema=None, samplingRatio=None)`\n",
    "\n",
    "This method creates DataFrame object from an existing RDD. <br>\n",
    "Argument `data` is a reference to a named RDD object consisting of tuples or list elements. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='Jeff', _2=46), Row(_1='Kellie', _2=44)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd=sc.parallelize([('Jeff',46),('Kellie',44)])\n",
    "sqlContext.createDataFrame(myrdd).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "The result is a list of `Row(pyspark.sql.Row)` objects. <br>\n",
    "Since `schema` was not specified the fields are referenced by `<fieldnumber>` starting at 1. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames from Files using DataFrameReader\n",
    "\n",
    "Starting from version 1.4 Spark has interface to load DataFrames from external storage systems, the DataFrameReader. <br>\n",
    "The DataFrameReader interface is accessed using `SQLContext.read()`. <br>\n",
    "\n",
    "#### Text()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`SQLContext.read.text(paths)`\n",
    "\n",
    "This method is used to load DataFrames from text files in an external file system: local, NFS, HDFS, S3, etc. <br>\n",
    "It is similar to `sc.textFile()`. <br>\n",
    "Argument `path` refers to a file, directory or file glob."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>\n",
    "Create and RDD and save it as partitioned text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "savedData-20171009125800\n"
     ]
    }
   ],
   "source": [
    "now=datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_dir=\"savedData\"\n",
    "dataDir=\"{}-{}\".format(root_dir,now)\n",
    "print(dataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myrdd=sc.parallelize([('Jeff',46),('Kellie',44)])\n",
    "myrdd.saveAsTextFile(dataDir+\"/myTestFile.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "Read the DataFrame and show its second row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value=\"('Kellie', 44)\")]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=sqlContext.read.text(dataDir+\"/myTestFile.txt/\")\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "The `Row` object returned line of the text file or files contains one complete string. <br>\n",
    "\n",
    "Remove folder `savedFiles` if you are planning to write to it again. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark.read.csv()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`spark.read.csv(path, schema=None, sep=None, encoding=None, quote=None, escape=None, comment=None, header=None, inferSchema=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, positiveInf=None, negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None, maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None)`\n",
    "\n",
    "This method was added starting Spark version 2.0. <br>\n",
    "It reads a `.csv` file pretty much like in R. <br>\n",
    "Argument `schema` describes the column structure of the data. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>\n",
    "Describe the columns as `schema`. <br>\n",
    "Read file with table of data for ANOVA. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[y: double, c: string]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(y=1.08931931750671, c='C2'),\n",
       " Row(y=4.03892811665213, c='C4'),\n",
       " Row(y=0.710538426311777, c='C1'),\n",
       " Row(y=4.54878758817833, c='C4'),\n",
       " Row(y=3.54970766958625, c='C3')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, Row\n",
    "\n",
    "df_struct = StructType([StructField('y', DoubleType()), \\\n",
    "                        StructField('c', StringType())])\n",
    "#df=spark.read.csv('./Examples/data.csv',sep=\" \",schema=df_struct)\n",
    "df = spark.read.csv('./data1.csv',sep=\" \",schema=df_struct)\n",
    "print(df)\n",
    "df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color=green>\n",
    " Write the DataFrame to a single `csv` file and read it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[y: double, c: string]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(y=1.08931931750671, c='C2'),\n",
       " Row(y=4.03892811665213, c='C4'),\n",
       " Row(y=0.710538426311777, c='C1'),\n",
       " Row(y=4.54878758817833, c='C4'),\n",
       " Row(y=3.54970766958625, c='C3')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas().to_csv(\"sample_file.csv\", sep=\",\",index=False,header=False,)\n",
    "dfback=spark.read.csv('sample_file.csv', sep=\",\",schema=df_struct)\n",
    "print(dfback)\n",
    "dfback.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting DataFrame to RDD\n",
    "\n",
    "Converting a DataFrame to RDD object is done by method `.rdd`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>\n",
    "Read the data for ANOVA as DataFrame, check that the result is a DataFrame. <br>\n",
    "Show first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[y: double, c: string]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(y=1.08931931750671, c='C2'),\n",
       " Row(y=4.03892811665213, c='C4'),\n",
       " Row(y=0.710538426311777, c='C1'),\n",
       " Row(y=4.54878758817833, c='C4'),\n",
       " Row(y=3.54970766958625, c='C3')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, Row\n",
    "\n",
    "df_struct = StructType([StructField('y', DoubleType()), \\\n",
    "                        StructField('c', StringType())])\n",
    "df=spark.read.csv('./data1.csv',sep=\" \",schema=df_struct)\n",
    "print(df)\n",
    "df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the DataFrame into RDD, show first 5 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapPartitionsRDD[15] at javaToPython at NativeMethodAccessorImpl.java:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(y=1.08931931750671, c='C2'),\n",
       " Row(y=4.03892811665213, c='C4'),\n",
       " Row(y=0.710538426311777, c='C1'),\n",
       " Row(y=4.54878758817833, c='C4'),\n",
       " Row(y=3.54970766958625, c='C3')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.rdd)\n",
    "df.rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Schemas\n",
    "\n",
    "For DataFrame object `schema` can be explicitely defined or inferred. <br>\n",
    "Generally, it is better to define schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferring the Schema\n",
    "\n",
    "Spark SQL can analyze an object (RDD that is converted to a DataFrame) and infer its composition to then use it as schema for the created DataFrame. This process is called **reflection**.\n",
    "\n",
    "During reflection each record of RDD is converted into a Row object and to each field gets  assigned a value. <br>\n",
    "\n",
    "Datatyes are inferred from the first record, it is important to have the first row representative of the dataset with no missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>\n",
    "Create an RDD `salespeople`, convert it to a DataFrame and print the inferred schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Henry', 100),\n",
       " (2, 'Karen', 100),\n",
       " (3, 'Paul', 101),\n",
       " (4, 'Jimmy', 102),\n",
       " (5, 'Janice', 103)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salespeople=sc.parallelize(['1\\tHenry\\t100', \\\n",
    "                            '2\\tKaren\\t100', \\\n",
    "                            '3\\tPaul\\t101', \\\n",
    "                            '4\\tJimmy\\t102', \\\n",
    "                            '5\\tJanice\\t103']) \\\n",
    ".map(lambda x: x.split('\\t')) \\\n",
    ".map(lambda x: (int(x[0]),x[1],int(x[2])))\n",
    "salespeople.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=sqlContext.createDataFrame(salespeople)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=1, _2='Henry', _3=100)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "Fields identifiers by default are field numbers, they have property `nullable=True`, meaning that they do not need to be supplied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Schema Programmatically\n",
    "\n",
    "It is preferrable to create schema rather than infer it. <br>\n",
    "Creating a schema requires creating a `StructType` object containing a collection of `StructField` objects. <br>\n",
    "Then this schema is applied to a DataFrame when it is created. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "myschema=StructType([ \\\n",
    "                    StructField(\"SalesPerson_ID\",IntegerType(),True), \\\n",
    "                    StructField(\"SalesPerson_Name\",StringType(),nullable=True), \\\n",
    "                    StructField(\"Store_ID\",IntegerType(),nullable=True) \\\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SalesPerson_ID: integer (nullable = true)\n",
      " |-- SalesPerson_Name: string (nullable = true)\n",
      " |-- Store_ID: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=sqlContext.createDataFrame(salespeople,myschema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(SalesPerson_ID=1, SalesPerson_Name='Henry', Store_ID=100)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "Note differences with the inferred schema of the previous example:\n",
    "-  Fields have custom names\n",
    "-  Types are defined not \"conservatively\": integer instead of long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Spark SQL DataFrames\n",
    "\n",
    "DataFrames has been the fastest developing area of the Spark project. <br>\n",
    "\n",
    "### DataFrame Metadata Operations\n",
    "\n",
    "Metadata functions return information about the data structure instead of data themselves. <br>\n",
    "\n",
    "#### Columns()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.columns`\n",
    "\n",
    "This method returns a list of column names for the given DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SalesPerson_ID', 'SalesPerson_Name', 'Store_ID']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dtypes\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.dtypes`\n",
    "\n",
    "This method returns a list of tuples, each of them consisting of column names and data types for a column of a given DataFrame object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SalesPerson_ID', 'int'),\n",
       " ('SalesPerson_Name', 'string'),\n",
       " ('Store_ID', 'int')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic DataFrame Operations\n",
    "\n",
    "#### Count()\n",
    "\n",
    "Function `count` returns the number of rows in the DataFrame. As in case of RDD it is an action that triggers evaluation of the DataFrame and its lineage.\n",
    "\n",
    "#### Collect()\n",
    "\n",
    "Same function as for RDD is available for DataFrames.\n",
    "\n",
    "#### Take(n)\n",
    "\n",
    "Same function as for RDD is available for DataFrames.\n",
    "\n",
    "#### Show()\n",
    "\n",
    "Syntax is:\n",
    "\n",
    "`.show(n=20,truncate=True)`\n",
    "\n",
    "Prints the first `n` rows of a DataFrame to the console. Unlike `collect()` or `take(n)` it cannot be returned to a variable. <br>\n",
    "Argument `truncate` is used to truncate long strings and align cells to the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(SalesPerson_ID=1, SalesPerson_Name='Henry', Store_ID=100),\n",
       " Row(SalesPerson_ID=2, SalesPerson_Name='Karen', Store_ID=100),\n",
       " Row(SalesPerson_ID=3, SalesPerson_Name='Paul', Store_ID=101),\n",
       " Row(SalesPerson_ID=4, SalesPerson_Name='Jimmy', Store_ID=102),\n",
       " Row(SalesPerson_ID=5, SalesPerson_Name='Janice', Store_ID=103)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.count())\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(SalesPerson_ID=1, SalesPerson_Name='Henry', Store_ID=100),\n",
       " Row(SalesPerson_ID=2, SalesPerson_Name='Karen', Store_ID=100)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+--------+\n",
      "|SalesPerson_ID|SalesPerson_Name|Store_ID|\n",
      "+--------------+----------------+--------+\n",
      "|             1|           Henry|     100|\n",
      "|             2|           Karen|     100|\n",
      "+--------------+----------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select, drop, filter and distinct\n",
    "\n",
    "These functions are used to prune columns or filter rows from a DataFrame. <br>\n",
    "A new DataFrame is created as a result. <br>\n",
    "\n",
    "Syntax:\n",
    "\n",
    "`.drop(col)`\n",
    "\n",
    "Returns a new DataFrame with column `col` removed. <br>\n",
    "\n",
    "Syntax:\n",
    "\n",
    "`.filter(condition)`\n",
    "\n",
    "Returns a new DataFrame containing only rows satisfying the condition.\n",
    "\n",
    "Syntax:\n",
    "\n",
    "`.distinct()`\n",
    "\n",
    "Returns DataFrame containing only distinct rows, equivalent to filtering out duplicate rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|SalesPerson_ID|SalesPerson_Name|\n",
      "+--------------+----------------+\n",
      "|             1|           Henry|\n",
      "|             2|           Karen|\n",
      "+--------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop('Store_ID').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+--------+\n",
      "|SalesPerson_ID|SalesPerson_Name|Store_ID|\n",
      "+--------------+----------------+--------+\n",
      "|             1|           Henry|     100|\n",
      "+--------------+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.SalesPerson_Name=='Henry').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+--------+\n",
      "|SalesPerson_ID|SalesPerson_Name|Store_ID|\n",
      "+--------------+----------------+--------+\n",
      "|             2|           Karen|     100|\n",
      "|             1|           Henry|     100|\n",
      "|             5|          Janice|     103|\n",
      "|             4|           Jimmy|     102|\n",
      "|             3|            Paul|     101|\n",
      "+--------------+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select, map, flatMap\n",
    "\n",
    "These methods are used to apply column-level functions to rows in Spark SQL DataFrames, similar to `apply` in R. <br>\n",
    "Use of lambda functions is a little different. <br>\n",
    "Functions `map()` and `flatMap()` used to be transformations that operate on DataFrames, but return an RDD. <br>\n",
    "Starting from PySpark 2 they are removed. <br>\n",
    "Use `.rdd.map()` or `.rdd.flatMap()` instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "**Example** <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+\n",
      "|SalesPerson_Name|Store_ID|\n",
      "+----------------+--------+\n",
      "|           Henry|     100|\n",
      "|           Karen|     100|\n",
      "|            Paul|     101|\n",
      "|           Jimmy|     102|\n",
      "|          Janice|     103|\n",
      "+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([\"SalesPerson_Name\",\"Store_ID\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 'Henry', 100, 2, 'Karen', 100, 3, 'Paul', 101, 4, 'Jimmy', 102, 5, 'Janice', 103]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(SalesPerson_ID=1, SalesPerson_Name='Henry', Store_ID=100),\n",
       " Row(SalesPerson_ID=2, SalesPerson_Name='Karen', Store_ID=100),\n",
       " Row(SalesPerson_ID=3, SalesPerson_Name='Paul', Store_ID=101),\n",
       " Row(SalesPerson_ID=4, SalesPerson_Name='Jimmy', Store_ID=102),\n",
       " Row(SalesPerson_ID=5, SalesPerson_Name='Janice', Store_ID=103)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.rdd.flatMap(lambda x: x).collect())\n",
    "df.rdd.map(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ANOVA Coding: DataFrame Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create structure type for 1-way ANOVA, name it `df_struct`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Skipped code\n",
    "df_struct = StructType([StructField('y', DoubleType()), \\\n",
    "                        StructField('c', StringType())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read file `data0.csv` into a DataFrame `df` and cache it.\n",
    "\n",
    "df.take(10) returns output: <br>\n",
    "\n",
    "[Row(y=0.0893193175067113, c='C2'), <br>\n",
    " Row(y=1.03892811665213, c='C4'), <br>\n",
    " Row(y=0.710538426311777, c='C1'), <br>\n",
    " Row(y=1.54878758817833, c='C4'), <br>\n",
    " Row(y=1.54970766958625, c='C3'), <br>\n",
    " Row(y=2.62544730346494, c='C2'), <br>\n",
    " Row(y=-0.890027143624024, c='C3'), <br>\n",
    " Row(y=-0.231707058414097, c='C2'), <br>\n",
    " Row(y=1.10632061985913, c='C4'), <br>\n",
    " Row(y=1.79791643753108, c='C2')] <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[y: double, c: string]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(y=0.0893193175067113, c='C2'),\n",
       " Row(y=1.03892811665213, c='C4'),\n",
       " Row(y=0.710538426311777, c='C1'),\n",
       " Row(y=1.54878758817833, c='C4'),\n",
       " Row(y=1.54970766958625, c='C3'),\n",
       " Row(y=2.62544730346494, c='C2'),\n",
       " Row(y=-0.890027143624024, c='C3'),\n",
       " Row(y=-0.231707058414097, c='C2'),\n",
       " Row(y=1.10632061985913, c='C4'),\n",
       " Row(y=1.79791643753108, c='C2')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Skipped code\n",
    "df = spark.read.csv('./data0.csv', sep=\" \", schema=df_struct)\n",
    "print(df)\n",
    "df.take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame that contains columns of classes, sums, numbers of elements per class and mean values:\n",
    "\n",
    "[Row(c='C3', sum=106.72212894321214, n=100, mean=1.0672212894321214), <br>\n",
    " Row(c='C4', sum=100.32253230729513, n=100, mean=1.0032253230729513), <br>\n",
    " Row(c='C1', sum=102.26684498157968, n=100, mean=1.0226684498157967), <br>\n",
    " Row(c='C2', sum=95.44484415126036, n=100, mean=0.9544484415126036)] <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(y=0.0893193175067113, c='C2'),\n",
       " Row(y=1.03892811665213, c='C4'),\n",
       " Row(y=0.710538426311777, c='C1'),\n",
       " Row(y=1.54878758817833, c='C4'),\n",
       " Row(y=1.54970766958625, c='C3'),\n",
       " Row(y=2.62544730346494, c='C2'),\n",
       " Row(y=-0.890027143624024, c='C3'),\n",
       " Row(y=-0.231707058414097, c='C2'),\n",
       " Row(y=1.10632061985913, c='C4'),\n",
       " Row(y=1.79791643753108, c='C2'),\n",
       " Row(y=-0.147657009236351, c='C1'),\n",
       " Row(y=0.043108118674381, c='C2'),\n",
       " Row(y=2.47025699708299, c='C2'),\n",
       " Row(y=0.130217126313507, c='C2'),\n",
       " Row(y=-0.720650662077186, c='C4'),\n",
       " Row(y=0.551825763396604, c='C2'),\n",
       " Row(y=0.498621682204911, c='C3'),\n",
       " Row(y=1.07734227692233, c='C4'),\n",
       " Row(y=0.933533225922491, c='C4'),\n",
       " Row(y=1.00837095999603, c='C1'),\n",
       " Row(y=0.556615195636073, c='C3'),\n",
       " Row(y=4.26641451887544, c='C4'),\n",
       " Row(y=0.0254475312855701, c='C4'),\n",
       " Row(y=1.9514984696255, c='C3'),\n",
       " Row(y=0.761613049940232, c='C4'),\n",
       " Row(y=2.00736436564955, c='C4'),\n",
       " Row(y=0.595012841515629, c='C4'),\n",
       " Row(y=1.77214218580453, c='C1'),\n",
       " Row(y=0.930982702501831, c='C4'),\n",
       " Row(y=1.02747533674519, c='C2'),\n",
       " Row(y=0.579002102183304, c='C3'),\n",
       " Row(y=0.112579854682902, c='C3'),\n",
       " Row(y=0.156384610079107, c='C4'),\n",
       " Row(y=-0.577218048894824, c='C4'),\n",
       " Row(y=1.36945910373103, c='C3'),\n",
       " Row(y=-0.119720061126983, c='C2'),\n",
       " Row(y=-0.133202206140118, c='C4'),\n",
       " Row(y=0.30904616030317, c='C1'),\n",
       " Row(y=0.670485807611821, c='C4'),\n",
       " Row(y=2.400590827286, c='C4'),\n",
       " Row(y=0.569120470395916, c='C2'),\n",
       " Row(y=1.83739439736689, c='C4'),\n",
       " Row(y=1.83204712857239, c='C1'),\n",
       " Row(y=0.581019900578041, c='C1'),\n",
       " Row(y=0.052924208258981, c='C2'),\n",
       " Row(y=0.651405315489247, c='C2'),\n",
       " Row(y=3.36971990795134, c='C2'),\n",
       " Row(y=0.69880301738033, c='C4'),\n",
       " Row(y=0.855391792689459, c='C3'),\n",
       " Row(y=1.49096737259706, c='C3'),\n",
       " Row(y=0.196786782635259, c='C2'),\n",
       " Row(y=2.11343318380002, c='C4'),\n",
       " Row(y=0.468956893998461, c='C4'),\n",
       " Row(y=0.108078872715431, c='C1'),\n",
       " Row(y=-0.158015254042425, c='C4'),\n",
       " Row(y=2.10096910219409, c='C1'),\n",
       " Row(y=1.68501476610822, c='C4'),\n",
       " Row(y=1.64667439049534, c='C1'),\n",
       " Row(y=2.54756693261827, c='C3'),\n",
       " Row(y=0.575189716622713, c='C1'),\n",
       " Row(y=-0.437586240829979, c='C1'),\n",
       " Row(y=0.70527955320944, c='C1'),\n",
       " Row(y=1.31928641795509, c='C4'),\n",
       " Row(y=2.54159306882696, c='C3'),\n",
       " Row(y=0.200990751010632, c='C1'),\n",
       " Row(y=1.3773956459817, c='C1'),\n",
       " Row(y=0.130539649205552, c='C4'),\n",
       " Row(y=2.68027820468955, c='C2'),\n",
       " Row(y=2.04106029830068, c='C4'),\n",
       " Row(y=0.838821493827654, c='C3'),\n",
       " Row(y=1.50360797223373, c='C1'),\n",
       " Row(y=1.91643265683777, c='C4'),\n",
       " Row(y=-0.514886795001597, c='C4'),\n",
       " Row(y=0.0275570640246365, c='C4'),\n",
       " Row(y=0.350989922291102, c='C1'),\n",
       " Row(y=0.625145237886105, c='C2'),\n",
       " Row(y=2.2724293214294, c='C1'),\n",
       " Row(y=0.780484373246561, c='C1'),\n",
       " Row(y=1.19750615370222, c='C4'),\n",
       " Row(y=0.739263691431877, c='C3'),\n",
       " Row(y=1.94838935074207, c='C4'),\n",
       " Row(y=1.57671878189649, c='C2'),\n",
       " Row(y=0.875565013579444, c='C4'),\n",
       " Row(y=0.730958987697495, c='C4'),\n",
       " Row(y=-0.629578287317995, c='C4'),\n",
       " Row(y=0.747816838609053, c='C2'),\n",
       " Row(y=1.766065384004, c='C4'),\n",
       " Row(y=-0.00180133547677341, c='C4'),\n",
       " Row(y=1.03693769084217, c='C3'),\n",
       " Row(y=0.929582617492845, c='C2'),\n",
       " Row(y=0.384010092292082, c='C2'),\n",
       " Row(y=2.26295428488079, c='C1'),\n",
       " Row(y=0.775732114721691, c='C1'),\n",
       " Row(y=2.08576936214569, c='C1'),\n",
       " Row(y=1.41464143445641, c='C1'),\n",
       " Row(y=0.588172043450105, c='C4'),\n",
       " Row(y=1.9921603654458, c='C1'),\n",
       " Row(y=-0.390166036635079, c='C2'),\n",
       " Row(y=1.4438053256379, c='C4'),\n",
       " Row(y=2.3297992629225, c='C1'),\n",
       " Row(y=1.78269769776364, c='C4'),\n",
       " Row(y=0.898496552368274, c='C3'),\n",
       " Row(y=1.40654273194493, c='C3'),\n",
       " Row(y=2.25408310644997, c='C1'),\n",
       " Row(y=1.25014132285415, c='C1'),\n",
       " Row(y=-0.4250983947325, c='C1'),\n",
       " Row(y=1.43568329935572, c='C1'),\n",
       " Row(y=1.01848557011386, c='C4'),\n",
       " Row(y=0.987627230216259, c='C3'),\n",
       " Row(y=-0.637421952151837, c='C4'),\n",
       " Row(y=1.01464681247518, c='C4'),\n",
       " Row(y=3.66156636665324, c='C4'),\n",
       " Row(y=-0.0750101908181724, c='C3'),\n",
       " Row(y=0.509167247633448, c='C2'),\n",
       " Row(y=-0.237538421929958, c='C1'),\n",
       " Row(y=-0.53995004190371, c='C1'),\n",
       " Row(y=2.58009168370384, c='C2'),\n",
       " Row(y=-0.166570547084707, c='C1'),\n",
       " Row(y=2.26938716360704, c='C3'),\n",
       " Row(y=1.0176365464246, c='C2'),\n",
       " Row(y=0.0231696496532813, c='C3'),\n",
       " Row(y=2.29931230256343, c='C1'),\n",
       " Row(y=1.95367979197183, c='C4'),\n",
       " Row(y=0.245029708139776, c='C4'),\n",
       " Row(y=1.76359346114046, c='C1'),\n",
       " Row(y=-0.10716481896875, c='C3'),\n",
       " Row(y=-0.475547635260523, c='C3'),\n",
       " Row(y=2.29440806741572, c='C4'),\n",
       " Row(y=0.0879316330516621, c='C1'),\n",
       " Row(y=1.56060045960055, c='C4'),\n",
       " Row(y=2.05375086302862, c='C2'),\n",
       " Row(y=1.77958400907585, c='C2'),\n",
       " Row(y=1.38141125909948, c='C3'),\n",
       " Row(y=0.185031291130083, c='C1'),\n",
       " Row(y=2.57243291747915, c='C4'),\n",
       " Row(y=1.96079238395053, c='C2'),\n",
       " Row(y=0.61009137069415, c='C2'),\n",
       " Row(y=-1.90489906034557, c='C3'),\n",
       " Row(y=1.1976842623458, c='C2'),\n",
       " Row(y=2.57615818121873, c='C3'),\n",
       " Row(y=0.303176453985664, c='C3'),\n",
       " Row(y=0.204660882744628, c='C1'),\n",
       " Row(y=1.43453665586966, c='C4'),\n",
       " Row(y=1.45812525940467, c='C4'),\n",
       " Row(y=0.520744413218603, c='C4'),\n",
       " Row(y=-0.11959910457218, c='C2'),\n",
       " Row(y=1.08181031035401, c='C3'),\n",
       " Row(y=-0.0080545781695387, c='C2'),\n",
       " Row(y=0.983251743848126, c='C2'),\n",
       " Row(y=1.39056594237021, c='C3'),\n",
       " Row(y=1.7756343188835, c='C3'),\n",
       " Row(y=1.80418950974491, c='C1'),\n",
       " Row(y=1.61696647568908, c='C3'),\n",
       " Row(y=0.987470653004283, c='C2'),\n",
       " Row(y=2.17215854698947, c='C3'),\n",
       " Row(y=0.232209815269141, c='C2'),\n",
       " Row(y=1.97620252467742, c='C4'),\n",
       " Row(y=2.49522344384323, c='C3'),\n",
       " Row(y=-0.0655905803882961, c='C1'),\n",
       " Row(y=1.33561720996882, c='C2'),\n",
       " Row(y=0.726842006924342, c='C4'),\n",
       " Row(y=1.99698686090911, c='C1'),\n",
       " Row(y=2.8739038985953, c='C3'),\n",
       " Row(y=1.04672617218835, c='C1'),\n",
       " Row(y=1.60430940871148, c='C4'),\n",
       " Row(y=1.25733837715553, c='C1'),\n",
       " Row(y=1.61824329356625, c='C1'),\n",
       " Row(y=0.165440966980938, c='C4'),\n",
       " Row(y=1.81655644874813, c='C3'),\n",
       " Row(y=-0.596718014297197, c='C3'),\n",
       " Row(y=0.180131720728705, c='C4'),\n",
       " Row(y=-0.314195139900004, c='C3'),\n",
       " Row(y=0.0367668195462679, c='C4'),\n",
       " Row(y=1.22901959069469, c='C1'),\n",
       " Row(y=1.84766496359603, c='C3'),\n",
       " Row(y=0.350528353203767, c='C1'),\n",
       " Row(y=0.700784882102684, c='C1'),\n",
       " Row(y=2.5396480875188, c='C4'),\n",
       " Row(y=0.044160896723202, c='C2'),\n",
       " Row(y=-0.452028290146153, c='C4'),\n",
       " Row(y=-0.523614882377553, c='C2'),\n",
       " Row(y=0.764293443560499, c='C1'),\n",
       " Row(y=1.72675074738545, c='C1'),\n",
       " Row(y=2.61475223546813, c='C3'),\n",
       " Row(y=0.294071414332338, c='C3'),\n",
       " Row(y=2.75790308981071, c='C1'),\n",
       " Row(y=1.26266524269077, c='C4'),\n",
       " Row(y=2.31840093119247, c='C3'),\n",
       " Row(y=1.06528818167162, c='C1'),\n",
       " Row(y=1.92606272533302, c='C3'),\n",
       " Row(y=0.587480112517602, c='C2'),\n",
       " Row(y=1.5436578591535, c='C4'),\n",
       " Row(y=0.126737888255565, c='C1'),\n",
       " Row(y=-0.232901199571264, c='C2'),\n",
       " Row(y=1.56074609088806, c='C1'),\n",
       " Row(y=0.939365221879447, c='C3'),\n",
       " Row(y=1.30830874866913, c='C4'),\n",
       " Row(y=0.99627646620782, c='C2'),\n",
       " Row(y=1.05579711092944, c='C4'),\n",
       " Row(y=2.23830410085338, c='C1'),\n",
       " Row(y=1.24919601165047, c='C4'),\n",
       " Row(y=1.6060734308621, c='C3'),\n",
       " Row(y=0.458239637898887, c='C3'),\n",
       " Row(y=0.813683031948468, c='C4'),\n",
       " Row(y=0.262688309160031, c='C2'),\n",
       " Row(y=0.410187209808829, c='C3'),\n",
       " Row(y=0.798038257644705, c='C4'),\n",
       " Row(y=2.45598840106634, c='C1'),\n",
       " Row(y=0.0277131644944403, c='C2'),\n",
       " Row(y=1.31788573509247, c='C2'),\n",
       " Row(y=-0.113765136319532, c='C2'),\n",
       " Row(y=0.425704585698594, c='C2'),\n",
       " Row(y=0.717826122677549, c='C1'),\n",
       " Row(y=0.588489167204933, c='C1'),\n",
       " Row(y=0.909543875759005, c='C2'),\n",
       " Row(y=1.66413569989411, c='C1'),\n",
       " Row(y=-0.714502296845825, c='C3'),\n",
       " Row(y=-0.0962367828065627, c='C4'),\n",
       " Row(y=0.79385543336079, c='C3'),\n",
       " Row(y=1.70211671066757, c='C3'),\n",
       " Row(y=2.15653699715018, c='C1'),\n",
       " Row(y=0.673766639294351, c='C1'),\n",
       " Row(y=-0.134330968516844, c='C3'),\n",
       " Row(y=-0.198597083318085, c='C3'),\n",
       " Row(y=1.47173637445323, c='C2'),\n",
       " Row(y=2.69694788072309, c='C3'),\n",
       " Row(y=1.22221807214183, c='C4'),\n",
       " Row(y=1.06797238952197, c='C4'),\n",
       " Row(y=-0.563782051071005, c='C1'),\n",
       " Row(y=-1.22390027400994, c='C1'),\n",
       " Row(y=1.13805270871174, c='C2'),\n",
       " Row(y=1.99654392854413, c='C1'),\n",
       " Row(y=2.7025705860145, c='C3'),\n",
       " Row(y=0.103788736039421, c='C3'),\n",
       " Row(y=0.407774627038412, c='C2'),\n",
       " Row(y=-0.280749431788324, c='C2'),\n",
       " Row(y=0.566689682543218, c='C1'),\n",
       " Row(y=2.8831825423826, c='C2'),\n",
       " Row(y=0.852729209961034, c='C3'),\n",
       " Row(y=0.676249245412038, c='C2'),\n",
       " Row(y=0.942893225616191, c='C1'),\n",
       " Row(y=2.95642526324584, c='C4'),\n",
       " Row(y=1.22257383085989, c='C4'),\n",
       " Row(y=2.79048505353782, c='C2'),\n",
       " Row(y=1.88577837390093, c='C2'),\n",
       " Row(y=1.55989527297743, c='C4'),\n",
       " Row(y=0.0593508373813916, c='C1'),\n",
       " Row(y=0.705803546393087, c='C2'),\n",
       " Row(y=0.0710289207988245, c='C2'),\n",
       " Row(y=3.50711114848337, c='C3'),\n",
       " Row(y=1.74127630526021, c='C2'),\n",
       " Row(y=0.0714329652864619, c='C1'),\n",
       " Row(y=1.72945127879333, c='C4'),\n",
       " Row(y=1.24226348085969, c='C1'),\n",
       " Row(y=0.385049729203645, c='C2'),\n",
       " Row(y=1.26264545866276, c='C2'),\n",
       " Row(y=1.12104023238115, c='C4'),\n",
       " Row(y=0.453413414254842, c='C4'),\n",
       " Row(y=0.827376497354143, c='C1'),\n",
       " Row(y=1.34479512366302, c='C4'),\n",
       " Row(y=2.46674459318725, c='C4'),\n",
       " Row(y=1.25222344815613, c='C1'),\n",
       " Row(y=-0.0159289468548922, c='C2'),\n",
       " Row(y=0.881207974221172, c='C2'),\n",
       " Row(y=1.13333636081484, c='C1'),\n",
       " Row(y=0.0181443311961293, c='C3'),\n",
       " Row(y=2.10217659507369, c='C2'),\n",
       " Row(y=0.167956703882168, c='C1'),\n",
       " Row(y=1.63506032759145, c='C3'),\n",
       " Row(y=0.0259974388874731, c='C2'),\n",
       " Row(y=1.35872889597135, c='C1'),\n",
       " Row(y=-0.284599353872188, c='C1'),\n",
       " Row(y=1.45600866937208, c='C4'),\n",
       " Row(y=-0.180490682884277, c='C2'),\n",
       " Row(y=1.2661373616721, c='C1'),\n",
       " Row(y=1.02538286758781, c='C2'),\n",
       " Row(y=2.40856335683396, c='C3'),\n",
       " Row(y=1.49479576711316, c='C2'),\n",
       " Row(y=0.243352390646661, c='C4'),\n",
       " Row(y=-0.0135396704947914, c='C3'),\n",
       " Row(y=-0.311420591624882, c='C2'),\n",
       " Row(y=-0.457707210122382, c='C3'),\n",
       " Row(y=0.793810997882715, c='C2'),\n",
       " Row(y=1.53240935667051, c='C2'),\n",
       " Row(y=1.55039335845505, c='C3'),\n",
       " Row(y=0.202910474928035, c='C1'),\n",
       " Row(y=0.870364649729087, c='C4'),\n",
       " Row(y=1.06851153327714, c='C2'),\n",
       " Row(y=0.768022548833383, c='C3'),\n",
       " Row(y=1.78185918460026, c='C2'),\n",
       " Row(y=1.31014137650469, c='C3'),\n",
       " Row(y=0.623297281416372, c='C1'),\n",
       " Row(y=0.882246401834049, c='C1'),\n",
       " Row(y=1.58258599988644, c='C2'),\n",
       " Row(y=0.457111744989746, c='C1'),\n",
       " Row(y=0.880831237581962, c='C1'),\n",
       " Row(y=-0.680182722395933, c='C2'),\n",
       " Row(y=0.79865879412782, c='C2'),\n",
       " Row(y=0.457118062888069, c='C2'),\n",
       " Row(y=0.480463332984461, c='C4'),\n",
       " Row(y=0.119128276747455, c='C1'),\n",
       " Row(y=1.2484126488726, c='C1'),\n",
       " Row(y=2.0895034952023, c='C4'),\n",
       " Row(y=0.720653718145731, c='C1'),\n",
       " Row(y=1.04042768164309, c='C4'),\n",
       " Row(y=1.16178863355724, c='C2'),\n",
       " Row(y=-0.113122953101409, c='C3'),\n",
       " Row(y=1.31451676832098, c='C4'),\n",
       " Row(y=-0.0457176517867426, c='C3'),\n",
       " Row(y=0.925053430040715, c='C4'),\n",
       " Row(y=1.09505622698864, c='C3'),\n",
       " Row(y=1.09739665458942, c='C4'),\n",
       " Row(y=3.22926220164091, c='C3'),\n",
       " Row(y=0.819742934557663, c='C4'),\n",
       " Row(y=-0.263614384970583, c='C1'),\n",
       " Row(y=1.36594112304922, c='C1'),\n",
       " Row(y=0.547216027446842, c='C1'),\n",
       " Row(y=1.98111615990662, c='C2'),\n",
       " Row(y=0.724221970911973, c='C1'),\n",
       " Row(y=3.02476139007773, c='C2'),\n",
       " Row(y=0.988954521534336, c='C1'),\n",
       " Row(y=0.994232827252463, c='C1'),\n",
       " Row(y=-0.514497008250332, c='C3'),\n",
       " Row(y=0.570486890508119, c='C1'),\n",
       " Row(y=0.884174677843046, c='C1'),\n",
       " Row(y=0.223223378235403, c='C2'),\n",
       " Row(y=1.07036614128515, c='C3'),\n",
       " Row(y=2.2560188173061, c='C1'),\n",
       " Row(y=-0.57237269904962, c='C4'),\n",
       " Row(y=2.72389413060593, c='C3'),\n",
       " Row(y=-0.0865030469936974, c='C2'),\n",
       " Row(y=0.701090686569338, c='C3'),\n",
       " Row(y=0.296305746101262, c='C2'),\n",
       " Row(y=0.604297077495696, c='C4'),\n",
       " Row(y=-0.130385777600692, c='C2'),\n",
       " Row(y=1.046580302805, c='C2'),\n",
       " Row(y=1.11971764128954, c='C1'),\n",
       " Row(y=-0.309679541699787, c='C4'),\n",
       " Row(y=-0.0686927112547859, c='C2'),\n",
       " Row(y=0.772671308575245, c='C1'),\n",
       " Row(y=0.391331503418825, c='C4'),\n",
       " Row(y=3.40465338885795, c='C1'),\n",
       " Row(y=1.71324052011282, c='C2'),\n",
       " Row(y=2.67829720781629, c='C2'),\n",
       " Row(y=3.65865802699847, c='C2'),\n",
       " Row(y=1.50132182772373, c='C3'),\n",
       " Row(y=0.63459820311611, c='C3'),\n",
       " Row(y=2.51167228281089, c='C2'),\n",
       " Row(y=0.806027255329703, c='C3'),\n",
       " Row(y=1.4965781726617, c='C3'),\n",
       " Row(y=1.27866472410635, c='C3'),\n",
       " Row(y=2.55737037634497, c='C3'),\n",
       " Row(y=-0.0662001735130171, c='C3'),\n",
       " Row(y=1.35287447331848, c='C3'),\n",
       " Row(y=0.134236245155738, c='C2'),\n",
       " Row(y=-0.648221742158828, c='C3'),\n",
       " Row(y=1.62105420273818, c='C3'),\n",
       " Row(y=0.761543646720938, c='C3'),\n",
       " Row(y=-0.688692327990126, c='C4'),\n",
       " Row(y=1.10542139019377, c='C3'),\n",
       " Row(y=1.15649049169609, c='C2'),\n",
       " Row(y=1.67392867643757, c='C4'),\n",
       " Row(y=0.945122526288421, c='C1'),\n",
       " Row(y=1.06347409627587, c='C3'),\n",
       " Row(y=-0.0641651633334259, c='C2'),\n",
       " Row(y=1.42160336538475, c='C3'),\n",
       " Row(y=1.59625901661066, c='C1'),\n",
       " Row(y=2.46235153874643, c='C3'),\n",
       " Row(y=2.49781876103841, c='C2'),\n",
       " Row(y=0.524301715570466, c='C2'),\n",
       " Row(y=-0.244551518352649, c='C4'),\n",
       " Row(y=0.202723904639145, c='C4'),\n",
       " Row(y=-0.624364529760589, c='C3'),\n",
       " Row(y=2.1519117540872, c='C1'),\n",
       " Row(y=0.827331436053798, c='C4'),\n",
       " Row(y=-0.808327579354903, c='C3'),\n",
       " Row(y=0.511194365163378, c='C2'),\n",
       " Row(y=0.499303403997295, c='C2'),\n",
       " Row(y=1.0056419848525, c='C3'),\n",
       " Row(y=1.89062647643742, c='C2'),\n",
       " Row(y=0.692619085774501, c='C3'),\n",
       " Row(y=0.407256729925068, c='C4'),\n",
       " Row(y=1.59897514981031, c='C2'),\n",
       " Row(y=0.786114048503688, c='C3'),\n",
       " Row(y=1.0426502497967, c='C3'),\n",
       " Row(y=0.917476237982836, c='C3'),\n",
       " Row(y=1.14922073567414, c='C4'),\n",
       " Row(y=-0.0512937783976473, c='C4'),\n",
       " Row(y=3.44136462889459, c='C1'),\n",
       " Row(y=1.01915639166027, c='C1'),\n",
       " Row(y=1.14377148075807, c='C1'),\n",
       " Row(y=2.28692204113243, c='C4'),\n",
       " Row(y=1.68937269776547, c='C2'),\n",
       " Row(y=0.903475077237018, c='C2'),\n",
       " Row(y=0.938292577987962, c='C3'),\n",
       " Row(y=2.03451432394434, c='C3'),\n",
       " Row(y=1.51349371648542, c='C3'),\n",
       " Row(y=1.18190219140427, c='C3'),\n",
       " Row(y=1.5938409485975, c='C3'),\n",
       " Row(y=2.63464428548373, c='C3')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use rdd to create this \n",
    "# make c the key, and y the value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+---+------------------+\n",
      "|  c|               sum|  n|              mean|\n",
      "+---+------------------+---+------------------+\n",
      "| C3|106.72212894321214|100|1.0672212894321214|\n",
      "| C4|100.32253230729513|100|1.0032253230729513|\n",
      "| C1|102.26684498157968|100|1.0226684498157967|\n",
      "| C2| 95.44484415126036|100|0.9544484415126036|\n",
      "+---+------------------+---+------------------+\n",
      "\n",
      "[Row(c='C3', sum=106.72212894321214, n=100, mean=1.0672212894321214), Row(c='C4', sum=100.32253230729513, n=100, mean=1.0032253230729513), Row(c='C1', sum=102.26684498157968, n=100, mean=1.0226684498157967), Row(c='C2', sum=95.44484415126036, n=100, mean=0.9544484415126036)]\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import Row\n",
    "#df.createOrReplaceTempView(\"df\")\n",
    "df_classes=spark.sql(\"SELECT c, SUM(y) AS sum, \\\n",
    "            COUNT(y) AS n, \\\n",
    "            AVG(y) AS mean \\\n",
    "            FROM df \\\n",
    "            GROUP BY c\")\n",
    "df_classes.show()\n",
    "classes_array = df_classes.collect()\n",
    "print(classes_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create class sums as dictionary:\n",
    "\n",
    "{'C3': 100, 'C4': 100, 'C1': 100, 'C2': 100} <br>\n",
    "{'C3': 106.72212894321214, 'C4': 100.32253230729513, 'C1': 102.26684498157968, 'C2': 95.44484415126036}  <br>\n",
    "{'C3': 1.0672212894321214, 'C4': 1.0032253230729513, 'C1': 1.0226684498157967, 'C2': 0.9544484415126036}  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C3': 100, 'C1': 100, 'C4': 100, 'C2': 100} {'C3': 106.72212894321214, 'C1': 102.26684498157968, 'C4': 100.32253230729513, 'C2': 95.44484415126036} {'C3': 1.0672212894321214, 'C1': 1.0226684498157967, 'C4': 1.0032253230729513, 'C2': 0.9544484415126036}\n"
     ]
    }
   ],
   "source": [
    "#Skipped code\n",
    "d1={row[0]:row[2] for row in classes_array}\n",
    "d2={row[0]:row[1] for row in classes_array}\n",
    "d3={row[0]:row[3] for row in classes_array}\n",
    "print(d1,d2,d3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate `nClasses`, `nObs` and `grandMean`, so that line of code\n",
    "`nClasses, nObs, grandMean` returns:\n",
    "\n",
    "(4, 400, 1.011890875958368)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 400 1.0118908759583682\n"
     ]
    }
   ],
   "source": [
    "#Skipped code\n",
    "nClasses = df_classes.count() # count the number of rows in sqldf\n",
    "nObs= df.count()\n",
    "grandMean=(sum([row[3] for row in classes_array]))/nClasses\n",
    "print(nClasses, nObs,grandMean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate `between_ss` and `meanSqClass = between_ss / dfClass`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2184111945098246\n"
     ]
    }
   ],
   "source": [
    "#meanSqClass\n",
    "betweenSS = sum([(row[3]-grandMean)**2 for row in classes_array])\n",
    "dfClass = nClasses -1\n",
    "meanSqClass = 100*betweenSS/dfClass\n",
    "print(meanSqClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate within sum of squares `withinSS` and `meanSqResid=withinSS / dfResid`\n",
    "\n",
    "0.9281210461910595"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------------------+---+------------------+--------------------+\n",
      "|  c|                 y|               sum|  n|              mean|                  sq|\n",
      "+---+------------------+------------------+---+------------------+--------------------+\n",
      "| C2|0.0893193175067113| 95.44484415126036|100|0.9544484415126036|  0.7484484012032026|\n",
      "| C4|  1.03892811665213|100.32253230729513|100|1.0032253230729513|0.001274689469357445|\n",
      "| C1| 0.710538426311777|102.26684498157968|100|1.0226684498157967| 0.09742515157261993|\n",
      "| C4|  1.54878758817833|100.32253230729513|100|1.0032253230729513|  0.2976381851069114|\n",
      "| C3|  1.54970766958625|106.72212894321214|100|1.0672212894321214| 0.23279310703423428|\n",
      "+---+------------------+------------------+---+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9281210461910595"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calc withinSS: use Spark again\n",
    "#meanSqResid\n",
    "large_df = df.join(df_classes,'c')\n",
    "large_df = large_df.withColumn('sq',(large_df.y - large_df.mean)**2)\n",
    "large_df.show(5)\n",
    "wSS = large_df.groupBy().sum('sq').collect()\n",
    "withinSS = wSS[0][0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dfResid = nObs-4\n",
    "meanSqResid = withinSS/dfResid\n",
    "meanSqResid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate `f-ststistics` and `p-value`\n",
    "\n",
    "f_statistics:\n",
    "\n",
    "0.2353261952265473\n",
    "\n",
    "p-value:\n",
    "\n",
    "0.87173536999418566"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2353261952265473"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Skipped code\n",
    "f_stat=meanSqClass/meanSqResid\n",
    "f_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87173536999418877"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p-value\n",
    "from scipy.stats import f\n",
    "1-f.cdf(f_stat,dfClass ,dfResid)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
