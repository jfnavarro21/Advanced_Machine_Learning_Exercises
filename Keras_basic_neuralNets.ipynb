{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "f765eb10-54e6-4a14-912b-e5117e8f433d"
    }
   },
   "source": [
    "# Advanced Machine Learning (MScA, 32017)\n",
    "\n",
    "# Project Paraphrase Detection, Part 2\n",
    "\n",
    "# Introduction to Keras\n",
    "\n",
    "### Yuri Balasanov, Leonid Nazarov, &copy; iLykei 2017\n",
    "\n",
    "Keras is an open source neural network library written in Python. It is capable of running on top of MXNet, Deeplearning4j, Tensorflow, CNTK or Theano. Designed to enable fast experimentation with deep neural networks, it focuses on being minimal, modular and extensible. It was developed as part of the research effort of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System), and its primary author and maintainer is François Chollet, a Google engineer.\n",
    "\n",
    "# Data for examples\n",
    "\n",
    "Examples in this notebook use [**breast cancer wisconsin dataset**](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)). \n",
    "\n",
    "The breast cancer dataset is a well known data for binary classification. It was created by Dr. William H. Wolberg, physician at the University Of Wisconsin Hospital at Madison, Wisconsin, USA.\n",
    "\n",
    "Features are created from a digitized images of a [fine needle aspirate (FNA)](https://en.wikipedia.org/wiki/Fine-needle_aspiration), a biopsy method, of a breast mass. They describe characteristics of the cell nuclei present in the image.\n",
    "\n",
    "To create the dataset Dr. Wolberg used ﬂuid samples, taken from patients with solid breast masses and graphical software [Xcyt](http://software.broadinstitute.org/mpg/xcyt/) capable of performing analysis of cytological features on a digital scan. \n",
    "\n",
    "Main research article on the subject of this analysis, including an image of malignant cells, is [Wisconsin Breast Cancer Dataset and Machine Learning for Breast Cancer Detection, by Lucas Rodrigues Borges](https://www.researchgate.net/publication/311950799_Analysis_of_the_Wisconsin_Breast_Cancer_Dataset_and_Machine_Learning_for_Breast_Cancer_Detection [accessed Nov 09 2017]).\n",
    "\n",
    "The data set has following characteristics:\n",
    "\n",
    "- Classes:\t2\n",
    "- Samples per class:\t212(malignant), 357(benign)\n",
    "- Samples total:\t569\n",
    "- Dimensionality:\t30\n",
    "- Features:\treal, positive\n",
    "\n",
    "Load the data using utility *load_breast_cancer* from *sklearn.datasets* module. <br>\n",
    "Print names of the features and dimensions of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "data (569, 30) labels (569,)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data  = load_breast_cancer()\n",
    "print(data.feature_names)\n",
    "print('data',data.data.shape,'labels',data.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential model\n",
    "\n",
    "The core data structure of Keras is **model**, a way to organize layers of the neural network. <br>\n",
    "\n",
    "The simplest type of model is the Sequential model, a linear stack of layers.<br>\n",
    "\n",
    "Design a simple neural network with one hidden layer consisting of 3 neurons.  \n",
    "\n",
    "First, define the type of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "model =  Sequential() # create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Specify the first (hidden) layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=3, input_dim=30)) # add hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative way of calling the same function creating hidden layer is: <br>\n",
    "\n",
    "`model.add(Dense(3, input_dim=30, activation='relu')).` <br>\n",
    "\n",
    "Parameter `input_dim` sets number of inputs (predictors). <br>\n",
    "Parameter `activation` is set to `relu` which is a common [activation function](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) for neural networks.\n",
    "\n",
    "Input layer is not explicitly created. Instead, only number of neurons (features) that feed into the first hidden layer, needs to be specified (30 for the breast cancer dataset). \n",
    "\n",
    "There is also no need to worry about the input dimensions for subsequent layers: neurons in the previous layer form input for the next layer. The output dimension of the first hidden layer is equal to 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selection of activation function for the hidden layer csn br donr from the list of available activations, providing formulas for the most popular ones: \n",
    "\n",
    "- softmax(x) :   $f_{i}(x)=\\frac{e^{x_{i}}}{\\sum_{i=1}^{n}e^{x_{i}}},i=1,...,n$\n",
    "- elu(x)\n",
    "- selu(x)\n",
    "- softplus(x) = $ln(1 + e^{x})$\n",
    "- softsign(x) = $\\frac {x}{1+|x|}$\n",
    "- relu(x) = $max(0,x)$; (those familiar with options trading will recognize call option payof function in `relu`)\n",
    "- tanh(x) \n",
    "- sigmoid(x) = $\\frac {1}{1+e^{-x}}$\n",
    "- hard_sigmoid(x)\n",
    "- linear(x) = $x$\n",
    "- LeakyReLU(x,alpha) = $max(\\alpha *x,x)$\n",
    "\n",
    "Select 'relu' as activation function usually providing faster convergence relative to 'tanh' or 'sigmoid'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Activation('relu')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the output layer with 1 neuron and 'sigmoid' activation function producing predictive probability for Class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output layer for binary classification\n",
    "model.add(Dense(units=1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot displays the nework architecture and explains the nature of **Dense** (fully connected) layer: each neuron of it is connected with all input variables of the next layer.\n",
    " \n",
    "![Model plot](https://ilykei.com/api/fileProxy/documents%2FAdvanced%20Machine%20Learning%2FLecture%207%20AdvML%2FR_nn_plot.png)\n",
    "\n",
    "The picture was created with function *plot()* from R **neuralnet** package. <br>\n",
    "\n",
    "Keras has tools for model visualization too. Module *keras.utils.vis_utils* provides utility functions to plot a Keras model (using **graphviz**). Unlike the above R function this utility does not show neurons. It describes network in terms of layers.   \n",
    "\n",
    "The code below plots another graph of the same network model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"337pt\" viewBox=\"0.00 0.00 174.00 337.00\" width=\"174pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 333)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-333 170,-333 170,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 1827861222064 -->\n",
       "<g class=\"node\" id=\"node1\"><title>1827861222064</title>\n",
       "<polygon fill=\"none\" points=\"0,-292.5 0,-328.5 166,-328.5 166,-292.5 0,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-306.8\">dense_1_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 1827861221504 -->\n",
       "<g class=\"node\" id=\"node2\"><title>1827861221504</title>\n",
       "<polygon fill=\"none\" points=\"31,-219.5 31,-255.5 135,-255.5 135,-219.5 31,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-233.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 1827861222064&#45;&gt;1827861221504 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>1827861222064-&gt;1827861221504</title>\n",
       "<path d=\"M83,-292.313C83,-284.289 83,-274.547 83,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"86.5001,-265.529 83,-255.529 79.5001,-265.529 86.5001,-265.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1827861222624 -->\n",
       "<g class=\"node\" id=\"node3\"><title>1827861222624</title>\n",
       "<polygon fill=\"none\" points=\"9,-146.5 9,-182.5 157,-182.5 157,-146.5 9,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-160.8\">activation_1: Activation</text>\n",
       "</g>\n",
       "<!-- 1827861221504&#45;&gt;1827861222624 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>1827861221504-&gt;1827861222624</title>\n",
       "<path d=\"M83,-219.313C83,-211.289 83,-201.547 83,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"86.5001,-192.529 83,-182.529 79.5001,-192.529 86.5001,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1827861436568 -->\n",
       "<g class=\"node\" id=\"node4\"><title>1827861436568</title>\n",
       "<polygon fill=\"none\" points=\"31,-73.5 31,-109.5 135,-109.5 135,-73.5 31,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-87.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 1827861222624&#45;&gt;1827861436568 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>1827861222624-&gt;1827861436568</title>\n",
       "<path d=\"M83,-146.313C83,-138.289 83,-128.547 83,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"86.5001,-119.529 83,-109.529 79.5001,-119.529 86.5001,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1827861436456 -->\n",
       "<g class=\"node\" id=\"node5\"><title>1827861436456</title>\n",
       "<polygon fill=\"none\" points=\"9,-0.5 9,-36.5 157,-36.5 157,-0.5 9,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-14.8\">activation_2: Activation</text>\n",
       "</g>\n",
       "<!-- 1827861436568&#45;&gt;1827861436456 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>1827861436568-&gt;1827861436456</title>\n",
       "<path d=\"M83,-73.3129C83,-65.2895 83,-55.5475 83,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"86.5001,-46.5288 83,-36.5288 79.5001,-46.5289 86.5001,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pydot\n",
    "#print pydot.find_graphviz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You can also save it to a file and then plot it with the function *plot_model*.  \n",
    " *plot_model* takes two optional arguments:\n",
    "- *show_shapes* (defaults to False) controls whether output shapes are shown in the graph.\n",
    "- *show_layer_names* (defaults to True) controls whether layer names are shown in the graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model1.png',show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the plot of the the same network model with additional information loaded from the file *model1.png* using a line of markdown code:\n",
    "\n",
    "\"! [model] (./Path to model)\"\n",
    "\n",
    "The resulting plot is:\n",
    "\n",
    "![Model plot](https://ilykei.com/api/fileProxy/documents%2FAdvanced%20Machine%20Learning%2FLecture%207%20AdvML%2Fmodel1.png)\n",
    "\n",
    "Summary of the model can also be shown including layers types, shapes and the number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 3)                 93        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 97\n",
      "Trainable params: 97\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use more compact syntax for describing the same Sequential network: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model =  Sequential()\n",
    "model.add(Dense(3, input_dim=30,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or even define the model in one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([Dense(3, input_dim=30),Activation('relu'),\n",
    "    Dense(1),Activation('sigmoid')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More complex architecture. Functional API\n",
    "\n",
    "Keras functional API provides tools for defining more complex models, such as multi-output models, directed acyclic graphs, or models with shared layers. \n",
    "\n",
    "Models are defined by creating instances of layers and connecting them directly to each other pair by pair, then defining a model that specifies the layers to act as the input and output to the model.\n",
    "\n",
    "Define the above simple model using functional API. \n",
    "\n",
    "Start with the input layer. Recall that input layer was not explicitly created in sequential model. Here it must be created and defined as a standalone Input layer that specifies the shape of input data.\n",
    "\n",
    "The input layer takes shape (tensor shape) argument that is a tuple that indicates the dimensionality of the input data.\n",
    "\n",
    "When input is one-dimensional, as in the example above, the shape must explicitly leave room for the shape of the mini-batch size used when splitting the data during training process. \n",
    "\n",
    "Therefore, the shape tuple is always defined with a hanging last dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_1:0' shape=(?, 30) dtype=float32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "\n",
    "# This returns a tensor\n",
    "inputs = Input(shape=(30,))\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A layer instance is callable (on a tensor), and it returns a tensor. The layers in the model are connected pairwise. <br>\n",
    "This is done by specifying where the input comes from when defining each new layer. \n",
    "\n",
    "The following line of code states that the input of Dense layer 'hidden_1' comes from *inputs*. <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden = Dense(3, activation='relu',name='hidden_1')(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter *name* is optional. It can be used to pass a name of any layer as an argument.<br>\n",
    "\n",
    "Define the output layer and create a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "output = Dense(1, activation='sigmoid')(hidden)\n",
    "model_API = Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model class is used to create a model from layers. It requires only the input and output layers specifications.\n",
    "\n",
    "Here is the model we built.\n",
    "![model_API](https://ilykei.com/api/fileProxy/documents%2FAdvanced%20Machine%20Learning%2FLecture%207%20AdvML%2Fmodel_API.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Concatenating inputs\n",
    "\n",
    "Consider now more complex models. \n",
    "\n",
    "For such constructions inputs need to be *concatenated* from a list of inputs.\n",
    "\n",
    "Input is taken as a list of tensors and the result is a single tensor, the concatenation of all inputs. \n",
    "\n",
    "All tensors must have the same shape except for the concatenation axis. <br>\n",
    "\n",
    "The meaning of the condition above can be illustrated on a simple example of tensors of rank 2 which are matrices. <br>\n",
    "Consider 2 matrices $A,~B$ with the same number of columns, but different number of rows:\n",
    "$$A=\\{a_{i,j},~i=0,1,2;~j=0,1,2,3 \\};~B=\\{b_{i,j},~i=0,1,2,3,4;~j=0,1,2,3 \\}.$$\n",
    "These matrices as inputs can be concatenated by first dimension (axis), i.e. by appending rows of $B$ to rows of $A$. <br>\n",
    "But they cannot be concatenated by the second dimension: columns of $A$ cannot be concatenated with columns of $B$ because the numbers of rows are different.\n",
    "\n",
    "Build a model with two inputs using layer of concatenated inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import concatenate\n",
    "input1 = Input(shape=(10,), name='input_1')\n",
    "hidden = Dense(3, activation='relu',name='hidden1')(input1)\n",
    "input2 = Input(shape=(5,), name='input_2')\n",
    "merge = concatenate([hidden,input2], name='merge')\n",
    "hidden2 = Dense(3, activation='relu',name='hidden2')(merge)\n",
    "output = Dense(1, activation='sigmoid')(hidden2)\n",
    "model_2_inputs = Model(inputs=[input1,input2], outputs=output)\n",
    "plot_model(model_2_inputs, to_file='model_2_inputs.png',show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the result.\n",
    "![model_2_inputs](https://ilykei.com/api/fileProxy/documents%2FAdvanced%20Machine%20Learning%2FLecture%207%20AdvML%2Fmodel_2_inputs.png)  \n",
    "\n",
    "## Shared layers\n",
    "\n",
    "Another way of adding flexibility to network design is shared layers.\n",
    "\n",
    "In Quora project we need to identify duplicated pairs of questions. \n",
    "\n",
    "One way to achieve this is to: \n",
    "- Encode two questions into two vectors; \n",
    "- Feed vectors to the same layer and compare outputs. Similar questions should produce similar outputs.  \n",
    "\n",
    "To share a layer across different inputs, simply instantiate the layer once, then call it on as many inputs as necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input1 = Input(shape=(8,), name='input_1')\n",
    "input2 = Input(shape=(8,), name='input_2')\n",
    "shared = Dense(5, activation='relu',name='shared')\n",
    "x1 = shared(input1)\n",
    "x2 = shared(input2)\n",
    "output1 = Dense(1, activation='sigmoid', name='output_1')(x1)\n",
    "output2 = Dense(1, activation='sigmoid', name='output_2')(x2)\n",
    "model_shared = Model(inputs=[input1,input2], outputs=[output1,output2])\n",
    "plot_model(model_shared, to_file='model_shared.png',\n",
    "           show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the model with two inputs, two outputs and shared layer\n",
    "![model_shared](https://ilykei.com/api/fileProxy/documents%2FAdvanced%20Machine%20Learning%2FLecture%207%20AdvML%2Fmodel_shared.png)  \n",
    "\n",
    "Note that when we reuse the same layer instance  multiple times, the weights of the layer  are also being reused  (it is effectively the same layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "\n",
    "**Assignment 1**\n",
    "\n",
    "Write code creating the following network architecture as *test_model1*.\n",
    "\n",
    "![test_model1](https://ilykei.com/api/fileProxy/documents%2FAdvanced%20Machine%20Learning%2FLecture%207%20AdvML%2Ftest_model1.png) \n",
    "\n",
    "Print out the plot of the resulting model.\n",
    "\n",
    "Enter code in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Skipped code\n",
    "# Assignment 1\n",
    "input1 = Input(shape=(3,), name='input_1')\n",
    "hidden1 = Dense(5, activation='relu',name='hidden1')(input1)\n",
    "input2 = Input(shape=(6,), name='input_2')\n",
    "merge1 = concatenate([hidden1,input2], name='merge1')\n",
    "input3 = Input(shape=(11,), name='input_3')\n",
    "shared = Dense(11, activation='relu',name='shared')\n",
    "x1 = shared(merge1)\n",
    "x2 = shared(input3)\n",
    "x2 = Dense(9, activation='relu',name='hidden2')(x2)\n",
    "merge = concatenate([x1,x2], name='merge2')\n",
    "output1 = Dense(1, activation='sigmoid', name='output_1')(merge)\n",
    "test_model1 = Model(inputs=[input1,input2,input3], outputs=output1)\n",
    "\n",
    "plot_model(test_model1, to_file='test_model1.png',\n",
    "          show_shapes=True,show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile model\n",
    "\n",
    "Once the model plot is finalized, configure its learning process using *compile()*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here *'model'* is the sequential model created above.  \n",
    "\n",
    "Compillation of the model requires declaration of the loss function (objective function) and the optimizer.\n",
    "\n",
    "Keras has a variety of [loss functions](https://keras.io/losses/) and [optimizers](https://keras.io/optimizers/) to choose from. \n",
    "\n",
    "An optional parameter *metric* also can be set. Metric is a function (or list of functions) that is used to judge the performance of the model. \n",
    "\n",
    "Metric can be selected from the [list of available metrics](https://keras.io/metrics/) or passed as custom metric function name during the compilation step. <br>\n",
    "The function would need to take  *(y_true, y_pred)* as arguments and return a single tensor value.\n",
    "\n",
    "In the example below we use logarithmic loss as objective function, which for a binary classification problem is defined in Keras as *'binary_crossentropy'*. \n",
    "\n",
    "We  also use gradient descent algorithm 'adam' and choose *'accuracy'* as metric for collectiing and reporting during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model on training data\n",
    "\n",
    "Train or fiting the model on loaded data is done by calling method *'fit'* on the model. \n",
    "\n",
    "Split the breast cancer dataset into train and test subsets and define simple sequential model with two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target,\n",
    "                                                    test_size=0.2, random_state=1)\n",
    "def get_model_1():\n",
    "    model =  Sequential()\n",
    "    model.add(Dense(5, input_dim=30,activation='relu'))\n",
    "    model.add(Dense(10,activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "                  metrics=['accuracy'])    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and compiling model by a function instead of creating it directly allows experimenting and fitting with the model multiple times. <br>\n",
    "If model is fitted again it will use the previously fitted weights which makes fitting experiment incorrect. <br>\n",
    "In order to reinitialize weights call function generating the model.  \n",
    "\n",
    "Fit the model on train set using 10 epochs and iterating on the data in batches of 32 samples.\n",
    "\n",
    "- Parameter *'epoch'* is an arbitrary cutoff, typically means one pass over entire dataset, used to separate training process in distinct phases. <br>\n",
    "- Parameter *'batch_size'* is size of batch, a set of samples (observations) that is a subsample processed simultaneously. Processing of each batch results in one update of the network weights. <br>\n",
    "- Batch approximates distribution of weights better than single observation. The larger the batch the better the approximation. But larger batch processing requires more memory. \n",
    "- For evaluation or prediction it is recommended to use as large batch size as mempry allows since it makes the inference faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "455/455 [==============================] - 0s - loss: 10.0959 - acc: 0.3736     \n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s - loss: 10.0959 - acc: 0.3736     \n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s - loss: 10.0959 - acc: 0.3736     \n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s - loss: 10.0959 - acc: 0.3736     \n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s - loss: 10.0959 - acc: 0.3736    \n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s - loss: 10.0960 - acc: 0.3736    \n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s - loss: 10.0959 - acc: 0.3736    \n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s - loss: 10.0960 - acc: 0.3736    \n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s - loss: 10.0959 - acc: 0.3736     \n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s - loss: 10.0959 - acc: 0.3736     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x173cb60ba20>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model_1()\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no progress in training process. <br>\n",
    "Scale the input data and fit  the model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "455/455 [==============================] - 0s - loss: 0.7751 - acc: 0.3516     \n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s - loss: 0.6760 - acc: 0.5341     \n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s - loss: 0.6065 - acc: 0.7407     \n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s - loss: 0.5479 - acc: 0.8374     \n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s - loss: 0.4856 - acc: 0.8747     \n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s - loss: 0.4205 - acc: 0.9011     \n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s - loss: 0.3563 - acc: 0.9209     \n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s - loss: 0.3030 - acc: 0.9297     \n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s - loss: 0.2586 - acc: 0.9341     \n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s - loss: 0.2222 - acc: 0.9407     \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "model = get_model_1()\n",
    "hist = model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling significantly improved fitting results. \n",
    "\n",
    "Method *'fit'* returns a History object. <br>\n",
    "Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'acc'])\n",
      "best train loss 0.222168984469\n"
     ]
    }
   ],
   "source": [
    "print(hist.history.keys())\n",
    "print('best train loss', min(hist.history[\"loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now predict test labels using method *'predict'* and evaluate prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1 = model.predict(X_test)\n",
    "pred1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method *evaluate* returns values of all metrics the model calculates. Their names are in the attribute *.metrics_names*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "[0.26024451328997028, 0.92982455722072666]\n"
     ]
    }
   ],
   "source": [
    "print(model.metrics_names)\n",
    "print(model.evaluate(X_test, y_test,verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "**Assignment 2**\n",
    "Compare loss of our network on the train  data scaled with *MinMaxScaler* and *StandardScaler*. <br>\n",
    "Run fiting process 10 times for each scaler and average the results. <br>\n",
    "Set  *verbose=0* in the *'fit()'* call to remove unnecessary logging. <br>\n",
    "Results may differ unsignificantly from the sample printout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Skipped code\n",
    "# Assignment 2\n",
    "#MinMaxScaler\n",
    "#0 0.590108319953\n",
    "#1 0.589973957591\n",
    "#2 0.502516596461\n",
    "#3 0.551777010632\n",
    "#4 0.492175462875\n",
    "#5 0.578729445201\n",
    "#6 0.555293402043\n",
    "#7 0.595016273299\n",
    "#8 0.577328508634\n",
    "#9 0.556122513001\n",
    "#average loss =  0.558904148969\n",
    "\n",
    "#StandardScaler\n",
    "#0 0.44238442307\n",
    "#1 0.398653513324\n",
    "#2 0.274371146108\n",
    "#3 0.337022536731\n",
    "#4 0.400712529251\n",
    "#5 0.283597299326\n",
    "#6 0.234807680596\n",
    "#7 0.237801259797\n",
    "#8 0.248024749887\n",
    "#9 0.176053563153\n",
    "#average loss =  0.303342870124\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization in Keras\n",
    "\n",
    "Keras has its own tool for data scaling: *'BatchNormalization'* layer. <br>\n",
    "It normalizes the activations of the previous layer at each batch, i.e. applies a transformation that maintains the mean activation (previous layer' output) close to 0 and the activation standard deviation close to 1. <br>\n",
    "Sergey Ioffe and Christian Szegedy show in [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167) that Batch Normalization improves convergence speed  of deep neural networks.  \n",
    "\n",
    "In order to apply BatchNormalization to the *pevious_layer* in Keras functional API you need two lines of code:\n",
    "\n",
    "`*from keras.layers import BatchNormalization*`  \n",
    "`*scaled = BatchNormalization()(input1)*`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "**Assignment 3**\n",
    "\n",
    "1. Change function *'get_model_1'* into *'get_model_2'* in functional API and insert BatchNormalization layer after *'input1'*. <br>\n",
    "2. Currently *X_train, X_test* are scaled. In order to restore the original train and test split the data again. \n",
    "3. Run fiting 10 times and average results. \n",
    "4. Set verbose=0 in the fit() call to remove unnecessary logging. \n",
    "5. Print loss function on every iteration.\n",
    "6. Compare average loss for  MinMaxScaler, StandardScaler and BatchNormalization.\n",
    "\n",
    "Results of different runs may not be exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Skipped code\n",
    "# Assignment 3 code\n",
    "\n",
    "#0 0.322570931387\n",
    "#1 0.323956322997\n",
    "#2 0.240647373062\n",
    "#3 0.201014531576\n",
    "#4 0.25480208731\n",
    "#5 0.223247167042\n",
    "#6 0.35539250924\n",
    "#7 0.235532289121\n",
    "#8 0.336723536449\n",
    "#9 0.207742149293\n",
    "#average loss =  0.270162889748\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks\n",
    "\n",
    "Keras has a set of functions that can be used to get a view on internal states and statistics of the model during training. They are called callbacks and *History* mentioned above is one of them.\n",
    "\n",
    "A list of callbacks can be passed (as the keyword argument *callbacks*) to the  *fit()* method of the Sequential or Model classes. The relevant methods of the callbacks will then be called at each stage of the training.  \n",
    "\n",
    "Here are some usefull callbacks that will be used below.\n",
    "\n",
    "- *CSVLogger*  \n",
    "    *keras.callbacks.CSVLogger(filename, separator=',', append=False)*  \n",
    "    Callback that streams epoch results to a csv file.  \n",
    "\n",
    "\n",
    "- *ModelCheckpoint*  \n",
    "    *keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)*  \n",
    "    By default save the model to the file *filepath* after every epoch. If  *save_best_only=True* only the latest best model is saved according to the quantity monitored. So one can restart trainning from the last saved state in case of failure. This is important when training deep learning models, which can often be a time-consuming task.  \n",
    "\n",
    "Here is an example of how we can save the model and restore it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: saving model to weights.hdf5\n",
      "Epoch 00001: saving model to weights.hdf5\n",
      "Epoch 00002: saving model to weights.hdf5\n",
      "Epoch 00003: saving model to weights.hdf5\n",
      "Epoch 00004: saving model to weights.hdf5\n",
      "Epoch 00005: saving model to weights.hdf5\n",
      "Epoch 00006: saving model to weights.hdf5\n",
      "Epoch 00007: saving model to weights.hdf5\n",
      "Epoch 00008: saving model to weights.hdf5\n",
      "Epoch 00009: saving model to weights.hdf5\n",
      "evaluation results of trained model\n",
      "['loss', 'acc']\n",
      "[0.21692870792589689, 0.92982455722072666]\n",
      "evaluation results of reinitialized model\n",
      "[0.70664922396341956, 0.66666666771236216]\n",
      "evaluation results of model with weights loaded from file\n",
      "[0.21692870792589689, 0.92982455722072666]\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# initialize model\n",
    "model = get_model_1()\n",
    "# saves the model weights after each epoch\n",
    "checkpointer = ModelCheckpoint('weights.hdf5', verbose=1)\n",
    "model.fit(X_train, y_train, epochs=10, verbose=0, callbacks=[checkpointer])\n",
    "print('evaluation results of trained model')\n",
    "print(model.metrics_names)\n",
    "print(model.evaluate(X_test, y_test,verbose=0))\n",
    "# reinitialize model\n",
    "model = get_model_1()\n",
    "print('evaluation results of reinitialized model')\n",
    "print(model.evaluate(X_test, y_test,verbose=0))\n",
    "model.load_weights('weights.hdf5')\n",
    "print('evaluation results of model with weights loaded from file')\n",
    "print(model.evaluate(X_test, y_test,verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by loading weights from file we restore saved model.\n",
    "\n",
    "- *EarlyStopping*  \n",
    "    *keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')*  \n",
    "    Stop training when a monitored quantity has stopped improving. Using it we can set large number of epochs but the process will be stopped if validation loss is not improved during *patience* epochs.  \n",
    "\n",
    "In the example below define *EarlyStopping* callback with *patience=5* and validation loss (\"val_loss\") as monitored quantity. \n",
    "\n",
    "Validation loss can be calculated only if *validation_data* parameter in the *fit* method call is defined. <br>\n",
    "In this case *validation_data=(X_test, y_test)*. <br>\n",
    "Allow 200 epochs to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 455 samples, validate on 114 samples\n",
      "Epoch 1/200\n",
      "455/455 [==============================] - 0s - loss: 9.9673 - acc: 0.3736 - val_loss: 9.9359 - val_acc: 0.3684\n",
      "Epoch 2/200\n",
      "455/455 [==============================] - 0s - loss: 9.6829 - acc: 0.3736 - val_loss: 9.4772 - val_acc: 0.3684\n",
      "Epoch 3/200\n",
      "455/455 [==============================] - 0s - loss: 8.7335 - acc: 0.3736 - val_loss: 7.9285 - val_acc: 0.3684\n",
      "Epoch 4/200\n",
      "455/455 [==============================] - 0s - loss: 6.3948 - acc: 0.3736 - val_loss: 4.6421 - val_acc: 0.3684\n",
      "Epoch 5/200\n",
      "455/455 [==============================] - 0s - loss: 3.0749 - acc: 0.3736 - val_loss: 1.8249 - val_acc: 0.3684\n",
      "Epoch 6/200\n",
      "455/455 [==============================] - 0s - loss: 1.0763 - acc: 0.3758 - val_loss: 0.5783 - val_acc: 0.8333\n",
      "Epoch 7/200\n",
      "455/455 [==============================] - 0s - loss: 0.5916 - acc: 0.7363 - val_loss: 0.5357 - val_acc: 0.8509\n",
      "Epoch 8/200\n",
      "455/455 [==============================] - 0s - loss: 0.5095 - acc: 0.8198 - val_loss: 0.5070 - val_acc: 0.8684\n",
      "Epoch 9/200\n",
      "455/455 [==============================] - 0s - loss: 0.4659 - acc: 0.8703 - val_loss: 0.4794 - val_acc: 0.8509\n",
      "Epoch 10/200\n",
      "455/455 [==============================] - 0s - loss: 0.4428 - acc: 0.8879 - val_loss: 0.4666 - val_acc: 0.8421\n",
      "Epoch 11/200\n",
      "455/455 [==============================] - 0s - loss: 0.4364 - acc: 0.8505 - val_loss: 0.4575 - val_acc: 0.8421\n",
      "Epoch 12/200\n",
      "455/455 [==============================] - 0s - loss: 0.4202 - acc: 0.8593 - val_loss: 0.4452 - val_acc: 0.8509\n",
      "Epoch 13/200\n",
      "455/455 [==============================] - 0s - loss: 0.4008 - acc: 0.8747 - val_loss: 0.4229 - val_acc: 0.8947\n",
      "Epoch 14/200\n",
      "455/455 [==============================] - 0s - loss: 0.3871 - acc: 0.8791 - val_loss: 0.4112 - val_acc: 0.8860\n",
      "Epoch 15/200\n",
      "455/455 [==============================] - 0s - loss: 0.3782 - acc: 0.8901 - val_loss: 0.4015 - val_acc: 0.8860\n",
      "Epoch 16/200\n",
      "455/455 [==============================] - 0s - loss: 0.3617 - acc: 0.8967 - val_loss: 0.3899 - val_acc: 0.8947\n",
      "Epoch 17/200\n",
      "455/455 [==============================] - 0s - loss: 0.3542 - acc: 0.8879 - val_loss: 0.3829 - val_acc: 0.8860\n",
      "Epoch 18/200\n",
      "455/455 [==============================] - 0s - loss: 0.3420 - acc: 0.8967 - val_loss: 0.3710 - val_acc: 0.8772\n",
      "Epoch 19/200\n",
      "455/455 [==============================] - 0s - loss: 0.3375 - acc: 0.8989 - val_loss: 0.3617 - val_acc: 0.8860\n",
      "Epoch 20/200\n",
      "455/455 [==============================] - 0s - loss: 0.3239 - acc: 0.9033 - val_loss: 0.3624 - val_acc: 0.8772\n",
      "Epoch 21/200\n",
      "455/455 [==============================] - 0s - loss: 0.3190 - acc: 0.8879 - val_loss: 0.3450 - val_acc: 0.9035\n",
      "Epoch 22/200\n",
      "455/455 [==============================] - 0s - loss: 0.3115 - acc: 0.9099 - val_loss: 0.3407 - val_acc: 0.8860\n",
      "Epoch 23/200\n",
      "455/455 [==============================] - 0s - loss: 0.3043 - acc: 0.9077 - val_loss: 0.3305 - val_acc: 0.8947\n",
      "Epoch 24/200\n",
      "455/455 [==============================] - 0s - loss: 0.2957 - acc: 0.8967 - val_loss: 0.3282 - val_acc: 0.8860\n",
      "Epoch 25/200\n",
      "455/455 [==============================] - 0s - loss: 0.2905 - acc: 0.9077 - val_loss: 0.3162 - val_acc: 0.8947\n",
      "Epoch 26/200\n",
      "455/455 [==============================] - 0s - loss: 0.2831 - acc: 0.9055 - val_loss: 0.3148 - val_acc: 0.8947\n",
      "Epoch 27/200\n",
      "455/455 [==============================] - 0s - loss: 0.2804 - acc: 0.9077 - val_loss: 0.3040 - val_acc: 0.8947\n",
      "Epoch 28/200\n",
      "455/455 [==============================] - 0s - loss: 0.2731 - acc: 0.9165 - val_loss: 0.3009 - val_acc: 0.8947\n",
      "Epoch 29/200\n",
      "455/455 [==============================] - 0s - loss: 0.2716 - acc: 0.9165 - val_loss: 0.3037 - val_acc: 0.8947\n",
      "Epoch 30/200\n",
      "455/455 [==============================] - 0s - loss: 0.2667 - acc: 0.9143 - val_loss: 0.2889 - val_acc: 0.9035\n",
      "Epoch 31/200\n",
      "455/455 [==============================] - 0s - loss: 0.2607 - acc: 0.9099 - val_loss: 0.2854 - val_acc: 0.8947\n",
      "Epoch 32/200\n",
      "455/455 [==============================] - 0s - loss: 0.2552 - acc: 0.9187 - val_loss: 0.2786 - val_acc: 0.8947\n",
      "Epoch 33/200\n",
      "455/455 [==============================] - 0s - loss: 0.2501 - acc: 0.9187 - val_loss: 0.2780 - val_acc: 0.8947\n",
      "Epoch 34/200\n",
      "455/455 [==============================] - 0s - loss: 0.2526 - acc: 0.9187 - val_loss: 0.2750 - val_acc: 0.8947\n",
      "Epoch 35/200\n",
      "455/455 [==============================] - 0s - loss: 0.2433 - acc: 0.9143 - val_loss: 0.2666 - val_acc: 0.8947\n",
      "Epoch 36/200\n",
      "455/455 [==============================] - 0s - loss: 0.2430 - acc: 0.9187 - val_loss: 0.2624 - val_acc: 0.8947\n",
      "Epoch 37/200\n",
      "455/455 [==============================] - 0s - loss: 0.2400 - acc: 0.9165 - val_loss: 0.2924 - val_acc: 0.8947\n",
      "Epoch 38/200\n",
      "455/455 [==============================] - 0s - loss: 0.2589 - acc: 0.9033 - val_loss: 0.2575 - val_acc: 0.9123\n",
      "Epoch 39/200\n",
      "455/455 [==============================] - 0s - loss: 0.2411 - acc: 0.9099 - val_loss: 0.2539 - val_acc: 0.9123\n",
      "Epoch 40/200\n",
      "455/455 [==============================] - 0s - loss: 0.2313 - acc: 0.9231 - val_loss: 0.2554 - val_acc: 0.8947\n",
      "Epoch 41/200\n",
      "455/455 [==============================] - 0s - loss: 0.2294 - acc: 0.9187 - val_loss: 0.2472 - val_acc: 0.8947\n",
      "Epoch 42/200\n",
      "455/455 [==============================] - 0s - loss: 0.2252 - acc: 0.9209 - val_loss: 0.2544 - val_acc: 0.8947\n",
      "Epoch 43/200\n",
      "455/455 [==============================] - 0s - loss: 0.2264 - acc: 0.9121 - val_loss: 0.2468 - val_acc: 0.9123\n",
      "Epoch 44/200\n",
      "455/455 [==============================] - 0s - loss: 0.2279 - acc: 0.9165 - val_loss: 0.2408 - val_acc: 0.9211\n",
      "Epoch 45/200\n",
      "455/455 [==============================] - 0s - loss: 0.2215 - acc: 0.9275 - val_loss: 0.2418 - val_acc: 0.8947\n",
      "Epoch 46/200\n",
      "455/455 [==============================] - 0s - loss: 0.2151 - acc: 0.9187 - val_loss: 0.2350 - val_acc: 0.8947\n",
      "Epoch 47/200\n",
      "455/455 [==============================] - 0s - loss: 0.2136 - acc: 0.9253 - val_loss: 0.2347 - val_acc: 0.8947\n",
      "Epoch 48/200\n",
      "455/455 [==============================] - 0s - loss: 0.2133 - acc: 0.9253 - val_loss: 0.2338 - val_acc: 0.8947\n",
      "Epoch 49/200\n",
      "455/455 [==============================] - 0s - loss: 0.2110 - acc: 0.9297 - val_loss: 0.2284 - val_acc: 0.9035\n",
      "Epoch 50/200\n",
      "455/455 [==============================] - 0s - loss: 0.2107 - acc: 0.9253 - val_loss: 0.2261 - val_acc: 0.9211\n",
      "Epoch 51/200\n",
      "455/455 [==============================] - 0s - loss: 0.2172 - acc: 0.9143 - val_loss: 0.2326 - val_acc: 0.8947\n",
      "Epoch 52/200\n",
      "455/455 [==============================] - 0s - loss: 0.2087 - acc: 0.9231 - val_loss: 0.2263 - val_acc: 0.8947\n",
      "Epoch 53/200\n",
      "455/455 [==============================] - 0s - loss: 0.2039 - acc: 0.9253 - val_loss: 0.2207 - val_acc: 0.9298\n",
      "Epoch 54/200\n",
      "455/455 [==============================] - 0s - loss: 0.2049 - acc: 0.9231 - val_loss: 0.2286 - val_acc: 0.8947\n",
      "Epoch 55/200\n",
      "455/455 [==============================] - 0s - loss: 0.2047 - acc: 0.9209 - val_loss: 0.2186 - val_acc: 0.9123\n",
      "Epoch 56/200\n",
      "455/455 [==============================] - 0s - loss: 0.2017 - acc: 0.9253 - val_loss: 0.2218 - val_acc: 0.9386\n",
      "Epoch 57/200\n",
      "455/455 [==============================] - 0s - loss: 0.2077 - acc: 0.9253 - val_loss: 0.2159 - val_acc: 0.9123\n",
      "Epoch 58/200\n",
      "455/455 [==============================] - 0s - loss: 0.2010 - acc: 0.9297 - val_loss: 0.2343 - val_acc: 0.8947\n",
      "Epoch 59/200\n",
      "455/455 [==============================] - 0s - loss: 0.2118 - acc: 0.9209 - val_loss: 0.2139 - val_acc: 0.9474\n",
      "Epoch 60/200\n",
      "455/455 [==============================] - 0s - loss: 0.2001 - acc: 0.9187 - val_loss: 0.2099 - val_acc: 0.9298\n",
      "Epoch 61/200\n",
      "455/455 [==============================] - 0s - loss: 0.1970 - acc: 0.9231 - val_loss: 0.2111 - val_acc: 0.9474\n",
      "Epoch 62/200\n",
      "455/455 [==============================] - 0s - loss: 0.2054 - acc: 0.9209 - val_loss: 0.2226 - val_acc: 0.8947\n",
      "Epoch 63/200\n",
      "455/455 [==============================] - 0s - loss: 0.1934 - acc: 0.9275 - val_loss: 0.2169 - val_acc: 0.9386\n",
      "Epoch 64/200\n",
      "455/455 [==============================] - 0s - loss: 0.2000 - acc: 0.9187 - val_loss: 0.2465 - val_acc: 0.8860\n",
      "Epoch 65/200\n",
      "455/455 [==============================] - 0s - loss: 0.2015 - acc: 0.9209 - val_loss: 0.2171 - val_acc: 0.9123\n",
      "Epoch 66/200\n",
      "455/455 [==============================] - 0s - loss: 0.1999 - acc: 0.9209 - val_loss: 0.2035 - val_acc: 0.9211\n",
      "Epoch 67/200\n",
      "455/455 [==============================] - 0s - loss: 0.1930 - acc: 0.9209 - val_loss: 0.2069 - val_acc: 0.9035\n",
      "Epoch 68/200\n",
      "455/455 [==============================] - 0s - loss: 0.2026 - acc: 0.9165 - val_loss: 0.2071 - val_acc: 0.9035\n",
      "Epoch 69/200\n",
      "455/455 [==============================] - 0s - loss: 0.1942 - acc: 0.9319 - val_loss: 0.2216 - val_acc: 0.8860\n",
      "Epoch 70/200\n",
      "455/455 [==============================] - 0s - loss: 0.1955 - acc: 0.9253 - val_loss: 0.2067 - val_acc: 0.9386\n",
      "Epoch 71/200\n",
      "455/455 [==============================] - 0s - loss: 0.1969 - acc: 0.9231 - val_loss: 0.1981 - val_acc: 0.9561\n",
      "Epoch 72/200\n",
      "455/455 [==============================] - 0s - loss: 0.1878 - acc: 0.9231 - val_loss: 0.1970 - val_acc: 0.9474\n",
      "Epoch 73/200\n",
      "455/455 [==============================] - 0s - loss: 0.1860 - acc: 0.9253 - val_loss: 0.1970 - val_acc: 0.9561\n",
      "Epoch 74/200\n",
      "455/455 [==============================] - 0s - loss: 0.2052 - acc: 0.9253 - val_loss: 0.2422 - val_acc: 0.8772\n",
      "Epoch 75/200\n",
      "455/455 [==============================] - 0s - loss: 0.2030 - acc: 0.9209 - val_loss: 0.2112 - val_acc: 0.8947\n",
      "Epoch 76/200\n",
      "455/455 [==============================] - 0s - loss: 0.1922 - acc: 0.9231 - val_loss: 0.1948 - val_acc: 0.9474\n",
      "Epoch 77/200\n",
      "455/455 [==============================] - 0s - loss: 0.1904 - acc: 0.9297 - val_loss: 0.2000 - val_acc: 0.9386\n",
      "Epoch 78/200\n",
      "455/455 [==============================] - 0s - loss: 0.1968 - acc: 0.9187 - val_loss: 0.1919 - val_acc: 0.9474\n",
      "Epoch 79/200\n",
      "455/455 [==============================] - 0s - loss: 0.1850 - acc: 0.9187 - val_loss: 0.2017 - val_acc: 0.9035\n",
      "Epoch 80/200\n",
      "455/455 [==============================] - 0s - loss: 0.1837 - acc: 0.9297 - val_loss: 0.1907 - val_acc: 0.9386\n",
      "Epoch 81/200\n",
      "455/455 [==============================] - 0s - loss: 0.1825 - acc: 0.9187 - val_loss: 0.2177 - val_acc: 0.8947\n",
      "Epoch 82/200\n",
      "455/455 [==============================] - 0s - loss: 0.1953 - acc: 0.9253 - val_loss: 0.1992 - val_acc: 0.9035\n",
      "Epoch 83/200\n",
      "455/455 [==============================] - 0s - loss: 0.1838 - acc: 0.9209 - val_loss: 0.1963 - val_acc: 0.9035\n",
      "Epoch 84/200\n",
      "455/455 [==============================] - 0s - loss: 0.1941 - acc: 0.9253 - val_loss: 0.1876 - val_acc: 0.9474\n",
      "Epoch 85/200\n",
      "455/455 [==============================] - 0s - loss: 0.1861 - acc: 0.9231 - val_loss: 0.1953 - val_acc: 0.9123\n",
      "Epoch 86/200\n",
      "455/455 [==============================] - 0s - loss: 0.1815 - acc: 0.9297 - val_loss: 0.1888 - val_acc: 0.9298\n",
      "Epoch 87/200\n",
      "455/455 [==============================] - 0s - loss: 0.1846 - acc: 0.9209 - val_loss: 0.1925 - val_acc: 0.9211\n",
      "Epoch 88/200\n",
      "455/455 [==============================] - 0s - loss: 0.1806 - acc: 0.9231 - val_loss: 0.1867 - val_acc: 0.9386\n",
      "Epoch 89/200\n",
      "455/455 [==============================] - 0s - loss: 0.1874 - acc: 0.9231 - val_loss: 0.1994 - val_acc: 0.9123\n",
      "Epoch 90/200\n",
      "455/455 [==============================] - 0s - loss: 0.1804 - acc: 0.9231 - val_loss: 0.1894 - val_acc: 0.9211\n",
      "Epoch 91/200\n",
      "455/455 [==============================] - 0s - loss: 0.1777 - acc: 0.9231 - val_loss: 0.1939 - val_acc: 0.9035\n",
      "Epoch 92/200\n",
      "455/455 [==============================] - 0s - loss: 0.1814 - acc: 0.9209 - val_loss: 0.1826 - val_acc: 0.9474\n",
      "Epoch 93/200\n",
      "455/455 [==============================] - 0s - loss: 0.1752 - acc: 0.9253 - val_loss: 0.1822 - val_acc: 0.9474\n",
      "Epoch 94/200\n",
      "455/455 [==============================] - 0s - loss: 0.1768 - acc: 0.9319 - val_loss: 0.2011 - val_acc: 0.9035\n",
      "Epoch 95/200\n",
      "455/455 [==============================] - 0s - loss: 0.1767 - acc: 0.9253 - val_loss: 0.2321 - val_acc: 0.9123\n",
      "Epoch 96/200\n",
      "455/455 [==============================] - 0s - loss: 0.1905 - acc: 0.9165 - val_loss: 0.1889 - val_acc: 0.9211\n",
      "Epoch 97/200\n",
      "455/455 [==============================] - 0s - loss: 0.1749 - acc: 0.9319 - val_loss: 0.1904 - val_acc: 0.9123\n",
      "Epoch 98/200\n",
      "455/455 [==============================] - 0s - loss: 0.1830 - acc: 0.9275 - val_loss: 0.1816 - val_acc: 0.9474\n",
      "Epoch 99/200\n",
      "455/455 [==============================] - 0s - loss: 0.1734 - acc: 0.9275 - val_loss: 0.1863 - val_acc: 0.9298\n",
      "Epoch 100/200\n",
      "455/455 [==============================] - 0s - loss: 0.1781 - acc: 0.9275 - val_loss: 0.1794 - val_acc: 0.9474\n",
      "Epoch 101/200\n",
      "455/455 [==============================] - 0s - loss: 0.1733 - acc: 0.9231 - val_loss: 0.1788 - val_acc: 0.9474\n",
      "Epoch 102/200\n",
      "455/455 [==============================] - 0s - loss: 0.1733 - acc: 0.9231 - val_loss: 0.1929 - val_acc: 0.9123\n",
      "Epoch 103/200\n",
      "455/455 [==============================] - 0s - loss: 0.1780 - acc: 0.9341 - val_loss: 0.1848 - val_acc: 0.9298\n",
      "Epoch 104/200\n",
      "455/455 [==============================] - 0s - loss: 0.1783 - acc: 0.9231 - val_loss: 0.1913 - val_acc: 0.9123\n",
      "Epoch 105/200\n",
      "455/455 [==============================] - 0s - loss: 0.1787 - acc: 0.9231 - val_loss: 0.2113 - val_acc: 0.8860\n",
      "Epoch 106/200\n",
      "455/455 [==============================] - 0s - loss: 0.1820 - acc: 0.9363 - val_loss: 0.1804 - val_acc: 0.9386\n",
      "Epoch 107/200\n",
      "455/455 [==============================] - 0s - loss: 0.1776 - acc: 0.9297 - val_loss: 0.1774 - val_acc: 0.9474\n",
      "Epoch 108/200\n",
      "455/455 [==============================] - 0s - loss: 0.1722 - acc: 0.9231 - val_loss: 0.2494 - val_acc: 0.8772\n",
      "Epoch 109/200\n",
      "455/455 [==============================] - 0s - loss: 0.1902 - acc: 0.9275 - val_loss: 0.1759 - val_acc: 0.9474\n",
      "Epoch 110/200\n",
      "455/455 [==============================] - 0s - loss: 0.1685 - acc: 0.9341 - val_loss: 0.1758 - val_acc: 0.9474\n",
      "Epoch 111/200\n",
      "455/455 [==============================] - 0s - loss: 0.1689 - acc: 0.9253 - val_loss: 0.1752 - val_acc: 0.9474\n",
      "Epoch 112/200\n",
      "455/455 [==============================] - 0s - loss: 0.1680 - acc: 0.9275 - val_loss: 0.1744 - val_acc: 0.9474\n",
      "Epoch 113/200\n",
      "455/455 [==============================] - 0s - loss: 0.1706 - acc: 0.9319 - val_loss: 0.1775 - val_acc: 0.9474\n",
      "Epoch 114/200\n",
      "455/455 [==============================] - 0s - loss: 0.1741 - acc: 0.9275 - val_loss: 0.1738 - val_acc: 0.9474\n",
      "Epoch 115/200\n",
      "455/455 [==============================] - 0s - loss: 0.1808 - acc: 0.9253 - val_loss: 0.1932 - val_acc: 0.9123\n",
      "Epoch 116/200\n",
      "455/455 [==============================] - 0s - loss: 0.1696 - acc: 0.9297 - val_loss: 0.1795 - val_acc: 0.9298\n",
      "Epoch 117/200\n",
      "455/455 [==============================] - 0s - loss: 0.1715 - acc: 0.9231 - val_loss: 0.1823 - val_acc: 0.9298\n",
      "Epoch 118/200\n",
      "455/455 [==============================] - 0s - loss: 0.1676 - acc: 0.9297 - val_loss: 0.1735 - val_acc: 0.9474\n",
      "Epoch 119/200\n",
      "455/455 [==============================] - 0s - loss: 0.1789 - acc: 0.9275 - val_loss: 0.1726 - val_acc: 0.9474\n",
      "Epoch 120/200\n",
      "455/455 [==============================] - 0s - loss: 0.1666 - acc: 0.9253 - val_loss: 0.1950 - val_acc: 0.9035\n",
      "Epoch 121/200\n",
      "455/455 [==============================] - 0s - loss: 0.1826 - acc: 0.9275 - val_loss: 0.1894 - val_acc: 0.9123\n",
      "Epoch 122/200\n",
      "455/455 [==============================] - 0s - loss: 0.1713 - acc: 0.9341 - val_loss: 0.1709 - val_acc: 0.9474\n",
      "Epoch 123/200\n",
      "455/455 [==============================] - 0s - loss: 0.1652 - acc: 0.9297 - val_loss: 0.1702 - val_acc: 0.9474\n",
      "Epoch 124/200\n",
      "455/455 [==============================] - 0s - loss: 0.1645 - acc: 0.9253 - val_loss: 0.1701 - val_acc: 0.9474\n",
      "Epoch 125/200\n",
      "455/455 [==============================] - 0s - loss: 0.1823 - acc: 0.9275 - val_loss: 0.1733 - val_acc: 0.9474\n",
      "Epoch 126/200\n",
      "455/455 [==============================] - 0s - loss: 0.1642 - acc: 0.9363 - val_loss: 0.1979 - val_acc: 0.9035\n",
      "Epoch 127/200\n",
      "455/455 [==============================] - 0s - loss: 0.1791 - acc: 0.9341 - val_loss: 0.2067 - val_acc: 0.8947\n",
      "Epoch 128/200\n",
      "455/455 [==============================] - 0s - loss: 0.1700 - acc: 0.9363 - val_loss: 0.1700 - val_acc: 0.9474\n",
      "Epoch 129/200\n",
      "455/455 [==============================] - 0s - loss: 0.1769 - acc: 0.9319 - val_loss: 0.1690 - val_acc: 0.9474\n",
      "Epoch 130/200\n",
      "455/455 [==============================] - 0s - loss: 0.1734 - acc: 0.9231 - val_loss: 0.1847 - val_acc: 0.9035\n",
      "Epoch 131/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455/455 [==============================] - 0s - loss: 0.1659 - acc: 0.9319 - val_loss: 0.1684 - val_acc: 0.9474\n",
      "Epoch 132/200\n",
      "455/455 [==============================] - 0s - loss: 0.1620 - acc: 0.9297 - val_loss: 0.1838 - val_acc: 0.9211\n",
      "Epoch 133/200\n",
      "455/455 [==============================] - 0s - loss: 0.1746 - acc: 0.9297 - val_loss: 0.1685 - val_acc: 0.9474\n",
      "Epoch 134/200\n",
      "455/455 [==============================] - 0s - loss: 0.1661 - acc: 0.9297 - val_loss: 0.1678 - val_acc: 0.9474\n",
      "Epoch 135/200\n",
      "455/455 [==============================] - 0s - loss: 0.1659 - acc: 0.9341 - val_loss: 0.1987 - val_acc: 0.9123\n",
      "Epoch 136/200\n",
      "455/455 [==============================] - 0s - loss: 0.1827 - acc: 0.9319 - val_loss: 0.1729 - val_acc: 0.9474\n",
      "Epoch 137/200\n",
      "455/455 [==============================] - 0s - loss: 0.1784 - acc: 0.9341 - val_loss: 0.2835 - val_acc: 0.8772\n",
      "Epoch 138/200\n",
      "455/455 [==============================] - 0s - loss: 0.2229 - acc: 0.9055 - val_loss: 0.1669 - val_acc: 0.9474\n",
      "Epoch 139/200\n",
      "455/455 [==============================] - 0s - loss: 0.1674 - acc: 0.9297 - val_loss: 0.1667 - val_acc: 0.9474\n",
      "Epoch 140/200\n",
      "455/455 [==============================] - 0s - loss: 0.1598 - acc: 0.9319 - val_loss: 0.1716 - val_acc: 0.9474\n",
      "Epoch 141/200\n",
      "455/455 [==============================] - 0s - loss: 0.1603 - acc: 0.9341 - val_loss: 0.1660 - val_acc: 0.9474\n",
      "Epoch 142/200\n",
      "455/455 [==============================] - 0s - loss: 0.1618 - acc: 0.9319 - val_loss: 0.1665 - val_acc: 0.9561\n",
      "Epoch 143/200\n",
      "455/455 [==============================] - 0s - loss: 0.1599 - acc: 0.9319 - val_loss: 0.1692 - val_acc: 0.9474\n",
      "Epoch 144/200\n",
      "455/455 [==============================] - 0s - loss: 0.1573 - acc: 0.9363 - val_loss: 0.1665 - val_acc: 0.9561\n",
      "Epoch 145/200\n",
      "455/455 [==============================] - 0s - loss: 0.1633 - acc: 0.9275 - val_loss: 0.1647 - val_acc: 0.9474\n",
      "Epoch 146/200\n",
      "455/455 [==============================] - 0s - loss: 0.1595 - acc: 0.9319 - val_loss: 0.1652 - val_acc: 0.9561\n",
      "Epoch 147/200\n",
      "455/455 [==============================] - 0s - loss: 0.1677 - acc: 0.9297 - val_loss: 0.1978 - val_acc: 0.9123\n",
      "Epoch 148/200\n",
      "455/455 [==============================] - 0s - loss: 0.1834 - acc: 0.9275 - val_loss: 0.1853 - val_acc: 0.9123\n",
      "Epoch 149/200\n",
      "455/455 [==============================] - 0s - loss: 0.1788 - acc: 0.9253 - val_loss: 0.2194 - val_acc: 0.8860\n",
      "Epoch 150/200\n",
      "455/455 [==============================] - 0s - loss: 0.1648 - acc: 0.9429 - val_loss: 0.1698 - val_acc: 0.9298\n",
      "Epoch 151/200\n",
      "455/455 [==============================] - 0s - loss: 0.1647 - acc: 0.9363 - val_loss: 0.1635 - val_acc: 0.9474\n",
      "Epoch 152/200\n",
      "455/455 [==============================] - 0s - loss: 0.1632 - acc: 0.9319 - val_loss: 0.1638 - val_acc: 0.9561\n",
      "Epoch 153/200\n",
      "455/455 [==============================] - 0s - loss: 0.1632 - acc: 0.9275 - val_loss: 0.2007 - val_acc: 0.9123\n",
      "Epoch 154/200\n",
      "455/455 [==============================] - 0s - loss: 0.2090 - acc: 0.9319 - val_loss: 0.2049 - val_acc: 0.8860\n",
      "Epoch 155/200\n",
      "455/455 [==============================] - 0s - loss: 0.1592 - acc: 0.9385 - val_loss: 0.1628 - val_acc: 0.9474\n",
      "Epoch 156/200\n",
      "455/455 [==============================] - 0s - loss: 0.1581 - acc: 0.9275 - val_loss: 0.1648 - val_acc: 0.9561\n",
      "Epoch 157/200\n",
      "455/455 [==============================] - 0s - loss: 0.1634 - acc: 0.9341 - val_loss: 0.1620 - val_acc: 0.9474\n",
      "Epoch 158/200\n",
      "455/455 [==============================] - 0s - loss: 0.1863 - acc: 0.9275 - val_loss: 0.2795 - val_acc: 0.8860\n",
      "Epoch 159/200\n",
      "455/455 [==============================] - 0s - loss: 0.1845 - acc: 0.9253 - val_loss: 0.1838 - val_acc: 0.9035\n",
      "Epoch 160/200\n",
      "455/455 [==============================] - 0s - loss: 0.1663 - acc: 0.9253 - val_loss: 0.1618 - val_acc: 0.9474\n",
      "Epoch 161/200\n",
      "455/455 [==============================] - 0s - loss: 0.1775 - acc: 0.9385 - val_loss: 0.1878 - val_acc: 0.9035\n",
      "Epoch 162/200\n",
      "455/455 [==============================] - 0s - loss: 0.1784 - acc: 0.9231 - val_loss: 0.1653 - val_acc: 0.9386\n",
      "Epoch 163/200\n",
      "455/455 [==============================] - 0s - loss: 0.1715 - acc: 0.9253 - val_loss: 0.1640 - val_acc: 0.9474\n",
      "Epoch 164/200\n",
      "455/455 [==============================] - 0s - loss: 0.1610 - acc: 0.9363 - val_loss: 0.1641 - val_acc: 0.9474\n",
      "Epoch 165/200\n",
      "455/455 [==============================] - 0s - loss: 0.1581 - acc: 0.9341 - val_loss: 0.1665 - val_acc: 0.9474\n",
      "Epoch 166/200\n",
      "455/455 [==============================] - 0s - loss: 0.1602 - acc: 0.9319 - val_loss: 0.1604 - val_acc: 0.9474\n",
      "Epoch 167/200\n",
      "455/455 [==============================] - 0s - loss: 0.1589 - acc: 0.9341 - val_loss: 0.1635 - val_acc: 0.9474\n",
      "Epoch 168/200\n",
      "455/455 [==============================] - 0s - loss: 0.1670 - acc: 0.9275 - val_loss: 0.1639 - val_acc: 0.9386\n",
      "Epoch 169/200\n",
      "455/455 [==============================] - 0s - loss: 0.1611 - acc: 0.9319 - val_loss: 0.1606 - val_acc: 0.9561\n",
      "Epoch 170/200\n",
      "455/455 [==============================] - 0s - loss: 0.1538 - acc: 0.9429 - val_loss: 0.1602 - val_acc: 0.9474\n",
      "Epoch 171/200\n",
      "455/455 [==============================] - 0s - loss: 0.1539 - acc: 0.9341 - val_loss: 0.1628 - val_acc: 0.9474\n",
      "Epoch 172/200\n",
      "455/455 [==============================] - 0s - loss: 0.1573 - acc: 0.9297 - val_loss: 0.1622 - val_acc: 0.9474\n",
      "Epoch 173/200\n",
      "455/455 [==============================] - 0s - loss: 0.1560 - acc: 0.9385 - val_loss: 0.1626 - val_acc: 0.9386\n",
      "Epoch 174/200\n",
      "455/455 [==============================] - 0s - loss: 0.1561 - acc: 0.9363 - val_loss: 0.1619 - val_acc: 0.9474\n",
      "Epoch 175/200\n",
      "455/455 [==============================] - 0s - loss: 0.1529 - acc: 0.9407 - val_loss: 0.1591 - val_acc: 0.9474\n",
      "Epoch 176/200\n",
      "455/455 [==============================] - 0s - loss: 0.1608 - acc: 0.9319 - val_loss: 0.2031 - val_acc: 0.8860\n",
      "Epoch 177/200\n",
      "455/455 [==============================] - 0s - loss: 0.1808 - acc: 0.9275 - val_loss: 0.1703 - val_acc: 0.9123\n",
      "Epoch 178/200\n",
      "455/455 [==============================] - 0s - loss: 0.1599 - acc: 0.9429 - val_loss: 0.1605 - val_acc: 0.9474\n",
      "Epoch 179/200\n",
      "455/455 [==============================] - 0s - loss: 0.1661 - acc: 0.9363 - val_loss: 0.1654 - val_acc: 0.9474\n",
      "Epoch 180/200\n",
      "455/455 [==============================] - 0s - loss: 0.1623 - acc: 0.9297 - val_loss: 0.1719 - val_acc: 0.9386\n",
      "Epoch 181/200\n",
      "455/455 [==============================] - 0s - loss: 0.1606 - acc: 0.9297 - val_loss: 0.1580 - val_acc: 0.9561\n",
      "Epoch 182/200\n",
      "455/455 [==============================] - 0s - loss: 0.1600 - acc: 0.9319 - val_loss: 0.1595 - val_acc: 0.9474\n",
      "Epoch 183/200\n",
      "455/455 [==============================] - 0s - loss: 0.1515 - acc: 0.9319 - val_loss: 0.1602 - val_acc: 0.9474\n",
      "Epoch 184/200\n",
      "455/455 [==============================] - 0s - loss: 0.1660 - acc: 0.9341 - val_loss: 0.1574 - val_acc: 0.9561\n",
      "Epoch 185/200\n",
      "455/455 [==============================] - 0s - loss: 0.1577 - acc: 0.9319 - val_loss: 0.1705 - val_acc: 0.9474\n",
      "Epoch 186/200\n",
      "455/455 [==============================] - 0s - loss: 0.1590 - acc: 0.9385 - val_loss: 0.1715 - val_acc: 0.9035\n",
      "Epoch 187/200\n",
      "455/455 [==============================] - 0s - loss: 0.1688 - acc: 0.9407 - val_loss: 0.1569 - val_acc: 0.9561\n",
      "Epoch 188/200\n",
      "455/455 [==============================] - 0s - loss: 0.1527 - acc: 0.9407 - val_loss: 0.1690 - val_acc: 0.9474\n",
      "Epoch 189/200\n",
      "455/455 [==============================] - 0s - loss: 0.1533 - acc: 0.9341 - val_loss: 0.1566 - val_acc: 0.9474\n",
      "Epoch 190/200\n",
      "455/455 [==============================] - 0s - loss: 0.1563 - acc: 0.9209 - val_loss: 0.1574 - val_acc: 0.9474\n",
      "Epoch 191/200\n",
      "455/455 [==============================] - 0s - loss: 0.1749 - acc: 0.9341 - val_loss: 0.1597 - val_acc: 0.9386\n",
      "Epoch 192/200\n",
      "455/455 [==============================] - 0s - loss: 0.1556 - acc: 0.9297 - val_loss: 0.1583 - val_acc: 0.9474\n",
      "Epoch 193/200\n",
      "455/455 [==============================] - 0s - loss: 0.1532 - acc: 0.9363 - val_loss: 0.1650 - val_acc: 0.9474\n",
      "Epoch 194/200\n",
      "455/455 [==============================] - 0s - loss: 0.1481 - acc: 0.9473 - val_loss: 0.1736 - val_acc: 0.9035\n",
      "Epoch 195/200\n",
      "455/455 [==============================] - 0s - loss: 0.1801 - acc: 0.9253 - val_loss: 0.1546 - val_acc: 0.9561\n",
      "Epoch 196/200\n",
      "455/455 [==============================] - 0s - loss: 0.1549 - acc: 0.9341 - val_loss: 0.1543 - val_acc: 0.9561\n",
      "Epoch 197/200\n",
      "455/455 [==============================] - 0s - loss: 0.1563 - acc: 0.9319 - val_loss: 0.1558 - val_acc: 0.9474\n",
      "Epoch 198/200\n",
      "455/455 [==============================] - 0s - loss: 0.1514 - acc: 0.9363 - val_loss: 0.1610 - val_acc: 0.9474\n",
      "Epoch 199/200\n",
      "455/455 [==============================] - 0s - loss: 0.1614 - acc: 0.9385 - val_loss: 0.1937 - val_acc: 0.9123\n",
      "Epoch 200/200\n",
      "455/455 [==============================] - 0s - loss: 0.1705 - acc: 0.9363 - val_loss: 0.1579 - val_acc: 0.9474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efe7b82bcc0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaler.transform(data.data), data.target,\n",
    "                                                    test_size=0.2, random_state=1)\n",
    "# initialize model\n",
    "model = get_model_1()\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200,\n",
    "          callbacks=[early_stopping], verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EarlyStopping callback terminated process earlier, but the model now is not the best one (according to \"val_loss\" criterion). The best model was achieved (patience+1) epochs before the end.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "**Assignment 4** Modify the last example.\n",
    "\n",
    "1. Add *ModelCheckpoint* saving the best model to list of callbacks.\n",
    "2. Fit the model and find validation loss history minimum.\n",
    "3. Evaluate validation loss with the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Skipped code\n",
    "# Assignment 4\n",
    "\n",
    "#Train on 455 samples, validate on 114 samples\n",
    "#Epoch 1/200\n",
    "#455/455 [==============================] - 0s - loss: 0.6614 - acc: 0.6022 - val_loss: 0.6281 - val_acc: 0.6579\n",
    "#Epoch 2/200\n",
    "#455/455 [==============================] - 0s - loss: 0.6100 - acc: 0.6615 - val_loss: 0.5865 - val_acc: 0.7018\n",
    "#Epoch 3/200\n",
    "#455/455 [==============================] - 0s - loss: 0.5635 - acc: 0.7363 - val_loss: 0.5470 - val_acc: 0.7368\n",
    "#Epoch 4/200\n",
    "#455/455 [==============================] - 0s - loss: 0.5184 - acc: 0.7956 - val_loss: 0.5091 - val_acc: 0.7807\n",
    "#Epoch 5/200\n",
    "#455/455 [==============================] - 0s - loss: 0.4740 - acc: 0.8374 - val_loss: 0.4735 - val_acc: 0.8070\n",
    "#Epoch 6/200\n",
    "#455/455 [==============================] - 0s - loss: 0.4323 - acc: 0.8571 - val_loss: 0.4356 - val_acc: 0.8246\n",
    "\n",
    "#validation loss history minimum  0.0757933767643\n",
    "#evaluation results of the best model 0.0757933767643\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge layers\n",
    "\n",
    "Merge layers is a set of layers that merge input tensors  executing arithmetic operations over them or just concatenating them. \n",
    "\n",
    "The list of merge layers is not large: \n",
    "- Add, Subtract, Multiply, Average, Maximum  - take as input a list of tensors, all of the same shape, and return a single tensor (also of the same shape) - the result of corresponding pointwise arithmetic operation. - Dot - dot product, \n",
    "- Concatenate has been already discussed earlier. \n",
    "\n",
    "Functional API uses lowercase variants of their names. Import them from *keras.layers* before using, e.g.\n",
    "\n",
    "`*from keras.layers import add*`\n",
    "\n",
    "NB! Though *subtract* layer is described in Keras documentation it may not work in earlier versions giving the error message **\"cannot import name 'subtract'\"** when you try to import it. \n",
    "In order to subtract two inputs you may use *Lambda* core layer that wraps arbitrary expression as a Layer object. For instance, the line  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Lambda\n",
    "sqr_input = Lambda(lambda x: x*x)(input1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defines layer that returns squared input. Here is a way of subtracting two inputs using *Lambda* layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Input(shape=(100,))\n",
    "y = Input(shape=(100,))\n",
    "# Note: shapes must match!\n",
    "subtract_layer = Lambda(lambda inputs: inputs[0] - inputs[1])\n",
    "diff = subtract_layer([x, y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "**Assignment 5**\n",
    "\n",
    "Create network with two inputs. <br>\n",
    "Input shapes are (10,) and (20,). <br>\n",
    "Each input is processed with its own Dense layer *hidden1* and *hidden2* consisting of 5 neurons. <br>\n",
    "The network output should be square of differences of outputs of *hidden1* and *hidden2*. <br>\n",
    "Draw this network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Skipped code\n",
    "# Assignment 5 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test_model5](https://ilykei.com/api/fileProxy/documents%2FAdvanced%20Machine%20Learning%2FLecture%207%20AdvML%2Ftest_model5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "One of the major issues with artificial neural networks is that the models are quite complicated. For example, let us consider a neural network that is pulling data from an image from the MNIST database (28 by 28 pixels), feeds into two hidden layers with 30 neurons, and finally reaches a soft-max layer of 10 neurons. The total number of parameters in the network is nearly 25,000. Aparently the model with so many parameters is prone to overfititng.\n",
    "\n",
    "Keras has a number of layers that can help prevent overfitting.  \n",
    "\n",
    "### Dropout\n",
    "\n",
    "Dropout is a regularization technique for neural network models proposed by Srivastava, et al. in their 2014 paper [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf).\n",
    "\n",
    "Dropout is a technique where randomly selected neurons are ignored during training. <br>\n",
    "They are “dropped-out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.\n",
    "\n",
    "Dropout layer can be created as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "dropout_layer = Dropout(rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where *rate* is float number between 0 and 1 meaning fraction of the input units to drop.\n",
    "\n",
    "Keras provides a number of Noise layers that are also useful to mitigate overfitting.  \n",
    "\n",
    "### GaussianNoise\n",
    "\n",
    "Layer *GaussianNoise(stddev)* applies additive zero-centered Gaussian noise with standard deviation *stdev*. <br>\n",
    "It could be treated as a form of random data augmentation. Since it is a regularization layer, it is only active at training time.\n",
    "\n",
    "### GaussianDropout\n",
    "\n",
    "Layer *GaussianDropout(rate)* applies multiplicative 1-centered Gaussian noise with standard deviation \n",
    "$$\\sqrt{\\frac{rate} { (1 - rate)}}.$$ \n",
    "It is only active at training time.\n",
    "\n",
    "The way of choosing appropriate dropout rate and some other parameters will be shown in TuningNetworks notebook."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
